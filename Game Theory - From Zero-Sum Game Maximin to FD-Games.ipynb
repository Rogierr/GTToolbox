{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook we will try to build an algorithm for so-called threatpoints in FD-games. However in order to find these threatpoints we shall start with looking at basic games. Therefore we begin with zero-sum games, these are games in which player 1 receives a certain payoff which should be the inverse of the payoff of the other player. We focus on two player single stage zero-sum games with two or more actions, i.e. two players each have two or more choices on one fixed period in time. We are going to look for maximin and minimax equilibria for these types of games, these are also generating the Nash Equilibria in these games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  #import numpy\n",
    "import scipy as sp  #import scipy\n",
    "from scipy import optimize #import the scipy optimize part\n",
    "from scipy import linalg #import the linear algebra section\n",
    "from scipy.spatial import ConvexHull #import scipy convex hull package\n",
    "import matplotlib.pyplot as plt #import package to plot stuff\n",
    "import msmtools as msm          #import package for markov chains\n",
    "from msmtools.analysis import stationary_distribution #import for calculating stationary distributions\n",
    "import mdptoolbox #import toolbox for MDP's\n",
    "import time #import time package\n",
    "from itertools import permutations #import package for permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ZeroSumGame:\n",
    "    \"\"\"In this class we build a simple zero-sum game, a person can input\n",
    "    the payoff matrix and this program calculates the Nash Equilibrium based on a Pure, Mixed or LP approach.\"\"\"\n",
    "    \n",
    "    def __init__(self,payoffs):\n",
    "        \"Create a class of zero-sum games.\"\n",
    "        \n",
    "        self.payoffs = payoffs                   #store payoff in the class\n",
    "        \n",
    "    def pure_solution(self):                     #create a function which only computes pure solutions\n",
    "        \"Compute a pure solution, if possible. Probably not the easiest route to compute, but it is a feasible route.\"\n",
    "               \n",
    "        best_resp_p1 = self.payoffs.argmax(0)    #look for maximal values in columns, return index value\n",
    "        best_resp_p2 = self.payoffs.argmin(1)    #look for minimal values in the rows, return index value\n",
    "        \n",
    "        best_resp_ind_p1 = []                    #empty placeholders to store index values\n",
    "        best_resp_ind_p2 = []                    \n",
    "        \n",
    "        # here below we convert the best responses to a readable array of index values\n",
    "        for i in range(0, best_resp_p1.shape[1]):\n",
    "            best_resp_ind_p1.append((best_resp_p1[0,i],i))\n",
    "            \n",
    "        for j in range(0, best_resp_p2.shape[0]):\n",
    "            best_resp_ind_p2.append((j,best_resp_p2[j,0]))\n",
    "        \n",
    "        NashEq_exists = False           #assume no Pure Nash exist first (for use if no pure is found)\n",
    "        \n",
    "        #check if a pure Nash Equilibrium exists by looping over index values\n",
    "        for i in range(0,len(best_resp_ind_p1)):\n",
    "            for j in range(0,len(best_resp_ind_p2)):\n",
    "                if(best_resp_ind_p1[i] == best_resp_ind_p2[j]) == True:\n",
    "                    NashEq = best_resp_ind_p1[i]\n",
    "                    NashEq_exists = True           #pure Nash has been found, so change it's value to true\n",
    "        \n",
    "        if (NashEq_exists == True):                               #if a pure Nash Equilibrium exists, do the following\n",
    "            print(\"Pure Nash Equilibrium has been found\")\n",
    "            index_NashEq = NashEq             #get value of the Nash Equilibrium and print it on the screen\n",
    "            print(\"Nash Equilibrium has value:\", self.payoffs[index_NashEq])\n",
    "            \n",
    "            valueNash = self.payoffs[index_NashEq] #store the value of the Nash\n",
    "            equal = np.equal(valueNash,self.payoffs) #look for equal values which are Nash candidates\n",
    "            print(\"\")\n",
    "            \n",
    "            if np.sum(equal) > 1:                #if multiple pure Nash candidates exist          \n",
    "                print(\"Multiple Nash Equilibria candidates found\")\n",
    "                indexvalues = np.argwhere(equal) #lookup all locations with pure Nash value\n",
    "                length_index = round(indexvalues.size/2) #make a value for the number of found candidates\n",
    "                \n",
    "                counter = 0                   #count number of nashes\n",
    "                \n",
    "                for k in range(0,length_index):  #loop over all possible candidates\n",
    "                    index_row_column = indexvalues[k] #store index value of the candidate\n",
    "                    \n",
    "                    value_found = self.payoffs[index_row_column[0],index_row_column[1]] #store found value\n",
    "                    row_min = np.amin(self.payoffs[index_row_column[0]],axis=1)         #look for minimal value in the row index\n",
    "                    column_max = np.amax(self.payoffs[:,index_row_column[1]],axis=0)    #look for maximal value in the column index\n",
    "                    \n",
    "                    if row_min == value_found:    #only execute if value found is indeed row minimum\n",
    "                        \n",
    "                        if column_max == value_found:  #and last but not least execute if value found is column maximum\n",
    "                            print(\"Player 1 can play row:\",index_row_column[0]) #print the strategy pair if it is a pure Nash\n",
    "                            print(\"Player 2 can play column:\",index_row_column[1])\n",
    "                            print(\"\")\n",
    "                            counter = counter + 1\n",
    "                    \n",
    "                if counter == 1:                  #if only one candidate is a Pure Nash, print this\n",
    "                    print(\"Only one pure Nash Equilibrium found within the candidates\")\n",
    "                    print(\"\")\n",
    "                        \n",
    "                else:                             #else print the total number of Pure Nash\n",
    "                    print(\"Number of pure Nash Equilibria found:\", counter)\n",
    "                    print(\"\")\n",
    "\n",
    "            else:                                #if only one pure Nash exists, print this on the screen\n",
    "                print(\"Only one pure Nash Equilibrium found\")\n",
    "                print(\"Player 1 plays row\",index_NashEq[0])\n",
    "                print(\"Player 2 plays column\",index_NashEq[1])\n",
    "                print(\"\")\n",
    "            \n",
    "        else:                                    #if no pure Nash exist, print this on the screen\n",
    "            print(\"No Pure Nash Equilibrium has been found\")\n",
    "            print(\"\")\n",
    "        \n",
    "    def mixed_solution(self,precision_steps,rounding):\n",
    "        \"Compute a mixed solution, which always exist in zero-sum games. This approach is probably an inefficient one.\"\n",
    "        \"Currently only works for games in which players have an equal amount of actions\"\n",
    "        \n",
    "        # make a list of all possiblities for mixed strategy values\n",
    "        mixed_strategies_possibilities = np.arange(0,1+(1/precision_steps),1/precision_steps)\n",
    "        \n",
    "        # the base number is the number of numbers (yeah, this is not really clear right?)\n",
    "        base_number = (precision_steps+1)\n",
    "        \n",
    "        # create a zero matrix which can be filled for future strategy candidates\n",
    "        strategy_candidates = np.zeros((base_number**self.payoffs.shape[0],self.payoffs.shape[0]))\n",
    "        \n",
    "        #build strategy_candidates, summing to 1\n",
    "        for i in range(0,self.payoffs.shape[0]):              #loop over the number of actions\n",
    "            for j in range(0,base_number):                    #loop over the base number\n",
    "                b = j*(base_number**(self.payoffs.shape[0]-1))    #b is used in order to select certain strategy candidates\n",
    "                count = 0\n",
    "                for k in range(1,(base_number**(self.payoffs.shape[0]-1))+1):\n",
    "                    if k % base_number == 0:\n",
    "                        count = count + 1\n",
    "                    strategy_candidates[b+k-1,i] = (mixed_strategies_possibilities[k-1-base_number*count])\n",
    "            \n",
    "            strategy_candidates = strategy_candidates[strategy_candidates[:,i].argsort()]\n",
    "        \n",
    "        check_prob_sum = np.sum(strategy_candidates, axis=1)\n",
    "        indices_real_candidates = np.where(check_prob_sum == 1)\n",
    "        indices_real_candidates = indices_real_candidates[0]\n",
    "        mixed_strategies = strategy_candidates[indices_real_candidates]\n",
    "        \n",
    "        #maximin strategy player1, defined here below\n",
    "        \n",
    "        p1_poss_payoffs = np.dot(mixed_strategies,self.payoffs)\n",
    "        p1_min_per_strat = p1_poss_payoffs.min(1)\n",
    "        p1_maximin = p1_min_per_strat.max()\n",
    "        \n",
    "        print(\"The maximin strategy for Player 1 gives:\",round(p1_maximin,rounding))\n",
    "        print(\"\")\n",
    "        \n",
    "        #minimax strategy player2, defined here below\n",
    "        \n",
    "        p2_poss_payoffs = np.dot(self.payoffs,mixed_strategies.transpose())\n",
    "        p2_max_per_strat = p2_poss_payoffs.max(0)\n",
    "        p2_minimax = p2_max_per_strat.min()\n",
    "        \n",
    "        print(\"The minimax strategy for Player 2 gives:\",round(p2_minimax,rounding))\n",
    "        print(\"\")\n",
    "        \n",
    "        if round(p1_maximin,rounding) == round(p2_minimax,rounding):\n",
    "            \n",
    "            # print that A Mixed Nash has been found\n",
    "            print(\"Mixed Nash equilbrium has been found with value\")\n",
    "            print(round(p1_maximin,rounding))                           #print found Mixed Nash equilibrium value\n",
    "            print(\"\")\n",
    "            \n",
    "            p1_nash_true = p1_poss_payoffs == p1_maximin\n",
    "            p1_nashcand_ind = np.argwhere(p1_nash_true)\n",
    "            p1_nashcand_rowind = np.unique(p1_nashcand_ind[:,0])\n",
    "            p1_nash_rowind = np.min(p1_poss_payoffs[p1_nashcand_rowind]) == p1_maximin\n",
    "            p1_real_mixed_nash_ind = (p1_nashcand_rowind[p1_nash_rowind])\n",
    "            \n",
    "            print(\"Player 1 plays maximin strategy:\")\n",
    "            print(mixed_strategies[p1_real_mixed_nash_ind])\n",
    "            print(\"\")\n",
    "            \n",
    "            p2_nash_true = p2_poss_payoffs == p2_minimax\n",
    "            p2_nashcand_ind = np.argwhere(p2_nash_true)\n",
    "            p2_nashcand_rowind = np.unique(p2_nashcand_ind[:,1])\n",
    "            \n",
    "            p2_nash_rowind = (p2_poss_payoffs[:,p2_nashcand_rowind].max(0) == p2_minimax)\n",
    "            p2_outofMatrix = p2_nash_rowind.tolist()\n",
    "            p2_real_mixed_nash_ind = p2_nashcand_rowind[p2_outofMatrix]\n",
    "            \n",
    "            print(\"Player 2 plays minimax strategy:\")\n",
    "            print(mixed_strategies[p2_real_mixed_nash_ind])\n",
    "            \n",
    "        else:\n",
    "            print(\"No mixed Nash equilibrium has been found with the current number of precision steps and rounding value\")\n",
    "        \n",
    "    def LP_solution(self):\n",
    "        \"Compute a Nash solution with a Linear Programming approach\"\n",
    "                            \n",
    "        u = np.ones(self.payoffs.shape[0])\n",
    "        M = self.payoffs\n",
    "        nega = u*-1\n",
    "        x0bounds = (0, 1)\n",
    "        x1bounds = (0, 1)\n",
    "        print(u)\n",
    "        print(nega)\n",
    "        print(sp.optimize.linprog(u, A_ub=M, b_ub=nega, bounds=(x0bounds,x1bounds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple zero-sum game with one pure NE, but multiple pure Nash candidates\n",
      "Pure Nash Equilibrium has been found\n",
      "Nash Equilibrium has value: 1\n",
      "\n",
      "Multiple Nash Equilibria candidates found\n",
      "Player 1 can play row: 0\n",
      "Player 2 can play column: 0\n",
      "\n",
      "Only one pure Nash Equilibrium found within the candidates\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Here we play a difficult zero-sum game which has a few pure NE\")\n",
    "# d = ZeroSumGame(np.matrix('1 1 1 1; 1 1 1 1; 1 1 1 1; 0 2 2 2'))\n",
    "# d.pure_solution()\n",
    "# #d.mixed_solution(100,2)\n",
    "\n",
    "# print(\"Simple zero-sum game with pure NE\")\n",
    "# e = ZeroSumGame(np.matrix('2 -2; 1 -3'))\n",
    "# e.pure_solution()\n",
    "# e.mixed_solution(8,0)\n",
    "\n",
    "print(\"Simple zero-sum game with one pure NE, but multiple pure Nash candidates\")\n",
    "f = ZeroSumGame(np.matrix('1 1;-2 3'))\n",
    "f.pure_solution()\n",
    "\n",
    "#print(\"Simple positive zero-sum game with one pure NE\")\n",
    "#g = ZeroSumGame(np.matrix('2 3;1 5'))\n",
    "#g.LP_solution()\n",
    "\n",
    "#print(\"Simple zero-sum game without pure NE\")\n",
    "#h = ZeroSumGame(np.matrix('2 7;7 6'))\n",
    "#h.pure_solution()\n",
    "#h.mixed_solution(5000,3)\n",
    "\n",
    "#j = ZeroSumGame(np.matrix('3 -2; -1 0'))\n",
    "#j.mixed_solution(5000,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below we start with the Frequency Dependent type games. We start with the recreation of the Type 1 Game Algorithm 1, described by Samuel in her Master Thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatedGame:\n",
    "    \"\"\"In this type of game we model a repeated type of game. It means that we play a repeating game\n",
    "    in which the payoffs are repeated for a certain (or unlimited) amount of time.\"\"\"\n",
    "    \n",
    "    def __init__(self,payoff_p1,payoff_p2):\n",
    "        \"Here we initialize the game with respective playoffs for both players\"\n",
    "        \n",
    "        # here below we set the payoffs as an aspect of the game, for both players each\n",
    "        self.payoff_p1 = payoff_p1\n",
    "        self.payoff_p2 = payoff_p2\n",
    "        \n",
    "        # we define a set of best pure strategies\n",
    "        self.best_pure_strategies = np.array([[1,0,1,0],[0,1,1,0],[1,0,0,1],[0,1,0,1]])\n",
    "        \n",
    "    def plot_pure_reward_points(self):\n",
    "        \"This function plots the pure reward points in a two dimensional figure\"\n",
    "        \n",
    "        # set the payoffs as an array\n",
    "        payoff_p1_array = self.payoff_p1.A1\n",
    "        payoff_p2_array = self.payoff_p2.A1\n",
    "        \n",
    "        \n",
    "        plt.figure()   #create a figure\n",
    "        plt.scatter(payoff_p1_array,payoff_p2_array, label=\"Pure reward points\", zorder=15, color='b')   #simple plot which plots the possible rewards\n",
    "        \n",
    "        # label the x- and y-axis\n",
    "        plt.xlabel(\"Payoff Player 1\")\n",
    "        plt.ylabel(\"Payoff Player 2\")\n",
    "\n",
    "        \n",
    "    def plot_all_reward_points(self,FD_yn):\n",
    "        \"Here we plot all reward points possible in this repeated game.\"\n",
    "        \n",
    "        # number of times the game is played (large number)\n",
    "        T = 100000\n",
    "\n",
    "        # create payoff vectors based on the number of times the game is played\n",
    "        Payoff1 = np.zeros(T)\n",
    "        Payoff2 = np.zeros(T)\n",
    "        \n",
    "        x = np.zeros(4)      #initialize x for frequency vector\n",
    "        r = np.zeros(4)      #initialize r for random drawing\n",
    "        \n",
    "        \n",
    "        #loop over the number of periods and length of x\n",
    "        for v in range(0,T):\n",
    "            for i in range(0,4):\n",
    "                r[i] = np.random.beta(0.5,0.5)     #draw r from beta distribution\n",
    "                norm_val = np.sum(r)               \n",
    "    \n",
    "            for i in range(0,4):\n",
    "                x[i] = r[i]/norm_val               #normalize r and put it in as x   \n",
    "            \n",
    "            # if there is a FD type of game, then active the FD function\n",
    "            if FD_yn == True:\n",
    "                FD = 1-0.25*(x[1]+2*x[2])-(2/3)*x[3]\n",
    "            else:\n",
    "                FD = 1\n",
    "            \n",
    "            # calculate the first payoffs for p1 and p2\n",
    "            V_p1 = x*np.transpose(self.payoff_p1.flatten())\n",
    "            V_p2 = x*np.transpose(self.payoff_p2.flatten())\n",
    "            \n",
    "            # calculate the payoff based on the FD function\n",
    "            Payoff1[v] = FD*np.sum(V_p1)\n",
    "            Payoff2[v] = FD*np.sum(V_p2)\n",
    "        \n",
    "        # store the maximal payoffs\n",
    "        self.maximal_payoffs = np.zeros(2)\n",
    "        self.maximal_payoffs[0] = np.max(Payoff1)\n",
    "        self.maximal_payoffs[1] = np.max(Payoff2)\n",
    "        \n",
    "        all_payoffs = np.array([Payoff1,Payoff2])  #payoffs player 1 and and p2 merging\n",
    "        all_payoffs = np.transpose(all_payoffs)        #transpose for use in convex_hull\n",
    "        Convex_Hull_Payoffs = ConvexHull(all_payoffs)  #calculate convex_hull of the payoffs\n",
    "        \n",
    "        # here below we plot the convex hull\n",
    "        plt.fill(all_payoffs[Convex_Hull_Payoffs.vertices,0], all_payoffs[Convex_Hull_Payoffs.vertices,1], color='y', zorder=5, label=\"Obtainable rewards\")\n",
    "    \n",
    "#         plt.plot(Payoff1,Payoff2, color ='y', zorder=2, label=\"Reward points\") # disabled for now\n",
    "        \n",
    "        # do some nice plotting\n",
    "        plt.title(\"Reward points of FD Type I game\")\n",
    "        plt.xlabel(\"Reward Player 1\")\n",
    "        plt.ylabel(\"Reward Player 2\")\n",
    "\n",
    "        \n",
    "    def plot_threat_point(self):\n",
    "        \"Plot the threat point found (only working after the threat_point_algorithm function has been applied)\"\n",
    "        \n",
    "        # here below the threat point is plotted\n",
    "        plt.scatter(self.threat_point[0],self.threat_point[1], color='r', zorder=17, label=\"Threat point\")\n",
    "        plt.legend()\n",
    "\n",
    "        \n",
    "    def plot_threat_point_lines(self):\n",
    "        \"Plot the lines which define the limits for the NE reachable under Folk Theorem.\"\n",
    "        \n",
    "        # plot based on the maximum payoffs and threat point\n",
    "        plt.plot([self.threat_point[0],self.threat_point[0]],[self.threat_point[1],self.maximal_payoffs[1]], color='k', zorder=16)\n",
    "        plt.plot([self.threat_point[0],self.maximal_payoffs[0]],[self.threat_point[1],self.threat_point[1]], color='k', zorder=16)\n",
    "        plt.axis('equal')\n",
    "        plt.savefig('TypeI-FD.png', dpi=400)\n",
    "\n",
    "                \n",
    "    def threat_point_p1(self,x):\n",
    "        \"Function in order to determinate the threat point for p1\"\n",
    "        \n",
    "        return np.max(np.dot(self.payoff_p1,x))\n",
    "        \n",
    "    def threat_point_p2(self,x):\n",
    "        \"Function in order to determinate the threat point for p2\"\n",
    "        \n",
    "        return np.max(np.dot(x,self.payoff_p2))\n",
    "    \n",
    "    def maximin_p1(self,x):\n",
    "        \"Function in order to determine the maximin strategy for p1\"\n",
    "        \n",
    "        return np.max(-np.dot(x,self.payoff_p1))\n",
    "    \n",
    "    def maximin_p2(self,x):\n",
    "        \"Function in order to determine the maximin strategy for p2\"\n",
    "        \n",
    "        return np.max(-np.dot(self.payoff_p2,x))\n",
    "    \n",
    "    def threat_point_algorithm(self):\n",
    "        \"This is the much shorter, optimized search for the threat point, not depending on the number of actions.\"\n",
    "        \n",
    "        start_time = time.time() # start a timer for speed measures\n",
    "        \n",
    "        print(\"Threat point algorithm initiated\")\n",
    "        \n",
    "        p_initialize = np.zeros(np.size(self.payoff_p1,0))     #initialize an array for p\n",
    "        q_initialize = np.zeros(np.size(self.payoff_p2,1))     #initialize an array for q\n",
    "        \n",
    "        def sum_con(x):\n",
    "            return 1 - np.sum(x)                               #constraint that all strategy frequencies should sum up to 1\n",
    "        \n",
    "        # here below we initialize an empty boundary\n",
    "        bndsP1 = []\n",
    "        bndsP2 = []\n",
    "        \n",
    "        # we append bounds based on the number of actions\n",
    "        for i in np.nditer(np.arange(np.size(self.payoff_p1,0))):\n",
    "            bndsP1.append((0,1))\n",
    "            \n",
    "        for j in np.nditer(np.arange(np.size(self.payoff_p2,1))):\n",
    "            bndsP2.append((0,1))\n",
    "        \n",
    "        bnds = [(0,1),(0,1)]                                   #player has to play a value between and including 0 and 1\n",
    "        con = [{'type': 'eq', 'fun': sum_con}]                 #defining the use of the summing to 1 constraint\n",
    "        \n",
    "        tp_p1 = sp.optimize.minimize(self.threat_point_p1,q_initialize,bounds=bndsP2,constraints=con)   #minimizer threat point p1\n",
    "        \n",
    "        print(\"Player 2 wants to minimize payoff Player 1 and plays:\", tp_p1.x)\n",
    "        print(\"Therefore the threat point of Player 1:\",tp_p1.fun)\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "        tp_p2 = sp.optimize.minimize(self.threat_point_p2,p_initialize,bounds=bndsP1,constraints=con) #minimzer threat point p2\n",
    "        \n",
    "        print(\"Player 1 wants to minimize payoff Player 2 and plays:\", tp_p2.x)\n",
    "        print(\"Therefore the threat point of Player 2:\",tp_p2.fun)\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "        #here below we store the found threat point\n",
    "        self.threat_point = np.zeros(2) \n",
    "        self.threat_point[0] = tp_p1.fun\n",
    "        self.threat_point[1] = tp_p2.fun\n",
    "        \n",
    "        end_time = time.time() # stop the time!\n",
    "        print(\"Threat point found is:\",self.threat_point)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Running time algorithm:\",end_time-start_time)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "    def maximin_algorithm(self):\n",
    "        \"Maximin algorithm which calculates the lower boundary for the threat point\"\n",
    "        \n",
    "        print(\"Now looking for the maximin values for a player\")\n",
    "        \n",
    "        # initialize p and q\n",
    "        p_initialize_max = np.zeros(np.size(self.payoff_p1,0))\n",
    "        q_initialize_max = np.zeros(np.size(self.payoff_p2,1))\n",
    "        \n",
    "        def sum_con(x):\n",
    "            return 1 - np.sum(x)                               #constraint that all strategy frequencies should sum up to 1\n",
    "        \n",
    "        # initialize the boundaries\n",
    "        bndsP1 = []\n",
    "        bndsP2 = []\n",
    "        \n",
    "        # append the boundaries based on the number of actions\n",
    "        for i in np.nditer(np.arange(np.size(self.payoff_p1,0))):\n",
    "            bndsP1.append((0,1))\n",
    "            \n",
    "        for j in np.nditer(np.arange(np.size(self.payoff_p2,1))):\n",
    "            bndsP2.append((0,1))\n",
    "                                                                #player has to play a value between and including 0 and 1\n",
    "        con = [{'type': 'eq', 'fun': sum_con}]                 #defining the use of the summing to 1 constraint\n",
    "        \n",
    "        maxmin_p1 = sp.optimize.minimize(self.maximin_p1,p_initialize_max,bounds=bndsP1,constraints=con) # optimize for the maximin p1\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Player 1 plays:\", maxmin_p1.x)\n",
    "        print(\"maximin value p1:\", -maxmin_p1.fun)\n",
    "        \n",
    "        maxmin_p2 = sp.optimize.minimize(self.maximin_p2,q_initialize_max,bounds=bndsP2,constraints=con) # optimize for the maximin p2\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Player 2 plays:\", maxmin_p2.x)\n",
    "        print(\"maximin value p2:\", -maxmin_p2.fun)\n",
    "        print(\"\")\n",
    "        \n",
    "    def threat_point_optimized(self,points,show_strat_p1,show_strat_p2,print_text,FD_yn):\n",
    "        \"\"\"This function is the optimized threat point function, partly based on work done by Samuel and Joosten\"\"\"\n",
    "        \n",
    "        def random_strategy_draw(points,number_of_actions):\n",
    "            \"This function draws random strategies from a beta distribution, based on the number of points and actions\"\n",
    "            \n",
    "            # draw some strategies and normalize them\n",
    "            strategies_drawn = np.random.beta(0.5,0.5,(points,number_of_actions))\n",
    "            strategies_drawn = strategies_drawn/np.sum(strategies_drawn, axis=1).reshape([points,1])\n",
    "            \n",
    "            return strategies_drawn\n",
    "        \n",
    "        def frequency_pairs_p1(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"This function sorts the drawn random strategies based on best replies from p1\"\n",
    "            frequency_pairs = np.zeros((points*p1_actions,p1_actions*p2_actions))  # pre-allocate a matrix\n",
    "            \n",
    "            # arange the actions\n",
    "            p1_actions_range = np.arange(p1_actions)\n",
    "            p2_actions_range = np.arange(p2_actions)\n",
    "            \n",
    "            # sort the frequency pair based on the best replies for p1\n",
    "            for i in np.nditer(p1_actions_range):\n",
    "                for j in np.nditer(p2_actions_range):\n",
    "\n",
    "                    frequency_pairs[i*points:points*(i+1),(p2_actions*i)+j] = strategies_drawn[:,j]\n",
    "                    \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def frequency_pairs_p2(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"This function sorts the drawn random strategies based on best replies from p2\"\n",
    "\n",
    "            frequency_pairs = np.zeros((points*p2_actions,p1_actions*p2_actions)) #pre-allocate a matrix\n",
    "            \n",
    "            # arange the actions\n",
    "            p1_actions_range = np.arange(p1_actions)\n",
    "            p2_actions_range = np.arange(p2_actions)\n",
    "            \n",
    "            # sort the frequency pairs based on the best replies for p2\n",
    "             \n",
    "                    \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def payoffs_sorted(points,payoffs,actions):\n",
    "            \"This functions sorts the payoffs based on the frequency pairs, result can be used for minimax\"\n",
    "            \n",
    "            # create a range based on the number of points and actions\n",
    "            points_range = np.arange(points)\n",
    "            actions_range = np.arange(actions)\n",
    "        \n",
    "            payoffs_sort = np.zeros((points,actions)) # pre-allocate a matrix\n",
    "            \n",
    "            # sort the payoffs loop\n",
    "            for x in np.nditer(points_range):\n",
    "                for i in np.nditer(actions_range):\n",
    "                    payoffs_sort[x,i] = payoffs[points*i+x]\n",
    "            \n",
    "            return payoffs_sort\n",
    "                \n",
    "        if print_text == True:\n",
    "            print(\"The start of the algorithm for finding the threat point\")\n",
    "            print(\"First let's find the threat point for Player 1\")\n",
    "\n",
    "        \n",
    "        # Start of algorithm for player 1\n",
    "        \n",
    "        start_time = time.time() # start the time!\n",
    "        \n",
    "        # store the number of actions for both players\n",
    "        number_of_actions_p1 = self.payoff_p1.shape[0]\n",
    "        number_of_actions_p2 = self.payoff_p1.shape[1]\n",
    "        \n",
    "        #flatten the payoff for p1\n",
    "        payoff_p1flatten = self.payoff_p1.flatten()\n",
    "        payoff_p1 = payoff_p1flatten\n",
    "\n",
    "        y_punisher = random_strategy_draw(points,number_of_actions_p2) # draw random strategies for player 2\n",
    "        \n",
    "        size_game_p1 = self.payoff_p1.size #store the size of the game\n",
    "        \n",
    "        frequency_pairs = frequency_pairs_p1(points,number_of_actions_p2,number_of_actions_p1,y_punisher) # sort the frequency pairs based on best replies\n",
    "        \n",
    "        # if the FD function is active, activate it\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+2*frequency_pairs[:,2])-(2/3)*frequency_pairs[:,3] # NOTE: THIS IS THE FD FUNCTION FROM THE THESIS\n",
    "        else:\n",
    "            FD = 1\n",
    "        \n",
    "        # calculate the payoffs\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p1),axis=1)\n",
    "        payoffs = payoffs.flatten()\n",
    "        \n",
    "        # calculate new payoffs based on FD function\n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "        \n",
    "        payoff_sort_p1 = payoffs_sorted(points,payoffs,number_of_actions_p1) # sort the payoffs for determining the threat point\n",
    "        \n",
    "        threat_point_p1 = np.min(np.max(payoff_sort_p1,axis=1)) # determine the threat point\n",
    "        \n",
    "        # if text printing is active, print some text\n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"Threat point value is\",threat_point_p1)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # if strategy printing is active, print the strategies\n",
    "        if show_strat_p1 == True:\n",
    "            threat_point_indices_p1 = np.where(payoff_sort_p1 == threat_point_p1)\n",
    "            found_strategy_p1 = y_punisher[threat_point_indices_p1[0]]\n",
    "            fnd_strategy_p1 = found_strategy_p1.flatten()\n",
    "            print(\"Player 2 plays stationary strategy:\", fnd_strategy_p1)\n",
    "            print(\"While player 1 replies with a best pure reply of:\", self.best_pure_strategies[threat_point_indices_p1[1]])\n",
    "            \n",
    "        end_time = time.time()  # stop the time!\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"Seconds done to generate\", points, \"points\", end_time-start_time)\n",
    "            print(\"\")\n",
    "        \n",
    "        # End of algorithm player 1\n",
    "        \n",
    "        # Start of algorithm player 2\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"First start the threat point for player 2\")\n",
    "        start_time_p2 = time.time() # start the time for p2\n",
    "        \n",
    "        # flatten the payoffs for p2\n",
    "        payoff_p2flatten = self.payoff_p2.flatten()\n",
    "        payoff_p2 = payoff_p2flatten\n",
    "        \n",
    "        x_punisher = random_strategy_draw(points,number_of_actions_p1) # generate strategies for p1 to punish\n",
    "\n",
    "        frequency_pairs = frequency_pairs_p2(points,number_of_actions_p2,number_of_actions_p1,x_punisher) # generate best replies for p1\n",
    "        \n",
    "        # if FD_function is active, activate it\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+2*frequency_pairs[:,2])-(2/3)*frequency_pairs[:,3] # NOTE: FUNCTION FROM THE THESIS\n",
    "        else:\n",
    "            FD = 1\n",
    "        \n",
    "        # calculate the payoffs\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p2),axis=1)\n",
    "        payoffs = payoffs.flatten()\n",
    "        \n",
    "        # adjust the payoffs based on the FD function\n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "        \n",
    "        payoff_sort_p2 = payoffs_sorted(points,payoffs,number_of_actions_p2) # sort the payoffs for determining threat point\n",
    "    \n",
    "        threat_point_p2 = np.min(np.max(payoff_sort_p2,axis=1)) # determine the threat point\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"Threat point value is\",threat_point_p2)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "        \n",
    "        if show_strat_p2 == True:\n",
    "            threat_point_indices_p2 = np.where(payoff_sort_p2 == threat_point_p2)\n",
    "            found_strategy = x_punisher[threat_point_indices_p2[0]]\n",
    "            fnd_strategy = found_strategy.flatten()\n",
    "            print(\"Player 1 plays stationairy strategy:\", fnd_strategy)\n",
    "            print(\"While player 2 replies with a best pure reply of:\", self.best_pure_strategies[threat_point_indices_p2[1]])\n",
    "            \n",
    "        end_time_p2 = time.time() # end the timer\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"Seconds done to generate\", points, \"points\", end_time_p2-start_time_p2)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "        \n",
    "        # store the threat point\n",
    "        self.threat_point = np.zeros(2) \n",
    "        self.threat_point[0] = threat_point_p1\n",
    "        self.threat_point[1] = threat_point_p2\n",
    "        \n",
    "        return [threat_point_p1,threat_point_p2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threat point algorithm initiated\n",
      "Player 2 wants to minimize payoff Player 1 and plays: [0. 1.]\n",
      "Therefore the threat point of Player 1: 24.0\n",
      "\n",
      "Player 1 wants to minimize payoff Player 2 and plays: [0. 1.]\n",
      "Therefore the threat point of Player 2: 24.0\n",
      "\n",
      "Threat point found is: [24. 24.]\n",
      "\n",
      "\n",
      "Running time algorithm: 0.002999544143676758\n",
      "\n",
      "\n",
      "Now looking for the maximin values for a player\n",
      "\n",
      "Player 1 plays: [5.21804822e-15 1.00000000e+00]\n",
      "maximin value p1: 23.999999999999943\n",
      "\n",
      "Player 2 plays: [5.21804822e-15 1.00000000e+00]\n",
      "maximin value p2: 23.999999999999943\n",
      "\n",
      "The start of the algorithm for finding the threat point\n",
      "First let's find the threat point for Player 1\n",
      "\n",
      "\n",
      "Threat point value is 24.000000005861843\n",
      "\n",
      "\n",
      "Player 2 plays stationary strategy: [1.46546023e-09 9.99999999e-01]\n",
      "While player 1 replies with a best pure reply of: [[0 1 1 0]]\n",
      "Seconds done to generate 100000 points 1.0372252464294434\n",
      "\n",
      "\n",
      "\n",
      "First start the threat point for player 2\n",
      "\n",
      "\n",
      "Threat point value is 24.000000000217366\n",
      "\n",
      "\n",
      "Player 1 plays stationairy strategy: [5.43418783e-11 1.00000000e+00]\n",
      "While player 2 replies with a best pure reply of: [[0 1 1 0]]\n",
      "\n",
      "Seconds done to generate 100000 points 0.9973814487457275\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here below we create a repeated game from the Llea thesis\n",
    "a = RepeatedGame(np.matrix('16 14;28 24'),np.matrix('16 28; 14 24'))\n",
    "a.threat_point_algorithm()\n",
    "a.plot_pure_reward_points()\n",
    "a.plot_all_reward_points(False)\n",
    "a.maximin_algorithm()\n",
    "a.threat_point_optimized(100000,True,True,True,False)\n",
    "a.plot_threat_point()\n",
    "a.plot_threat_point_lines()\n",
    "\n",
    "\n",
    "\n",
    "# Self build repeated game which contains two pure Nash Equilibriums in which the Pareto inferior option is a threat point\n",
    "# b = RepeatedGame(np.matrix('6 9; 3 12'),np.matrix('9 6; 12 18'))\n",
    "# b.threat_point_algorithm()\n",
    "# b.plot_pure_reward_points()\n",
    "# b.plot_all_reward_points(False)\n",
    "# b.plot_threat_point()\n",
    "# b.maximin_algorithm()\n",
    "\n",
    "# Self build repeated game containing no pure Nash Equilibria\n",
    "# c = RepeatedGame(np.matrix('7 2; 6 3'),np.matrix('10 15; 14 13'))\n",
    "# c.threat_point_algorithm()\n",
    "# c.plot_pure_reward_points()\n",
    "# c.plot_all_reward_points(False)\n",
    "# c.plot_threat_point()\n",
    "# c.maximin_algorithm()\n",
    "\n",
    "\n",
    "#Game discussed during meeting with Joosten\n",
    "# d = RepeatedGame(np.matrix('16 0; 14 24'),np.matrix('16 0; 14 24'))\n",
    "# d.threat_point_algorithm()\n",
    "# d.plot_pure_reward_points()\n",
    "# d.plot_all_reward_points(False)\n",
    "# d.plot_threat_point()\n",
    "# d.maximin_algorithm()\n",
    "\n",
    "#Identity game\n",
    "# e = RepeatedGame(np.matrix('1 0; 0 1'),np.matrix('1 0; 0 1'))\n",
    "# e.threat_point_algorithm()\n",
    "# e.plot_pure_reward_points()\n",
    "# e.plot_all_reward_points(False)\n",
    "# e.plot_threat_point()\n",
    "# e.maximin_algorithm()\n",
    "\n",
    "# Random game code (disabled by default)\n",
    "# RandP1 = np.random.randint(25, size=(4,4))\n",
    "# RandP2 = np.random.randint(25, size=(4,4))\n",
    "\n",
    "# RandP110 = np.random.randint(25, size=(10,10))\n",
    "# RandP210 = np.random.randint(25, size=(10,10))\n",
    "\n",
    "# g = RepeatedGame(RandP1,RandP2)\n",
    "# g.threat_point_algorithm()\n",
    "# g.maximin_algorithm()\n",
    "# g.threat_point_optimized(1000000,False,False,True,False)\n",
    "\n",
    "# h = RepeatedGame(RandP110,RandP210)\n",
    "# h.threat_point_algorithm()\n",
    "# h.maximin_algorithm()\n",
    "# h.threat_point_optimized(1000000,False,False,True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGame:\n",
    "    \"\"\"Ïn this class we model Stochastic Games, also known in the thesis as Type II games\"\"\"\n",
    "    \n",
    "    def __init__(self,payoff_p1_game1,payoff_p2_game1,payoff_p1_game2,payoff_p2_game2,trmatrixg1,trmatrixg2,trmatrixg3,trmatrixg4):\n",
    "        \"Here below we initialize the game by storing payoff and transition matrices according to the upper input.\"\n",
    "        self.payoff_p1_game1 = payoff_p1_game1                  #payoff p1 in game 1\n",
    "        self.payoff_p2_game1 = payoff_p2_game1                  #payoff p2 in game 1\n",
    "         \n",
    "        self.payoff_p1_game2 = payoff_p1_game2                  #payoff p1 in game 2\n",
    "        self.payoff_p2_game2 = payoff_p2_game2                  #payoff p2 in game 2\n",
    "        \n",
    "        self.transition_matrix_game1_to1 = trmatrixg1           #transition matrix from game 1 to game 1\n",
    "        self.transition_matrix_game2_to1 = trmatrixg2           #transition matrix from game 2 to game 1\n",
    "         \n",
    "        self.transition_matrix_game1_to2 = trmatrixg3           #transition matrix from game 1 to game 2\n",
    "        self.transition_matrix_game2_to2 = trmatrixg4           #transition matrix from game 2 to game 2\n",
    "        \n",
    "        self.printing = False   #set printing to False\n",
    "        \n",
    "    def plot_single_period_pure_rewards(self):\n",
    "        \"Here we plot the pure rewards possible for a single period\"\n",
    "        \n",
    "        plt.figure()                                            #create a figure\n",
    "        payoff_p1_g1_flat = self.payoff_p1_game1.A1             #create a flattend payoff of p1 in game 1\n",
    "        payoff_p2_g1_flat = self.payoff_p2_game1.A1             #create a flattend payoff of p2 in game 1\n",
    "        plt.scatter(payoff_p1_g1_flat,payoff_p2_g1_flat, label=\"Pure reward points Game 1\", zorder = 15) #plot payoffs game 1\n",
    "     \n",
    "        payoff_p1_g2_flat = self.payoff_p1_game2.A1             #create a flattend payoff of p1 in game 2\n",
    "        payoff_p2_g2_flat = self.payoff_p2_game2.A1             #and for p2 in game 2\n",
    "        plt.scatter(payoff_p1_g2_flat,payoff_p2_g2_flat, label=\"Pure reward points Game 2\", zorder = 15)  #plotting this again\n",
    "        \n",
    "        plt.xlabel(\"Reward Player 1\")                           #giving the x-axis the label of payoff p1\n",
    "        plt.ylabel(\"Reward Player 2\")                           #and the payoff of the y-axis is that of p2\n",
    "        plt.title(\"Reward points of FD Type II game\")      #and we give it a nice titel\n",
    "        plt.legend()\n",
    "        \n",
    "    def plot_convex_hull_pure_rewards(self):\n",
    "        \"Here we plot a convex hull around the pure reward point, therefore resulting in the total possible reward space\"\n",
    "        \n",
    "        payoff_p1_g1_flat = self.payoff_p1_game1.A1       #store the flattend payoff of p1 game 1\n",
    "        payoff_p2_g1_flat = self.payoff_p2_game1.A1       #store the flattend payoff of p2 game 1\n",
    "        \n",
    "        payoff_p1_g2_flat = self.payoff_p1_game2.A1       #store the flattend payoff of p1 game 2\n",
    "        payoff_p2_g2_flat = self.payoff_p2_game2.A1       #store the flattend payoff of p2 game 2\n",
    "        \n",
    "        payoff_p1_merged = np.concatenate((payoff_p1_g1_flat,payoff_p1_g2_flat))  #merge p1 payoffs\n",
    "        payoff_p2_merged = np.concatenate((payoff_p2_g1_flat,payoff_p2_g2_flat))  #merge p2 payoffs\n",
    "        \n",
    "        all_payoffs = np.array([payoff_p1_merged,payoff_p2_merged])  #create one array of payoffs\n",
    "        all_payoffs = np.transpose(all_payoffs)                      #and rotate this one\n",
    "        \n",
    "        rewards_convex_hull = ConvexHull(all_payoffs)                #retain the convex hull of the payoffs\n",
    "        plt.fill(all_payoffs[rewards_convex_hull.vertices,0], all_payoffs[rewards_convex_hull.vertices,1], color='k')\n",
    "        plt.title(\"Convex hull of payoffs\")\n",
    "        \n",
    "        #here above we fill the convex hull in black\n",
    "\n",
    "    def plot_threat_point(self):\n",
    "        \"This function plots the threat point of the game\" \n",
    "        plt.scatter(self.threat_point[0],self.threat_point[1], zorder=10, color = 'r', label='Threat point')\n",
    "        plt.legend()\n",
    "        \n",
    "    \n",
    "    def plot_threat_point_lines(self):\n",
    "        \"This function plots the threat point lines defining the NE borders\"\n",
    "        \n",
    "        plt.plot([self.threat_point[0],self.threat_point[0]],[self.threat_point[1],self.maximal_payoffs[1]], color='k', zorder=15)\n",
    "        plt.plot([self.threat_point[0],self.maximal_payoffs[0]],[self.threat_point[1],self.threat_point[1]], color='k', zorder=15)\n",
    "        plt.axis('equal')\n",
    "        plt.savefig('TypeIIFD.png', dpi=400)\n",
    "        \n",
    "    def plot_all_reward_points(self,FD_yn):\n",
    "        \"Here we use the algorithm developed in the thesis of Llea with supervision of Joosten for Type 2 games\"\n",
    "        \n",
    "        ###Payoffs and probabilitys\n",
    "        payoff_p1_g1_flat = self.payoff_p1_game1.A1          #flatten payoff p1 game 1\n",
    "        payoff_p2_g1_flat = self.payoff_p2_game1.A1          #flatten payoff p2 game 1\n",
    "        \n",
    "        payoff_p1_g2_flat = self.payoff_p1_game2.A1          #flatten payoff p1 game 2\n",
    "        payoff_p2_g2_flat = self.payoff_p2_game2.A1          #flatten payoff p2 game 2\n",
    "        \n",
    "        A1 = np.concatenate((payoff_p1_g1_flat,payoff_p1_g2_flat))   #A1 is payoff Player 1\n",
    "        B1 = np.concatenate((payoff_p2_g1_flat,payoff_p2_g2_flat))   #B1 is payoff Player 2\n",
    "        \n",
    "        trans_game1_1_flat = self.transition_matrix_game1_to1.A1  #flatten transition probabilities game 1 to 1\n",
    "        trans_game2_1_flat = self.transition_matrix_game2_to1.A1  #flatten transition probabilities game 2 to 1\n",
    "        \n",
    "        p = np.concatenate((trans_game1_1_flat,trans_game2_1_flat)) #merge them into one transition array\n",
    "\n",
    "\n",
    "        ###Intialization of variables\n",
    "        T = 100000                        #number of points to generate\n",
    "\n",
    "        x = np.zeros(8)                   #preallocate frequency array\n",
    "        xstar = np.zeros(8)               #preallocate converged frequency array\n",
    "        r = np.zeros(8)                   #preallocate random samples from beta distribution\n",
    "        y = np.zeros(8)                   #preallocate intermediate vector\n",
    "        yp = np.zeros(4)                  #preallocate intermediate vector with transition\n",
    "        yp_not = np.zeros(4)              #preallocate intermediate vector with transition\n",
    "        v_p1 = np.zeros(8)                #preallocate intermediate payoffs p1\n",
    "        v_p2 = np.zeros(8)                #preallocate intermediate payoffs p2\n",
    "        payoff_p1 = np.zeros(T)           #preallocate definite payoffs p1\n",
    "        payoff_p2 = np.zeros(T)           #preallocate definite payoffs p2\n",
    "        \n",
    "        for t in range(0,T):              #sum over the number of points to generate\n",
    "            for i in range(0,8):\n",
    "                r[i] = np.random.beta(0.5,0.5)      #generate random frequency pair\n",
    "\n",
    "            for i in range(0,8):\n",
    "                x[i] = r[i]/np.sum(r)               #normalize the frequency pair so it sums to 1\n",
    "\n",
    "        ###intermediate calculations (flow equations)\n",
    "            for i in range(0,4):\n",
    "                y[i] = x[i]/np.sum(x[0:4])          #calculate intermediate vector\n",
    "\n",
    "            for i in range(4,8):\n",
    "                y[i] = x[i]/np.sum(x[4:8])          #see above\n",
    "\n",
    "            for i in range(0,4):\n",
    "                yp_not[i] = y[i] * (1-p[i])         #prepare for Q calculations\n",
    "                yp[i] = y[i+4]*p[i+4]\n",
    "\n",
    "            Q = np.sum(yp)/(np.sum(yp)+np.sum(yp_not))   #calculate Q and Qnot\n",
    "            Q_not = 1-Q\n",
    "\n",
    "        ###Solve for X\n",
    "            for i in range(0,4):\n",
    "                xstar[i] = Q*y[i]                   #now calculate converged frequency pairs\n",
    "\n",
    "            for i in range(4,8):\n",
    "                xstar[i] = Q_not*y[i]               #and for second game\n",
    "                \n",
    "            if FD_yn == True:\n",
    "                FD = 1-0.25*(xstar[1]+xstar[2])-(1/3)*xstar[3]-(1/2)*(xstar[5] + xstar[6]) - (2/3) * xstar[7]\n",
    "            else:\n",
    "                FD = 1 \n",
    "                \n",
    "            for i in range(0,8):\n",
    "                v_p1[i] = xstar[i]*A1[i]            #calculate payoffs player1\n",
    "                v_p2[i] = xstar[i]*B1[i]            #calculate payoffs player2\n",
    "\n",
    "\n",
    "        ### Stage payoff vectors\n",
    "            payoff_p1[t] = FD*np.sum(v_p1)            #result is one payoff of player 1\n",
    "            payoff_p2[t] = FD*np.sum(v_p2)            #result is one payoff of player 2\n",
    "            \n",
    "        all_payoffs = np.array([payoff_p1,payoff_p2])  #payoffs player 1 and and p2 merging\n",
    "        all_payoffs = np.transpose(all_payoffs)        #transpose for use in convex_hull\n",
    "        Convex_Hull_Payoffs = ConvexHull(all_payoffs)  #calculate convex_hull of the payoffs\n",
    "        \n",
    "        self.maximal_payoffs = np.zeros(2)\n",
    "        self.maximal_payoffs[0] = np.max(payoff_p1)\n",
    "        self.maximal_payoffs[1] = np.max(payoff_p2)\n",
    "        \n",
    "        #here below we fill the convex_hull of the payoffs and plot it\n",
    "        plt.fill(all_payoffs[Convex_Hull_Payoffs.vertices,0], all_payoffs[Convex_Hull_Payoffs.vertices,1], color='y', zorder=5, label=\"Obtainable rewards\")\n",
    "    \n",
    "    def markov_decision_process_p1(self,x):\n",
    "        \"Here we run the threat point algorithm for P1 with fixed strategy for the punisher and the punished one maximizes this MDP\"\n",
    "        \n",
    "        def chance_multiplication(self,x):\n",
    "            \"In this function we compute the new transition probabilities based on chosen actions\"\n",
    "            \n",
    "            # store the actions of the players\n",
    "            actions_p1_game1 = self.payoff_p1_game1.shape[0]\n",
    "            actions_p1_game2 = self.payoff_p1_game2.shape[0]            \n",
    "            actions_p2_game1 = self.payoff_p1_game1.shape[1]\n",
    "            actions_p2_game2 = self.payoff_p1_game2.shape[1]\n",
    "            \n",
    "            length_of_actions = np.max([actions_p1_game1,actions_p1_game2]) # determine the maximum length\n",
    "            \n",
    "            p1_combined = np.zeros(length_of_actions, dtype=object) # prepare an object for the probabilities\n",
    "            \n",
    "            # in the loop below we calculate the new transition probabilities\n",
    "            for i in np.nditer(np.arange(length_of_actions)):\n",
    "                working_probs = np.zeros((2,2))\n",
    "                \n",
    "                working_probs[0,0] = np.asscalar(np.dot(self.transition_matrix_game1_to1[i,:],x[0:actions_p2_game1]))\n",
    "                working_probs[0,1] = np.asscalar(np.dot(self.transition_matrix_game1_to2[i,:],x[0:actions_p2_game1])) \n",
    "                working_probs[1,0] = np.asscalar(np.dot(self.transition_matrix_game2_to1[i,:],x[actions_p2_game1:actions_p2_game1+actions_p2_game2]))\n",
    "                working_probs[1,1] = np.asscalar(np.dot(self.transition_matrix_game2_to2[i,:],x[actions_p2_game1:actions_p2_game1+actions_p2_game2]))\n",
    "                \n",
    "                p1_combined[i] = working_probs/np.sum(working_probs,1)\n",
    "\n",
    "            return p1_combined\n",
    "\n",
    "        def reward_multiplication(self,x):\n",
    "            \"This function calculates the adjusted rewards\"\n",
    "            \n",
    "            # initialize empty arrays\n",
    "            rewards_game1 = []\n",
    "            rewards_game2 = []\n",
    "            \n",
    "            # store the actions for p2\n",
    "            actions_p2_game1 = self.payoff_p1_game1.shape[1]\n",
    "            actions_p2_game2 = self.payoff_p1_game2.shape[1]\n",
    "            \n",
    "            # calculate the rewards for game 1\n",
    "            for i in np.nditer(np.arange(self.payoff_p1_game1.shape[0])):\n",
    "                rewards_game1.append(np.asscalar(np.dot(self.payoff_p1_game1[i,:],x[0:actions_p2_game1])))\n",
    "            \n",
    "            # calculate the rewards for game 2\n",
    "            for i in np.nditer(np.arange(self.payoff_p1_game2.shape[0])):\n",
    "                rewards_game2.append(np.asscalar(np.dot(self.payoff_p1_game2[i,:],x[actions_p2_game1:actions_p2_game1+actions_p2_game2])))\n",
    "            \n",
    "            Reward = np.array([rewards_game1,rewards_game2]) # Combine the rewards in one array\n",
    "    \n",
    "            return Reward\n",
    "        \n",
    "        p1_combined = chance_multiplication(self,x) # run the transition probability function\n",
    "        Reward_matrix = reward_multiplication(self,x) # run the rewards function\n",
    "        \n",
    "        # Run the MDP process to find the threat point for P1\n",
    "        mdp_threatpointp1 = mdptoolbox.mdp.RelativeValueIteration(p1_combined, Reward_matrix, epsilon=0.00000000001)\n",
    "        mdp_threatpointp1.setSilent()\n",
    "        mdp_threatpointp1.run()\n",
    "\n",
    "        self.threatpoint_p1_policy = mdp_threatpointp1.policy # store the policy (strategy) found\n",
    "\n",
    "        if self.printing == True:\n",
    "            print(\"Running MDP\")\n",
    "            print(\"With X as\",x)\n",
    "            print(\"Resulting in:\")\n",
    "            print(mdp_treatpointp1.average_reward)\n",
    "            print(\"With policy:\")\n",
    "            print(mdp_treatpointp1.policy)\n",
    "\n",
    "\n",
    "        return mdp_threatpointp1.average_reward\n",
    "        \n",
    "\n",
    "    def markov_decision_process_p2(self,x):\n",
    "        \"This function creates a MDP for the second player to determine the threat point\"\n",
    "        \n",
    "        def chance_multiplication(self,x):\n",
    "            \"Calculates the transitions probabilities for the MDP\"\n",
    "            \n",
    "            # store the actions of the players\n",
    "            actions_p1_game1 = self.payoff_p1_game1.shape[0]\n",
    "            actions_p1_game2 = self.payoff_p1_game2.shape[0]            \n",
    "            actions_p2_game1 = self.payoff_p2_game1.shape[1]\n",
    "            actions_p2_game2 = self.payoff_p2_game2.shape[1]\n",
    "\n",
    "            length_of_actions = np.max([actions_p2_game1,actions_p2_game2]) # determine the maximum length of the actions\n",
    "\n",
    "            p2_combined = np.zeros(length_of_actions, dtype=object) # initialize an object for storing the probabilities\n",
    "            \n",
    "            # in this loop we calculate the probabilities and return them\n",
    "            for i in np.nditer(np.arange(length_of_actions)):\n",
    "                working_probs = np.zeros((2,2))\n",
    "\n",
    "                working_probs[0,0] = np.asscalar(np.dot(self.transition_matrix_game1_to1[:,i],x[0:actions_p1_game1]))\n",
    "                working_probs[0,1] = np.asscalar(np.dot(self.transition_matrix_game1_to2[:,i],x[0:actions_p1_game1])) \n",
    "                working_probs[1,0] = np.asscalar(np.dot(self.transition_matrix_game2_to1[:,i],x[actions_p1_game1:actions_p1_game1+actions_p1_game2]))\n",
    "                working_probs[1,1] = np.asscalar(np.dot(self.transition_matrix_game2_to2[:,i],x[actions_p1_game1:actions_p1_game1+actions_p1_game2]))\n",
    "\n",
    "                p2_combined[i] = working_probs/np.sum(working_probs,1)\n",
    "\n",
    "            return p2_combined\n",
    "\n",
    "        def reward_multiplication(self,x):\n",
    "            \"Calculate the rewards for the game\"\n",
    "            \n",
    "            # initialize and store the rewards\n",
    "            rewards_game1 = []\n",
    "            rewards_game2 = []\n",
    "            \n",
    "            # store the actions of p2\n",
    "            actions_p1_game1 = self.payoff_p2_game1.shape[0]\n",
    "            actions_p1_game2 = self.payoff_p2_game2.shape[0]\n",
    "            \n",
    "            # calculate the rewards of the first game\n",
    "            for i in np.nditer(np.arange(self.payoff_p1_game1.shape[0])):\n",
    "                rewards_game1.append(np.asscalar(np.dot(self.payoff_p2_game1[:,i],x[0:actions_p1_game1])))\n",
    "            \n",
    "            # calculate the rewards of the second game\n",
    "            for i in np.nditer(np.arange(self.payoff_p1_game2.shape[0])):\n",
    "                rewards_game2.append(np.asscalar(np.dot(self.payoff_p2_game2[:,i],x[actions_p1_game1:actions_p1_game1+actions_p1_game2])))\n",
    "\n",
    "            Reward = np.array([rewards_game1,rewards_game2]) # combine the rewards of both games\n",
    "\n",
    "            return Reward\n",
    "\n",
    "        p2_combined = chance_multiplication(self,x) # run the chance function\n",
    "        Reward2 = reward_multiplication(self,x) # run the rewards function\n",
    "        \n",
    "        # Run the MDP for P2 and calculate his threat point\n",
    "        mdp_threatpointp2 = mdptoolbox.mdp.RelativeValueIteration(p2_combined, Reward2, epsilon=0.00000000001)\n",
    "        mdp_threatpointp2.setSilent()\n",
    "        mdp_threatpointp2.run()\n",
    "\n",
    "        self.threatpoint_p2_policy = mdp_threatpointp2.policy # store the policy of p2 (best strategy)\n",
    "\n",
    "        if self.printing == True:\n",
    "            print(\"Running MDP\")\n",
    "            print(\"With X as\",x)\n",
    "            print(\"Resulting in:\")\n",
    "            print(mdp_treatpointp2.average_reward)\n",
    "            print(\"With policy:\")\n",
    "            print(mdp_treatpointp2.policy)\n",
    "\n",
    "        return mdp_threatpointp2.average_reward\n",
    "       \n",
    "    def threat_point_algorithm(self,T,sensi):\n",
    "        \"This algorithm calculates the threat point in a two-player game with\"\n",
    "        \n",
    "        # set the shape of the actions for p1\n",
    "        x_shape_p1_g1 = self.payoff_p1_game1.shape[0]\n",
    "        x_shape_p1_g2 = self.payoff_p1_game2.shape[0]\n",
    "        \n",
    "        # set the shape of the actions for p2\n",
    "        x_shape_p2_g1 = self.payoff_p2_game1.shape[1]\n",
    "        x_shape_p2_g2 = self.payoff_p2_game2.shape[1]\n",
    "        \n",
    "        # combine the shapes\n",
    "        x_shape_p1 = x_shape_p1_g1 + x_shape_p1_g2\n",
    "        x_shape_p2 = x_shape_p2_g1 + x_shape_p2_g2\n",
    "        \n",
    "        # initialiaze the strategies\n",
    "        xtry = np.zeros(4)\n",
    "        xtry_p1 = np.zeros(x_shape_p1)\n",
    "        xtry_p2 = np.zeros(x_shape_p2)\n",
    "        \n",
    "        # initialize the MDP values\n",
    "        tried_mdp_p1 = 0\n",
    "        stored_value_p1 = 0\n",
    "\n",
    "        tried_mdp_p2 = 0\n",
    "        stored_value_p2 = 0\n",
    "        \n",
    "        # initialize storage of best threat strategy\n",
    "        x_best_p1 = np.zeros(x_shape_p1)\n",
    "        x_best_p2 = np.zeros(x_shape_p2)\n",
    "        \n",
    "        new_time = time.time() # start the timer!\n",
    "            \n",
    "        for i in np.nditer(np.arange(T)): #loop over the number of to generate points\n",
    "            \n",
    "            # generate random strategies for p1 and p2\n",
    "            xtry_p1 = np.random.beta(0.5,0.5,x_shape_p1)\n",
    "            xtry_p2 = np.random.beta(0.5,0.5,x_shape_p2)\n",
    "            \n",
    "            # normalize these strategies for both p1 and p2\n",
    "            xtry_p1[0:x_shape_p1_g1] = xtry_p1[0:x_shape_p1_g1]/np.sum(xtry_p1[0:x_shape_p1_g1])\n",
    "            xtry_p1[x_shape_p1_g1:x_shape_p1] = xtry_p1[x_shape_p1_g1:x_shape_p1]/np.sum(xtry_p1[x_shape_p1_g1:x_shape_p1])\n",
    "            \n",
    "            xtry_p2[0:x_shape_p2_g1] = xtry_p2[0:x_shape_p2_g1]/np.sum(xtry_p2[0:x_shape_p2_g1])\n",
    "            xtry_p2[x_shape_p2_g1:x_shape_p2] = xtry_p2[x_shape_p2_g1:x_shape_p2]/np.sum(xtry_p2[x_shape_p2_g1:x_shape_p2])\n",
    "            \n",
    "            # run the MDP for both P1 and P2\n",
    "            tried_mdp_p1 = self.markov_decision_process_p1(xtry_p1)\n",
    "            tried_mdp_p2 = self.markov_decision_process_p2(xtry_p2)\n",
    "            \n",
    "            if i == 0: # if it is the first run, just store the values\n",
    "                stored_value_p1 = tried_mdp_p1\n",
    "                stored_value_p2 = tried_mdp_p2\n",
    "                x_best_p1 = xtry_p1            \n",
    "                x_best_p2 = xtry_p2\n",
    "\n",
    "            else: # if not, check if we have found lower values for both MDP's and store them if so\n",
    "                if tried_mdp_p1 < stored_value_p1:\n",
    "                    stored_value_p1 = tried_mdp_p1\n",
    "                    x_best_p1 = xtry_p1\n",
    "                    \n",
    "                elif tried_mdp_p2 < stored_value_p2:\n",
    "                    stored_value_p2 = tried_mdp_p2\n",
    "                    x_best_p2 = xtry_p2\n",
    "        \n",
    "        # print the preliminary results\n",
    "        print(\"Rough value found =\",stored_value_p1)\n",
    "        print(\"With best threaten strategy:\",x_best_p1)\n",
    "\n",
    "        print(\"For p2 =\",stored_value_p2)\n",
    "        print(\"With strategy\",x_best_p2)\n",
    "        print(\"Found within (seconds): \",time.time() - new_time)\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "        print(\"Now let's find a more precise point by generating more precise points in an interval\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # again initialize the best threat strategies\n",
    "        updatex_p1 = np.zeros(x_shape_p1)\n",
    "        updatex_p2 = np.zeros(x_shape_p2)\n",
    "        \n",
    "        # loop again for better results\n",
    "        for i in np.nditer(np.arange(T)):\n",
    "            \n",
    "            # calculate updates to the found best threat strategies\n",
    "            updatex_p1 = x_best_p1-sensi+((x_best_p1+sensi)-x_best_p1)*np.random.beta(0.5,0.5,x_shape_p1)\n",
    "            updatex_p2 = x_best_p2-sensi+((x_best_p2+sensi)-x_best_p2)*np.random.beta(0.5,0.5,x_shape_p2)\n",
    "            \n",
    "            # fail safe in case any action becomes negative\n",
    "            for j in np.nditer(np.arange(x_shape_p1)):\n",
    "                if updatex_p1[j] < 0:\n",
    "                    updatex_p1[j] = -updatex_p1[j]\n",
    "            \n",
    "            for j in np.nditer(np.arange(x_shape_p2)):\n",
    "                if updatex_p2[j] < 0:\n",
    "                    updatex_p2[j] = -updatex_p2[j]\n",
    "            \n",
    "            # normalize the strategies\n",
    "            updatex_p1[0:x_shape_p1_g1] = updatex_p1[0:x_shape_p1_g1]/np.sum(updatex_p1[0:x_shape_p1_g1])\n",
    "            updatex_p1[x_shape_p1_g1:x_shape_p1] = updatex_p1[x_shape_p1_g1:x_shape_p1]/np.sum(updatex_p1[x_shape_p1_g1:x_shape_p1])\n",
    "\n",
    "            updatex_p2[0:x_shape_p2_g1] = updatex_p2[0:x_shape_p2_g1]/np.sum(updatex_p2[0:x_shape_p2_g1])\n",
    "            updatex_p2[x_shape_p2_g1:x_shape_p2] = updatex_p2[x_shape_p2_g1:x_shape_p2]/np.sum(updatex_p2[x_shape_p2_g1:x_shape_p2])\n",
    "            \n",
    "            # run the MDP for new chosen strategies\n",
    "            new_try_mdp_p1 = self.markov_decision_process_p1(updatex_p1)\n",
    "            new_try_mdp_p2 = self.markov_decision_process_p2(updatex_p2)\n",
    "            \n",
    "            # if new found values are lower then store them\n",
    "            if new_try_mdp_p1 < stored_value_p1:\n",
    "                stored_value_p1 = new_try_mdp_p1\n",
    "                x_best_p1 = updatex_p1\n",
    "\n",
    "            elif new_try_mdp_p2 < stored_value_p2:\n",
    "                stored_value_p2 = new_try_mdp_p2\n",
    "                x_best_p2 = updatex_p2\n",
    "\n",
    "\n",
    "        # print the threat point\n",
    "        print(\"New value found =\",stored_value_p1)\n",
    "        print(\"With best strategy =\",x_best_p1)\n",
    "        print(\"\")\n",
    "        print(\"For p2 =\",stored_value_p2)\n",
    "        print(\"With strategy\",x_best_p2)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"End of algorithm\")\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "\n",
    "        self.threat_point = [stored_value_p1,stored_value_p2] # store the threat point\n",
    "   \n",
    "    def markov_try_out_max_p1(self,x):\n",
    "        \"Maximize the payoff by using negative values\"\n",
    "        \n",
    "        def chance_multiplication(self,x):\n",
    "            \"This function calculates the transition probabilities\"\n",
    "            \n",
    "            # this function stores the actions of the players\n",
    "            actions_p1_game1 = self.payoff_p1_game1.shape[0]\n",
    "            actions_p1_game2 = self.payoff_p1_game2.shape[0]            \n",
    "            actions_p2_game1 = self.payoff_p2_game1.shape[1]\n",
    "            actions_p2_game2 = self.payoff_p2_game2.shape[1]\n",
    "\n",
    "            length_of_actions = np.max([actions_p2_game1,actions_p2_game2]) # determine the maximum length of the actions\n",
    "\n",
    "            p2_combined = np.zeros(length_of_actions, dtype=object) # initialize an empty object\n",
    "            \n",
    "            # calculate the transition probabilities based on chosen actions\n",
    "            for i in np.nditer(np.arange(length_of_actions)):\n",
    "                working_probs = np.zeros((2,2))\n",
    "\n",
    "                working_probs[0,0] = np.asscalar(np.dot(self.transition_matrix_game1_to1[:,i],x[0:actions_p1_game1]))\n",
    "                working_probs[0,1] = np.asscalar(np.dot(self.transition_matrix_game1_to2[:,i],x[0:actions_p1_game1])) \n",
    "                working_probs[1,0] = np.asscalar(np.dot(self.transition_matrix_game2_to1[:,i],x[actions_p1_game1:actions_p1_game1+actions_p1_game2]))\n",
    "                working_probs[1,1] = np.asscalar(np.dot(self.transition_matrix_game2_to2[:,i],x[actions_p1_game1:actions_p1_game1+actions_p1_game2]))\n",
    "\n",
    "                p2_combined[i] = working_probs/np.sum(working_probs,1)\n",
    "\n",
    "            return p2_combined    \n",
    "    \n",
    "        \n",
    "        p2_combined_max = chance_multiplication(self,x) # run the chance multiplication function\n",
    "        \n",
    "        def reward_multiplication(self,x):\n",
    "            \"This function computes the reward matrix based on the input strategy\"\n",
    "            \n",
    "            # initialize the reewards for game 1 and game 2\n",
    "            rewards_game1 = []\n",
    "            rewards_game2 = []\n",
    "            \n",
    "            # store the actions for p1 in game 1 and 2\n",
    "            actions_p1_game1 = self.payoff_p1_game1.shape[0]\n",
    "            actions_p1_game2 = self.payoff_p1_game2.shape[0]\n",
    "            \n",
    "            # calculate the new rewards for game 1\n",
    "            for i in np.nditer(np.arange(self.payoff_p1_game1.shape[0])):\n",
    "                rewards_game1.append(np.asscalar(-np.dot(self.payoff_p1_game1[:,i],x[0:actions_p1_game1])))\n",
    "            \n",
    "            # calculate the new rewards for game 2\n",
    "            for i in np.nditer(np.arange(self.payoff_p1_game2.shape[0])):\n",
    "                rewards_game2.append(np.asscalar(-np.dot(self.payoff_p1_game2[:,i],x[actions_p1_game1:actions_p1_game1+actions_p1_game2])))\n",
    "\n",
    "            Reward = np.array([rewards_game1,rewards_game2]) # store them in one array\n",
    "\n",
    "            return Reward\n",
    "        \n",
    "        Reward2_max = reward_multiplication(self,x) # run the reward function\n",
    "        \n",
    "        # run the MDP which computes the minimax for p1\n",
    "        mdp_threatpoint_minp1 = mdptoolbox.mdp.RelativeValueIteration(p2_combined_max, Reward2_max, epsilon=0.00000000001)\n",
    "        mdp_threatpoint_minp1.setSilent()\n",
    "        mdp_threatpoint_minp1.run()\n",
    "\n",
    "        self.threatpoint_p1_min_policy = mdp_threatpoint_minp1.policy # store the policy\n",
    "        \n",
    "        # print the result\n",
    "        if self.printing == True:\n",
    "            print(\"Running MDP\")\n",
    "            print(\"With X as\",x)\n",
    "            print(\"Resulting in:\")\n",
    "            print(mdp_treatpoint_minp1.average_reward)\n",
    "            print(\"With policy:\")\n",
    "            print(mdp_treatpoint_minp1.policy)\n",
    "\n",
    "        return mdp_threatpoint_minp1.average_reward\n",
    "        \n",
    "    def markov_try_out_max_p2(self,x):\n",
    "        \"The maximin version of the MDP for P2\"\n",
    "        \n",
    "        def chance_multiplication(self,x):\n",
    "            \"The transition probability function\"\n",
    "            \n",
    "            # store the actions for both players\n",
    "            actions_p1_game1 = self.payoff_p1_game1.shape[0]\n",
    "            actions_p1_game2 = self.payoff_p1_game2.shape[0]            \n",
    "            actions_p2_game1 = self.payoff_p1_game1.shape[1]\n",
    "            actions_p2_game2 = self.payoff_p1_game2.shape[1]\n",
    "            \n",
    "            length_of_actions = np.max([actions_p1_game1,actions_p1_game2]) # select the maximal length of the actions\n",
    "            \n",
    "            p1_combined = np.zeros(length_of_actions, dtype=object) # initialize the p1 transition probability\n",
    "            \n",
    "            # calculate the transition probabilities for p1\n",
    "            for i in np.nditer(np.arange(length_of_actions)):\n",
    "                working_probs = np.zeros((2,2))\n",
    "                \n",
    "                working_probs[0,0] = np.asscalar(np.dot(self.transition_matrix_game1_to1[i,:],x[0:actions_p2_game1]))\n",
    "                working_probs[0,1] = np.asscalar(np.dot(self.transition_matrix_game1_to2[i,:],x[0:actions_p2_game1])) \n",
    "                working_probs[1,0] = np.asscalar(np.dot(self.transition_matrix_game2_to1[i,:],x[actions_p2_game1:actions_p2_game1+actions_p2_game2]))\n",
    "                working_probs[1,1] = np.asscalar(np.dot(self.transition_matrix_game2_to2[i,:],x[actions_p2_game1:actions_p2_game1+actions_p2_game2]))\n",
    "                \n",
    "                p1_combined[i] = working_probs/np.sum(working_probs,1)\n",
    "\n",
    "            return p1_combined\n",
    "        \n",
    "        def reward_multiplication(self,x):\n",
    "            \"Multiply the rewards for p2's maximin\"\n",
    "            \n",
    "            # initialize the array for storage\n",
    "            rewards_game1 = []\n",
    "            rewards_game2 = []\n",
    "            \n",
    "            # store the actions for p2 in both games\n",
    "            actions_p2_game1 = self.payoff_p2_game1.shape[1]\n",
    "            actions_p2_game2 = self.payoff_p2_game2.shape[1]\n",
    "            \n",
    "            # compute the rewards for game 1\n",
    "            for i in np.nditer(np.arange(self.payoff_p2_game1.shape[0])):\n",
    "                rewards_game1.append(-np.asscalar(np.dot(self.payoff_p2_game1[i,:],x[0:actions_p2_game1])))\n",
    "            \n",
    "            # compute the rewards for game 2\n",
    "            for i in np.nditer(np.arange(self.payoff_p1_game2.shape[0])):\n",
    "                rewards_game2.append(-np.asscalar(np.dot(self.payoff_p2_game2[i,:],x[actions_p2_game1:actions_p2_game1+actions_p2_game2])))\n",
    "        \n",
    "            Reward = np.array([rewards_game1,rewards_game2]) # combine the rewards in one array\n",
    "    \n",
    "            return Reward\n",
    "    \n",
    "        p1_combined = chance_multiplication(self,x) # compute the new transition probabilities\n",
    "        Reward = reward_multiplication(self,x) # compute the new rewards\n",
    "        \n",
    "        # run the MDP to compute the maximin for p2\n",
    "        mdp_threatpointp1 = mdptoolbox.mdp.RelativeValueIteration(p1_combined, Reward, epsilon=0.00000000001)\n",
    "        mdp_threatpointp1.setSilent()\n",
    "        mdp_threatpointp1.run()\n",
    "\n",
    "        self.threatpoint_p1_policy = mdp_threatpointp1.policy #store the policy\n",
    "        \n",
    "        # print some blabla (wonder if someone ever reads this)\n",
    "        if self.printing == True:\n",
    "            print(\"Running MDP\")\n",
    "            print(\"With X as\",x)\n",
    "            print(\"Resulting in:\")\n",
    "            print(mdp_treatpointp1.average_reward)\n",
    "            print(\"With policy:\")\n",
    "            print(mdp_treatpointp1.policy)\n",
    "\n",
    "\n",
    "        return mdp_threatpointp1.average_reward\n",
    "    \n",
    "    def maximin_point(self,T):\n",
    "        \"This part tries to find the maximin point for T points\"\n",
    "        print(\"Trying to find the maximin values for both players\")\n",
    "        \n",
    "        # store the shape of the games for both players\n",
    "        x_shape_p1_g1 = self.payoff_p1_game1.shape[0]\n",
    "        x_shape_p1_g2 = self.payoff_p1_game2.shape[0]\n",
    "        \n",
    "        x_shape_p2_g1 = self.payoff_p2_game1.shape[1]\n",
    "        x_shape_p2_g2 = self.payoff_p2_game2.shape[1]\n",
    "        \n",
    "        # add up the shapes for both players\n",
    "        x_shape_p1 = x_shape_p1_g1 + x_shape_p1_g2\n",
    "        x_shape_p2 = x_shape_p2_g1 + x_shape_p2_g2\n",
    "        \n",
    "        # initialize for trying strategies\n",
    "        x_try_p1_max = np.zeros(x_shape_p2)\n",
    "        x_try_p2_max = np.zeros(x_shape_p1)\n",
    "        \n",
    "        # set all 'found' values to zero\n",
    "        tried_mdp_p1_max = 0\n",
    "        stored_value_p1_max = 0\n",
    "\n",
    "        tried_mdp_p2_max = 0\n",
    "        stored_value_p2_max = 0\n",
    "        \n",
    "        # loop over the number of points to generate\n",
    "        for i in range(0,T):\n",
    "                \n",
    "            # draw new strategies\n",
    "            x_try_p1_max = np.random.beta(0.5,0.5,x_shape_p2)\n",
    "            x_try_p2_max = np.random.beta(0.5,0.5,x_shape_p1)\n",
    "            \n",
    "            # normalize these strategies\n",
    "            x_try_p1_max[0:x_shape_p2_g1] = x_try_p1_max[0:x_shape_p2_g1]/np.sum(x_try_p1_max[0:x_shape_p2_g1])\n",
    "            x_try_p1_max[x_shape_p2_g1:x_shape_p2] = x_try_p1_max[x_shape_p2_g1:x_shape_p2]/np.sum(x_try_p1_max[x_shape_p2_g1:x_shape_p2])\n",
    "            \n",
    "            x_try_p2_max[0:x_shape_p1_g1] = x_try_p2_max[0:x_shape_p1_g1]/np.sum(x_try_p2_max[0:x_shape_p1_g1])\n",
    "            x_try_p2_max[x_shape_p1_g1:x_shape_p1] = x_try_p2_max[x_shape_p1_g1:x_shape_p1]/np.sum(x_try_p2_max[x_shape_p1_g1:x_shape_p1])\n",
    "\n",
    "            # run the markov maximin problems\n",
    "            tried_mdp_p1_max = -self.markov_try_out_max_p1(x_try_p1_max)\n",
    "            tried_mdp_p2_max = -self.markov_try_out_max_p2(x_try_p2_max)\n",
    "\n",
    "            # if it is the first result, store it\n",
    "            if i == 0:\n",
    "                stored_value_p1_max = tried_mdp_p1_max\n",
    "                stored_value_p2_max = tried_mdp_p2_max\n",
    "\n",
    "                x_best_p1_max = x_try_p1_max\n",
    "                x_best_p2_max = x_try_p2_max\n",
    "            \n",
    "            # if a higher value is found and it is not the first result, then store again\n",
    "            else:\n",
    "                if tried_mdp_p1_max > stored_value_p1_max:\n",
    "                    stored_value_p1_max = tried_mdp_p1_max\n",
    "                    x_best_p1_max = x_try_p1_max\n",
    "                if tried_mdp_p2_max > stored_value_p2_max:\n",
    "                    stored_value_p2_max = tried_mdp_p2_max\n",
    "                    x_best_p2_max = x_try_p2_max\n",
    "\n",
    "        print(\"Maximin value found for P1\",stored_value_p1_max)\n",
    "        print(\"With best maximization strategy:\",x_best_p1_max)       \n",
    "        print(\"\")\n",
    "        print(\"Maximin value found for P2\",stored_value_p2_max)\n",
    "        print(\"With best maximization strategy:\",x_best_p2_max)\n",
    "        print(\"\")\n",
    "        print(\"\")    \n",
    "        \n",
    "    def optimized_maximin(self,points,show_strat_p1,show_strat_p2,FD_yn):\n",
    "        \"This algorithim is a more optimized way of calculating the maximin results \"\n",
    "        print(\"Start of the maximin algorithm\")        \n",
    "        \n",
    "        def random_strategy_draw(points,number_of_actions):\n",
    "            \"This function draws random strategies from a beta distribution, based on the number of points and actions\"\n",
    "            \n",
    "            # draw the strategies and normalize them\n",
    "            strategies_drawn = np.random.beta(0.5,0.5,(points,number_of_actions))\n",
    "            strategies_drawn = strategies_drawn/np.sum(strategies_drawn, axis=1).reshape([points,1])\n",
    "            \n",
    "            return strategies_drawn\n",
    "        \n",
    "        def frequency_pairs_p1(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"This function sorts the strategies that punish based on the best replies for p1\"\n",
    "            \n",
    "            # store the size of the game\n",
    "            game_size_1 = self.payoff_p1_game1.size\n",
    "            game_size_2 = self.payoff_p1_game2.size\n",
    "            \n",
    "            # store the actions of the game\n",
    "            p1_actions_game1 = self.payoff_p1_game1.shape[0]\n",
    "            p1_actions_game2 = self.payoff_p1_game2.shape[0]\n",
    "            \n",
    "            # calculate the combination of actions and set it in a range\n",
    "            p1_actions_combi = p1_actions_game1*p1_actions_game2\n",
    "            p1_action_range = np.arange(p1_actions_combi)\n",
    "            \n",
    "            # initialize a frequency pair\n",
    "            frequency_pairs = np.zeros((points*(p1_actions_game1*p1_actions_game2),game_size_1+game_size_2))\n",
    "            \n",
    "            # set ranges for game 1 and 2\n",
    "            p1_act_game1_range = np.arange(p1_actions_game1)\n",
    "            p1_act_game2_range = np.arange(p1_actions_game2)\n",
    "            \n",
    "            # set the frequency pairs for game 1 based on best replies\n",
    "            for i in np.nditer(p1_action_range):\n",
    "                for j in np.nditer(p1_act_game1_range):\n",
    "                    mod_remain = np.mod(i,p1_actions_game1)\n",
    "                    frequency_pairs[i*points:(i+1)*points,p1_actions_game1*mod_remain+j] = strategies_drawn[:,j]\n",
    "            \n",
    "            # set the frequency pairs for game 2 based on best replies\n",
    "            for i in np.nditer(p1_action_range):\n",
    "                for j in np.nditer(p1_act_game2_range):\n",
    "                    floor_div = np.floor_divide(i,p1_actions_game2)\n",
    "                    frequency_pairs[i*points:(i+1)*points,j+game_size_1+(p1_actions_game1*floor_div)] = strategies_drawn[:,p1_actions_game1+j]\n",
    "\n",
    "            return frequency_pairs\n",
    "        \n",
    "        def balance_equation(self,tot_act_ut,tot_act_thr,tot_payoffs_game1,tot_payoffs,frequency_pairs):\n",
    "            \"Calculates the result of the balance equations in order to adjust the frequency pairs\"\n",
    "            \n",
    "            # store the game size\n",
    "            game_size_1 = self.payoff_p1_game1.size\n",
    "            game_size_2 = self.payoff_p1_game2.size\n",
    "            \n",
    "            # initialize yi and Q\n",
    "            yi = np.zeros((points*(tot_act_thr*tot_act_ut),game_size_1+game_size_2))\n",
    "            Q = np.zeros((1,points*(tot_act_thr*tot_act_ut)))\n",
    "            \n",
    "            # calculate yi\n",
    "            yi[:,0:tot_payoffs_game1] = frequency_pairs[:,0:tot_payoffs_game1]/np.sum(frequency_pairs[:,0:tot_payoffs_game1], axis=1).reshape([points*tot_payoffs_game1,1])\n",
    "            yi[:,tot_payoffs_game1:tot_payoffs] = frequency_pairs[:,tot_payoffs_game1:tot_payoffs]/np.sum(frequency_pairs[:,tot_payoffs_game1:tot_payoffs], axis=1).reshape([points*(tot_payoffs-tot_payoffs_game1),1])\n",
    "            \n",
    "            # store px\n",
    "            p1_px_between = np.asarray(px)\n",
    "            p1_px = p1_px_between[0]\n",
    "            \n",
    "            # calculate Q\n",
    "            Q[0,:] = (np.sum(yi[:,tot_payoffs_game1:tot_payoffs]*p1_px[tot_payoffs_game1:tot_payoffs],axis=1))/((np.dot(yi[:,0:tot_payoffs_game1],(1-p1_px[0:tot_payoffs_game1]))+np.dot(yi[:,tot_payoffs_game1:tot_payoffs],p1_px[tot_payoffs_game1:tot_payoffs])))\n",
    "            \n",
    "            # calculate new frequency pairs based on Q\n",
    "            frequency_pairs[:,0:tot_payoffs_game1] = (np.multiply(Q.transpose(),yi[:,0:tot_payoffs_game1]))\n",
    "            frequency_pairs[:,tot_payoffs_game1:tot_payoffs] = np.multiply((1-Q.transpose()),yi[:,tot_payoffs_game1:tot_payoffs])\n",
    "            \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def frequency_pairs_p2(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"This function sorts the punish strategies based on the best replies of p2\"\n",
    "            \n",
    "            # store the game size\n",
    "            game_size_1 = self.payoff_p2_game1.size\n",
    "            game_size_2 = self.payoff_p2_game2.size\n",
    "            \n",
    "            # create a range of actions of both players\n",
    "            p1_actions_range = np.arange(p1_actions)\n",
    "            p2_actions_range = np.arange(p2_actions)\n",
    "            \n",
    "            # store the shape of the both games\n",
    "            p2_actions_game1 = self.payoff_p2_game1.shape[1]\n",
    "            p2_actions_game2 = self.payoff_p2_game2.shape[1]\n",
    "            \n",
    "            # calculate the range of possible actions\n",
    "            p2_actions_combo = p2_actions_game1*p2_actions_game2\n",
    "            p2_action_range = np.arange(p2_actions_combo)\n",
    "            \n",
    "            # initialize the frequency pairs\n",
    "            frequency_pairs = np.zeros((points*(p2_actions_game1*p2_actions_game2),game_size_1+game_size_2))\n",
    "            \n",
    "            # sort the frequency pairs for the first game\n",
    "            for i in np.nditer(np.arange(p2_actions_game1)):\n",
    "                for j in np.nditer(p2_action_range):\n",
    "                    modul = np.mod(j,p2_actions_game1)\n",
    "                    frequency_pairs[j*points:(j+1)*points,p2_actions_game1*i+modul] = strategies_drawn[:,i]\n",
    "            \n",
    "            # sort the frequency pairs for the second game\n",
    "            for i in np.nditer(np.arange(p2_actions_game2)):\n",
    "                for j in np.nditer(p2_action_range):\n",
    "                    divide = np.floor_divide(j,p2_actions_game2)\n",
    "                    frequency_pairs[j*points:(j+1)*points,p2_actions_combo+divide+(i*p2_actions_game2)] = strategies_drawn[:,i+p2_actions_game1]                 \n",
    "                    \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def payoffs_sorted(points,payoffs,actions):\n",
    "            \"Sort the payoffs for use on the max and min functions\"\n",
    "            \n",
    "            # create two ranges based on the number of points and actions\n",
    "            points_range = np.arange(points)\n",
    "            actions_range = np.arange(actions)\n",
    "        \n",
    "            payoffs_sort = np.zeros((points,actions)) # initialize the payoff_sort array\n",
    "            \n",
    "            # sort the payoffs\n",
    "            for x in np.nditer(points_range):\n",
    "                for i in np.nditer(actions_range):\n",
    "                    payoffs_sort[x,i] = payoffs[points*i+x]\n",
    "            \n",
    "            return payoffs_sort\n",
    "        \n",
    "        ## Start of p1 maximin ##\n",
    "        \n",
    "        start_time = time.time() # start the time!\n",
    "        \n",
    "        # flatten the transition matrices\n",
    "        flatten1_1 = self.transition_matrix_game1_to1.flatten()\n",
    "        flatten2_1 = self.transition_matrix_game2_to1.flatten()\n",
    "        \n",
    "        # save and compute the total number of actions\n",
    "        actions_p2_game1 = self.payoff_p1_game1.shape[1]\n",
    "        actions_p2_game2 = self.payoff_p1_game2.shape[1]\n",
    "        total_actions_p2 = actions_p2_game1 + actions_p2_game2\n",
    "    \n",
    "        actions_p1_game1 = self.payoff_p1_game1.shape[0]\n",
    "        actions_p1_game2 = self.payoff_p1_game2.shape[0]\n",
    "        total_actions_p1 = actions_p1_game1 + actions_p1_game2\n",
    "        \n",
    "        # flatten the payoffs\n",
    "        payoff_p1_game_1flatten = self.payoff_p1_game1.flatten()\n",
    "        payoff_p1_game_2flatten = self.payoff_p1_game2.flatten()\n",
    "        \n",
    "        total_payoffs_p1_game1 = payoff_p1_game_1flatten.size\n",
    "        total_payoffs_p1_game2 = payoff_p1_game_2flatten.size\n",
    "        total_payoffs_p1 = total_payoffs_p1_game1 + total_payoffs_p1_game2\n",
    "        \n",
    "        # initialize and assign the payoffs for p1\n",
    "        payoff_p1 = np.zeros(total_payoffs_p1)\n",
    "        payoff_p1[0:total_payoffs_p1_game1] = payoff_p1_game_1flatten\n",
    "        payoff_p1[total_payoffs_p1_game1:total_payoffs_p1] = payoff_p1_game_2flatten\n",
    "\n",
    "        px = np.concatenate([flatten1_1,flatten2_1],axis=1) # create px\n",
    "        \n",
    "        y_punisher = random_strategy_draw(points,total_actions_p1) # draw random strategies for the punisher\n",
    "\n",
    "        frequency_pairs = frequency_pairs_p2(points,total_actions_p1,total_actions_p2,y_punisher) # sort these \n",
    "        \n",
    "        # run the balance equation\n",
    "        frequency_pairs = balance_equation(self,actions_p2_game1,actions_p2_game2,total_payoffs_p1_game1,total_payoffs_p1,frequency_pairs)\n",
    "        \n",
    "        # if the FD_function is available, run this (note: Only the FD function from the thesis)\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+frequency_pairs[:,2])-(1/3)*frequency_pairs[:,3]-(1/2)*(frequency_pairs[:,5] + frequency_pairs[:,6]) - (2/3) * frequency_pairs[:,7]\n",
    "        else:\n",
    "            FD = 1  \n",
    "        \n",
    "        # calculate the payoffs with multiplication of payoffs and Fd function\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p1),axis=1)\n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "\n",
    "        max_payoffs = payoffs_sorted(points,payoffs,(actions_p2_game1*actions_p2_game2)) # sort the payoffs\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        minimax_found = np.max(np.min(max_payoffs,axis=1))\n",
    "        print(\"Maximin value for P1 is\",minimax_found)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # print the results\n",
    "        if show_strat_p1 == True:\n",
    "            minimax_indices_p2 = np.where(max_payoffs == minimax_found)\n",
    "            found_strategy_p2 = y_punisher[minimax_indices_p2[0]]\n",
    "            fnd_strategy_p2 = found_strategy_p2.flatten()\n",
    "            fnd_strategy_p2[0:2] = fnd_strategy_p2[0:2]/np.sum(fnd_strategy_p2[0:2])\n",
    "            fnd_strategy_p2[2:4] = fnd_strategy_p2[2:4]/np.sum(fnd_strategy_p2[2:4])\n",
    "            print(\"Player 1 plays stationary strategy:\", fnd_strategy_p2)\n",
    "            print(\"While player 2 replies with a best pure reply of:\", self.best_pure_strategies[minimax_indices_p2[1]])\n",
    "            \n",
    "        end_time = time.time() # stop the time!\n",
    "        print(\"Seconds done to generate\", points, \"points\", end_time-start_time)\n",
    "        \n",
    "        ## End of P1 maximin algorithm ##\n",
    "        \n",
    "        start_time_p2 = time.time() # start the time (Again!)\n",
    "        \n",
    "        #flatten the payoffs\n",
    "        payoff_p2_game_1flatten = self.payoff_p2_game1.flatten()\n",
    "        payoff_p2_game_2flatten = self.payoff_p2_game2.flatten()\n",
    "        \n",
    "        # store and compute the total payoffs\n",
    "        total_payoffs_p2_game1 = payoff_p2_game_1flatten.size\n",
    "        total_payoffs_p2_game2 = payoff_p2_game_2flatten.size\n",
    "        total_payoffs_p2 = total_payoffs_p2_game1 + total_payoffs_p2_game2\n",
    "        \n",
    "        # initialize and assign the payoffs for p2\n",
    "        payoff_p2 = np.zeros(total_payoffs_p2)\n",
    "        payoff_p2[0:total_payoffs_p2_game1] = payoff_p2_game_1flatten\n",
    "        payoff_p2[total_payoffs_p2_game1:total_payoffs_p2] = payoff_p2_game_2flatten\n",
    "\n",
    "        px = np.concatenate([flatten1_1,flatten2_1],axis=1) # store px\n",
    "\n",
    "        x_punisher = random_strategy_draw(points,total_actions_p2) # draw punisher strategies\n",
    "        \n",
    "        frequency_pairs = frequency_pairs_p1(points,total_actions_p1,total_actions_p2,x_punisher) # best replies\n",
    "        \n",
    "        # do the balance equation trick\n",
    "        frequency_pairs = balance_equation(self,actions_p1_game1,actions_p1_game2,total_payoffs_p1_game1,total_payoffs_p1,frequency_pairs)\n",
    "        \n",
    "        # if the FD function is on, then run it\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+frequency_pairs[:,2])-(1/3)*frequency_pairs[:,3]-(1/2)*(frequency_pairs[:,5] + frequency_pairs[:,6]) - (2/3) * frequency_pairs[:,7]\n",
    "        else:\n",
    "            FD = 1     \n",
    "        \n",
    "        # compute the payoffs with the payoffs and FD function\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p2),axis=1)\n",
    "        \n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "        \n",
    "        max_payoffs = payoffs_sorted(points,payoffs,(actions_p1_game1*actions_p1_game2))     # sort the payoffs    \n",
    "\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        minimax_found_p2 = np.max(np.min(max_payoffs,axis=1)) # find the maximin value\n",
    "        print(\"Maximin value for P2 is\",minimax_found_p2)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # print the strategies\n",
    "        if show_strat_p2 == True:\n",
    "            maximin_indices_p2 = np.where(max_payoffs == minimax_found_p2)\n",
    "            found_strategy = x_punisher[maximin_indices_p2[0]]\n",
    "            fnd_strategy = found_strategy.flatten()\n",
    "            fnd_strategy[0:2] = fnd_strategy[0:2]/np.sum(fnd_strategy[0:2])\n",
    "            fnd_strategy[2:4] = fnd_strategy[2:4]/np.sum(fnd_strategy[2:4])\n",
    "            print(\"Player 2 plays stationairy strategy:\", fnd_strategy)\n",
    "            print(\"While player 2 replies with a best pure reply of:\", self.best_pure_strategies[maximin_indices_p2[1]])\n",
    "        \n",
    "        end_time_p2 = time.time() # stop the time\n",
    "        print(\"Seconds done to generate\", points, \"points\", end_time_p2-start_time_p2)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "    def threat_point_optimized(self,points,show_strat_p1,show_strat_p2,print_text,FD_yn):\n",
    "        \"The optimized, super awesome threat point algorithm!\"\n",
    "        \n",
    "        def random_strategy_draw(points,number_of_actions):\n",
    "            \"This function draws random strategies from a beta distribution, based on the number of points and actions\"\n",
    "            \n",
    "            # draw the strategies and normalize them\n",
    "            strategies_drawn = np.random.beta(0.5,0.5,(points,number_of_actions))\n",
    "            strategies_drawn = strategies_drawn/np.sum(strategies_drawn, axis=1).reshape([points,1])\n",
    "            \n",
    "            return strategies_drawn\n",
    "        \n",
    "        def frequency_pairs_p1(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"This function sorts the punisher strategies based on the replies for p1\"\n",
    "            \n",
    "            # store the game sizes\n",
    "            game_size_1 = self.payoff_p1_game1.size\n",
    "            game_size_2 = self.payoff_p1_game2.size\n",
    "            \n",
    "            # store the actions of p1 for both games\n",
    "            p1_actions_game1 = self.payoff_p1_game1.shape[0]\n",
    "            p1_actions_game2 = self.payoff_p1_game2.shape[0]\n",
    "            \n",
    "            # set both actions within a certain range\n",
    "            p1_actions_combi = p1_actions_game1*p1_actions_game2\n",
    "            p1_action_range = np.arange(p1_actions_combi)\n",
    "            \n",
    "            # initialize the frequency pair\n",
    "            frequency_pairs = np.zeros((points*(p1_actions_game1*p1_actions_game2),game_size_1+game_size_2))\n",
    "            \n",
    "            # set action ranges for both games\n",
    "            p1_act_game1_range = np.arange(p1_actions_game1)\n",
    "            p1_act_game2_range = np.arange(p1_actions_game2)\n",
    "            \n",
    "            # set the best replies for the first game\n",
    "            for i in np.nditer(p1_action_range):\n",
    "                for j in np.nditer(p1_act_game1_range):\n",
    "                    mod_remain = np.mod(i,p1_actions_game1)\n",
    "                    frequency_pairs[i*points:(i+1)*points,p1_actions_game1*mod_remain+j] = strategies_drawn[:,j]\n",
    "            \n",
    "            # set the best replies for the second game\n",
    "            for i in np.nditer(p1_action_range):\n",
    "                for j in np.nditer(p1_act_game2_range):\n",
    "                    floor_div = np.floor_divide(i,p1_actions_game2)\n",
    "                    frequency_pairs[i*points:(i+1)*points,j+game_size_1+(p1_actions_game1*floor_div)] = strategies_drawn[:,p1_actions_game1+j]\n",
    "            \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def balance_equation(self,tot_act_ut,tot_act_thr,tot_payoffs_game1,tot_payoffs,frequency_pairs):\n",
    "            \"Calculates the result of the balance equations in order to adjust the frequency pairs\"\n",
    "            \n",
    "            # store size of game 1 and 2\n",
    "            game_size_1 = self.payoff_p1_game1.size\n",
    "            game_size_2 = self.payoff_p1_game2.size\n",
    "            \n",
    "            # initialize yi and Q\n",
    "            yi = np.zeros((points*(tot_act_thr*tot_act_ut),game_size_1+game_size_2))\n",
    "            Q = np.zeros((1,points*(tot_act_thr*tot_act_ut)))\n",
    "            \n",
    "            # compute Yi\n",
    "            yi[:,0:tot_payoffs_game1] = frequency_pairs[:,0:tot_payoffs_game1]/np.sum(frequency_pairs[:,0:tot_payoffs_game1], axis=1).reshape([points*tot_payoffs_game1,1])\n",
    "            yi[:,tot_payoffs_game1:tot_payoffs] = frequency_pairs[:,tot_payoffs_game1:tot_payoffs]/np.sum(frequency_pairs[:,tot_payoffs_game1:tot_payoffs], axis=1).reshape([points*(tot_payoffs-tot_payoffs_game1),1])\n",
    "            \n",
    "            p1_px_between = np.asarray(px) # some tricks with px  (ha-ha)\n",
    "            p1_px = p1_px_between[0]\n",
    "\n",
    "            # compute Q\n",
    "            Q[0,:] = (np.sum(yi[:,tot_payoffs_game1:tot_payoffs]*p1_px[tot_payoffs_game1:tot_payoffs],axis=1))/((np.dot(yi[:,0:tot_payoffs_game1],(1-p1_px[0:tot_payoffs_game1]))+np.dot(yi[:,tot_payoffs_game1:tot_payoffs],p1_px[tot_payoffs_game1:tot_payoffs])))\n",
    "            \n",
    "            # adjust the frequency pairs based on Q\n",
    "            frequency_pairs[:,0:tot_payoffs_game1] = (np.multiply(Q.transpose(),yi[:,0:tot_payoffs_game1]))\n",
    "            frequency_pairs[:,tot_payoffs_game1:tot_payoffs] = np.multiply((1-Q.transpose()),yi[:,tot_payoffs_game1:tot_payoffs])\n",
    "            \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def frequency_pairs_p2(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"This function sorts the punisher strategies based on the replies for p2\"\n",
    "            \n",
    "            # store the sizes of both games\n",
    "            game_size_1 = self.payoff_p2_game1.size\n",
    "            game_size_2 = self.payoff_p2_game2.size\n",
    "            \n",
    "            # make a nice range of the actions for both players and store them\n",
    "            p1_actions_range = np.arange(p1_actions)\n",
    "            p2_actions_range = np.arange(p2_actions)\n",
    "            \n",
    "            p2_actions_game1 = self.payoff_p2_game1.shape[1]\n",
    "            p2_actions_game2 = self.payoff_p2_game2.shape[1]\n",
    "            \n",
    "            p2_actions_combo = p2_actions_game1*p2_actions_game2\n",
    "            p2_action_range = np.arange(p2_actions_combo)\n",
    "            \n",
    "            # initialize the frequency pairs\n",
    "            frequency_pairs = np.zeros((points*(p2_actions_game1*p2_actions_game2),game_size_1+game_size_2))\n",
    "            \n",
    "            # loop over the first games best responses\n",
    "            for i in np.nditer(np.arange(p2_actions_game1)):\n",
    "                for j in np.nditer(p2_action_range):\n",
    "                    modul = np.mod(j,p2_actions_game1)\n",
    "                    frequency_pairs[j*points:(j+1)*points,p2_actions_game1*i+modul] = strategies_drawn[:,i]\n",
    "            \n",
    "            # loop over the second games best responses\n",
    "            for i in np.nditer(np.arange(p2_actions_game2)):\n",
    "                for j in np.nditer(p2_action_range):\n",
    "                    divide = np.floor_divide(j,p2_actions_game2)\n",
    "                    frequency_pairs[j*points:(j+1)*points,p2_actions_combo+divide+(i*p2_actions_game2)] = strategies_drawn[:,i+p2_actions_game1]\n",
    "                    \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def payoffs_sorted(points,payoffs,actions):\n",
    "            \"This function sorts the payoffs for use on the threat point function\"\n",
    "            \n",
    "            # set a points and actions range\n",
    "            points_range = np.arange(points)\n",
    "            actions_range = np.arange(actions)\n",
    "        \n",
    "            payoffs_sort = np.zeros((points,actions)) # initialize the payoffs sorted\n",
    "             \n",
    "            # and sort them in a loop\n",
    "            for x in np.nditer(points_range):\n",
    "                for i in np.nditer(actions_range):\n",
    "                    payoffs_sort[x,i] = payoffs[points*i+x]\n",
    "            \n",
    "            return payoffs_sort\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"The start of the algorithm for finding the threat point\")\n",
    "            print(\"First let's find the threat point for Player 1\")\n",
    "\n",
    "        # flatten the transition probabilities\n",
    "        flatten1_1 = self.transition_matrix_game1_to1.flatten()\n",
    "        flatten2_1 = self.transition_matrix_game2_to1.flatten()\n",
    "        \n",
    "        # store the actions of both players\n",
    "        actions_p2_game1 = self.payoff_p1_game1.shape[1]\n",
    "        actions_p2_game2 = self.payoff_p1_game2.shape[1]\n",
    "        total_actions_p2 = actions_p2_game1 + actions_p2_game2\n",
    "        \n",
    "        actions_p1_game1 = self.payoff_p1_game1.shape[0]\n",
    "        actions_p1_game2 = self.payoff_p1_game2.shape[0]\n",
    "        total_actions_p1 = actions_p1_game1 + actions_p1_game2\n",
    "        \n",
    "        # Start of algorithm for player 1\n",
    "        \n",
    "        start_time = time.time() # and start the time!\n",
    "        \n",
    "        # flatten the payoffs of both players\n",
    "        payoff_p1_game_1flatten = self.payoff_p1_game1.flatten()\n",
    "        payoff_p1_game_2flatten = self.payoff_p1_game2.flatten()\n",
    "        \n",
    "        total_payoffs_p1_game1 = payoff_p1_game_1flatten.size\n",
    "        total_payoffs_p1_game2 = payoff_p1_game_2flatten.size\n",
    "        total_payoffs_p1 = total_payoffs_p1_game1 + total_payoffs_p1_game2\n",
    "        \n",
    "        # initialize and assign the payoffs for p1\n",
    "        payoff_p1 = np.zeros(total_payoffs_p1)\n",
    "        payoff_p1[0:total_payoffs_p1_game1] = payoff_p1_game_1flatten\n",
    "        payoff_p1[total_payoffs_p1_game1:total_payoffs_p1] = payoff_p1_game_2flatten\n",
    "        \n",
    "        px = np.concatenate([flatten1_1,flatten2_1],axis=1) # create px\n",
    "        \n",
    "        y_punisher = random_strategy_draw(points,total_actions_p2) #  draw strategies for the punisher\n",
    "              \n",
    "        frequency_pairs = frequency_pairs_p1(points,total_actions_p2,total_actions_p1,y_punisher) # sort based on best replies\n",
    "        \n",
    "        # calculate the adjustments based on the balance equation\n",
    "        frequency_pairs = balance_equation(self,actions_p1_game1,actions_p1_game2,total_payoffs_p1_game1,total_payoffs_p1,frequency_pairs)\n",
    "        \n",
    "        # if FD function is activated, activate it\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+frequency_pairs[:,2])-(1/3)*frequency_pairs[:,3]-(1/2)*(frequency_pairs[:,5] + frequency_pairs[:,6]) - (2/3) * frequency_pairs[:,7]\n",
    "        else:\n",
    "            FD = 1  \n",
    "        \n",
    "        # calculate the payoffs based on the frequency pairs and FD function\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p1),axis=1)\n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "        \n",
    "        max_payoffs = payoffs_sorted(points,payoffs,(actions_p1_game1*actions_p1_game2)) # sort the payoffs\n",
    "\n",
    "        threat_point_p1 = np.min(np.max(max_payoffs,axis=1)) # determine the threat point\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"Threat point value is\",threat_point_p1)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "        \n",
    "        if show_strat_p1 == True:\n",
    "            threat_point_indices_p1 = np.where(max_payoffs == threat_point_p1)\n",
    "            found_strategy_p1 = y_punisher[threat_point_indices_p1[0]]\n",
    "            fnd_strategy_p1 = found_strategy_p1.flatten()\n",
    "            fnd_strategy_p1[0:2] = fnd_strategy_p1[0:2]/np.sum(fnd_strategy_p1[0:2])\n",
    "            fnd_strategy_p1[2:4] = fnd_strategy_p1[2:4]/np.sum(fnd_strategy_p1[2:4])\n",
    "            print(\"Player 2 plays stationary strategy:\", fnd_strategy_p1)\n",
    "            print(\"While player 1 replies with a best pure reply of:\", self.best_pure_strategies[threat_point_indices_p1[1]])\n",
    "            \n",
    "        end_time = time.time() # stop the time!\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"Seconds done to generate\", points, \"points\", end_time-start_time)\n",
    "            print(\"\")\n",
    "        \n",
    "        # End of algorithm player 1\n",
    "        \n",
    "        # Start of algorithm player 2\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"First start the threat point for player 2\")\n",
    "        start_time_p2 = time.time() # new timer, new times\n",
    "        \n",
    "        # flatten the payoffs for p2\n",
    "        payoff_p2_game_1flatten = self.payoff_p2_game1.flatten()\n",
    "        payoff_p2_game_2flatten = self.payoff_p2_game2.flatten()\n",
    "        \n",
    "        # assign tha payoffs for p2\n",
    "        total_payoffs_p2_game1 = payoff_p2_game_1flatten.size\n",
    "        total_payoffs_p2_game2 = payoff_p2_game_2flatten.size\n",
    "        total_payoffs_p2 = total_payoffs_p2_game1 + total_payoffs_p2_game2\n",
    "        \n",
    "        # initialize payoffs p2 and assign them\n",
    "        payoff_p2 = np.zeros(total_payoffs_p2)\n",
    "        payoff_p2[0:total_payoffs_p2_game1] = payoff_p2_game_1flatten\n",
    "        payoff_p2[total_payoffs_p2_game1:total_payoffs_p2] = payoff_p2_game_2flatten\n",
    "\n",
    "        px = np.concatenate([flatten1_1,flatten2_1],axis=1) # px in the mix\n",
    "\n",
    "        x_punisher = random_strategy_draw(points,total_actions_p1) # draw strategies for the punisher\n",
    "\n",
    "        frequency_pairs = frequency_pairs_p2(points,total_actions_p2,total_actions_p1,x_punisher) # sort based on best replies\n",
    "        \n",
    "        # adjust based on the balance equation\n",
    "        frequency_pairs = balance_equation(self,actions_p2_game1,actions_p2_game2,total_payoffs_p2_game1,total_payoffs_p2,frequency_pairs)\n",
    "        \n",
    "        # Activate the FD function, or not\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+frequency_pairs[:,2])-(1/3)*frequency_pairs[:,3]-(1/2)*(frequency_pairs[:,5] + frequency_pairs[:,6]) - (2/3) * frequency_pairs[:,7]\n",
    "        else:\n",
    "            FD = 1     \n",
    "        \n",
    "        # determine the payoffs based on the frequency pairs and FD function\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p2),axis=1)\n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "        \n",
    "        max_payoffs = payoffs_sorted(points,payoffs,(actions_p2_game1*actions_p2_game2)) # sort the payoffs  \n",
    "    \n",
    "        threat_point_p2 = np.min(np.max(max_payoffs,axis=1)) # determine the threat point\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"Threat point value is\",threat_point_p2)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "        \n",
    "        if show_strat_p2 == True:\n",
    "            threat_point_indices_p2 = np.where(max_payoffs == threat_point_p2)\n",
    "            found_strategy = x_punisher[threat_point_indices_p2[0]]\n",
    "            fnd_strategy = found_strategy.flatten()\n",
    "            fnd_strategy[0:2] = fnd_strategy[0:2]/np.sum(fnd_strategy[0:2])\n",
    "            fnd_strategy[2:4] = fnd_strategy[2:4]/np.sum(fnd_strategy[2:4])\n",
    "            print(\"Player 1 plays stationairy strategy:\", fnd_strategy)\n",
    "            print(\"While player 2 replies with a best pure reply of:\", self.best_pure_strategies[threat_point_indices_p2[1]])\n",
    "            \n",
    "        end_time_p2 = time.time() # stop the time!\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"Seconds done to generate\", points, \"points\", end_time_p2-start_time_p2)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            \n",
    "        self.threat_point = [threat_point_p1,threat_point_p2] # store the threat point\n",
    "        \n",
    "        return [threat_point_p1,threat_point_p2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start of the algorithm for finding the threat point\n",
      "First let's find the threat point for Player 1\n",
      "\n",
      "\n",
      "Threat point value is 8.02113767899626\n",
      "\n",
      "\n",
      "Seconds done to generate 100000 points 0.9953503608703613\n",
      "\n",
      "\n",
      "\n",
      "First start the threat point for player 2\n",
      "\n",
      "\n",
      "Threat point value is 8.021119980220448\n",
      "\n",
      "\n",
      "\n",
      "Seconds done to generate 100000 points 1.1983256340026855\n",
      "\n",
      "\n",
      "Start of the maximin algorithm\n",
      "\n",
      "\n",
      "Maximin value for P1 is 8.020262340551996\n",
      "\n",
      "\n",
      "Seconds done to generate 100000 points 1.0464112758636475\n",
      "\n",
      "\n",
      "Maximin value for P2 is 8.020753427811123\n",
      "\n",
      "\n",
      "Seconds done to generate 100000 points 1.1220407485961914\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"The first Stochastic Game is based on a Stochastic Game described in the thesis\"\n",
    "\n",
    "p1_1 = np.matrix('16 14; 28 24')\n",
    "p2_1 = np.matrix('16 28; 14 24')\n",
    "\n",
    "p1_2 = np.matrix('4 3.5; 7 6')\n",
    "p2_2 = np.matrix('4 7; 3.5 6')\n",
    "\n",
    "trans1_1 = np.matrix('0.8 0.7; 0.7 0.6')\n",
    "trans2_1 = np.matrix('0.5 0.4; 0.4 0.15')\n",
    "\n",
    "trans1_2 = np.matrix('0.2 0.3; 0.3 0.4')\n",
    "trans2_2 = np.matrix('0.5 0.6; 0.6 0.85')   \n",
    "\n",
    "FirstTry = StochasticGame(p1_1,p2_1,p1_2,p2_2,trans1_1,trans2_1,trans1_2,trans2_2)\n",
    "FirstTry.plot_single_period_pure_rewards()\n",
    "FirstTry.plot_all_reward_points(True)\n",
    "# FirstTry.maximin_point(100)\n",
    "# timing = time.time()\n",
    "# FirstTry.threat_point_algorithm(100000,0.025)\n",
    "# now = time.time()\n",
    "# print(now-timing)\n",
    "FirstTry.threat_point_optimized(100000,False,False,True,True)\n",
    "FirstTry.plot_threat_point()\n",
    "FirstTry.plot_threat_point_lines()\n",
    "FirstTry.optimized_maximin(100000,False,False,True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# d1_1 = np.matrix('1 0;0 1')\n",
    "# d2_1 = np.matrix('1 1;0 1')\n",
    "\n",
    "# d1_2 = np.matrix('16 0;28 24')\n",
    "# d2_2 = np.matrix('0 14;12 8')\n",
    "\n",
    "# SecondTry = StochasticGame(d1_1,d2_1,d1_2,d2_2,trans1_1,trans2_1,trans1_2,trans2_2)\n",
    "# SecondTry.plot_single_period_pure_rewards()\n",
    "# SecondTry.plot_convex_hull_pure_rewards()\n",
    "# SecondTry.plot_all_reward_points()\n",
    "# SecondTry.maximin_point()\n",
    "# SecondTry.threat_point_algorithm()\n",
    "# SecondTry.plot_threat_point()\n",
    "\n",
    "\n",
    "\n",
    "# e1_1 = np.matrix('10 15; 21 10')\n",
    "# e1_2 = np.matrix('0 10;3 0')\n",
    "\n",
    "# e2_1 = np.matrix('1 0;5 5')\n",
    "# e2_2 = np.matrix('3 4;2 5')\n",
    "\n",
    "# tre1_1 = np.matrix('0.9999 0.9;0.95 0.85')\n",
    "# tre2_1 = np.matrix('0.6 0.99;0.75 0.5')\n",
    "\n",
    "# tre1_2 = np.matrix('0.0001 0.1;0.05 0.15')\n",
    "# tre2_2 = np.matrix('0.4 0.01;0.25 0.5')\n",
    "\n",
    "# ThirdTry = StochasticGame(e1_1,e2_1,e1_2,e2_2,tre1_1,tre2_1,tre1_2,tre2_2)\n",
    "# ThirdTry.plot_single_period_pure_rewards()\n",
    "# ThirdTry.plot_convex_hull_pure_rewards()\n",
    "# ThirdTry.plot_all_reward_points()\n",
    "# ThirdTry.maximin_point()\n",
    "# ThirdTry.threat_point_algorithm()\n",
    "# ThirdTry.plot_threat_point()\n",
    "\n",
    "# f1_1 = np.matrix('10 10; 10 10')\n",
    "# f1_2 = np.matrix('10 10; 10 10')\n",
    "\n",
    "# f2_1 = np.matrix('10 10; 10 10')\n",
    "# f2_2 = np.matrix('10 10; 10 10')\n",
    "\n",
    "# fre1_1 = np.matrix('0.5 0.5;0.5 0.5')\n",
    "# fre2_1 = np.matrix('0.5 0.5;0.5 0.5')\n",
    "\n",
    "# fre1_2 = np.matrix('0.5 0.5;0.5 0.5')\n",
    "# fre2_2 = np.matrix('0.5 0.5;0.5 0.5')\n",
    "\n",
    "\n",
    "# Indif = StochasticGame(f1_1,f2_1,f1_2,f2_2,fre1_1,fre2_1,fre1_2,fre2_2)\n",
    "# Indif.plot_single_period_pure_rewards()\n",
    "# Indif.plot_convex_hull_pure_rewards()\n",
    "# Indif.plot_all_reward_points()\n",
    "# Indif.threat_point_algorithm()\n",
    "# Indif.plot_threat_point()\n",
    "\n",
    "# sizex = 10\n",
    "\n",
    "# TP1_1 = np.random.randint(0,20, size=(sizex,sizex))\n",
    "# TP1_2 = np.random.randint(0,20, size=(sizex,sizex))\n",
    "\n",
    "# TP2_1 = np.random.randint(0,20, size=(sizex,sizex))\n",
    "# TP2_2 = np.random.randint(0,20, size=(sizex,sizex))\n",
    "\n",
    "# TPTP1_1 = np.matrix(np.random.rand(sizex,sizex))\n",
    "# TPTP2_1 = np.matrix(np.random.rand(sizex,sizex))\n",
    "\n",
    "# TPTP1_2 = 1 - TPTP1_1\n",
    "# TPTP2_2 = 1 - TPTP2_1\n",
    "\n",
    "# TestGame = StochasticGame(TP1_1,TP2_1,TP1_2,TP2_2,TPTP1_1,TPTP2_1,TPTP1_2,TPTP2_2)\n",
    "# start = time.time()\n",
    "# TestGame.maximin_point(1000000)\n",
    "# print(time.time()-start)\n",
    "\n",
    "# start = time.time()\n",
    "# TestGame.threat_point_algorithm(1000000,0.025)\n",
    "# print(time.time()-start)\n",
    "\n",
    "# start = time.time()\n",
    "# TestGame.try_out_optimized_minimax(1000000,False,False,False)\n",
    "# print(time.time()-start)\n",
    "\n",
    "# start = time.time()\n",
    "# TestGame.try_out_optimized(1000000,False,False,True,False)\n",
    "# print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ETPGame:\n",
    "    \"The ETP Game class represents the Type III games from the thesis, with or without ESP.\"\n",
    "    \n",
    "    def __init__(self,payoff_p1_game1,payoff_p2_game1,payoff_p1_game2,payoff_p2_game2,trmatrixg1,trmatrixg2,trmatrixg3,trmatrixg4,matrixA):\n",
    "        \"Here below we initialize the game by storing payoff and transition matrices according to the upper input.\"\n",
    "        self.payoff_p1_game1 = payoff_p1_game1                  #payoff p1 in game 1\n",
    "        self.payoff_p2_game1 = payoff_p2_game1                  #payoff p2 in game 1\n",
    "         \n",
    "        self.payoff_p1_game2 = payoff_p1_game2                  #payoff p1 in game 2\n",
    "        self.payoff_p2_game2 = payoff_p2_game2                  #payoff p2 in game 2\n",
    "        \n",
    "        self.transition_matrix_game1_to1 = trmatrixg1           #transition matrix from game 1 to game 1\n",
    "        self.transition_matrix_game2_to1 = trmatrixg2           #transition matrix from game 2 to game 1\n",
    "         \n",
    "        self.transition_matrix_game1_to2 = trmatrixg3           #transition matrix from game 1 to game 2\n",
    "        self.transition_matrix_game2_to2 = trmatrixg4           #transition matrix from game 2 to game 2\n",
    "        \n",
    "        self.etp_matrix = matrixA\n",
    "        \n",
    "        self.printing = False   #set printing to False\n",
    "        \n",
    "        self.best_pure_strategies = np.array([[1,0,1,0],[0,1,1,0],[1,0,0,1],[0,1,0,1]])\n",
    "      \n",
    "    def plot_single_period_pure_rewards(self):\n",
    "        \"Here we plot the pure rewards possible for a single period\"\n",
    "        \n",
    "        plt.figure()                                            #create a figure\n",
    "        payoff_p1_g1_flat = self.payoff_p1_game1.A1             #create a flattend payoff of p1 in game 1\n",
    "        payoff_p2_g1_flat = self.payoff_p2_game1.A1             #create a flattend payoff of p2 in game 1\n",
    "        plt.scatter(payoff_p1_g1_flat,payoff_p2_g1_flat, label=\"Pure reward points Game 1\", zorder = 15) #plot payoffs game 1\n",
    "     \n",
    "        payoff_p1_g2_flat = self.payoff_p1_game2.A1             #create a flattend payoff of p1 in game 2\n",
    "        payoff_p2_g2_flat = self.payoff_p2_game2.A1             #and for p2 in game 2\n",
    "        plt.scatter(payoff_p1_g2_flat,payoff_p2_g2_flat, label=\"Pure reward points Game 2\", zorder = 15)  #plotting this again\n",
    "        \n",
    "        plt.xlabel(\"Reward Player 1\")                           #giving the x-axis the label of payoff p1\n",
    "        plt.ylabel(\"Reward Player 2\")                           #and the payoff of the y-axis is that of p2\n",
    "        plt.title(\"Reward points of FD Type III game\")      #and we give it a nice titel\n",
    "        plt.legend()\n",
    "        \n",
    "    def plot_convex_hull_pure_rewards(self):\n",
    "        \"Here we plot a convex hull around the pure reward point, therefore resulting in the total possible reward space\"\n",
    "        \n",
    "        payoff_p1_g1_flat = self.payoff_p1_game1.A1       #store the flattend payoff of p1 game 1\n",
    "        payoff_p2_g1_flat = self.payoff_p2_game1.A1       #store the flattend payoff of p2 game 1\n",
    "        \n",
    "        payoff_p1_g2_flat = self.payoff_p1_game2.A1       #store the flattend payoff of p1 game 2\n",
    "        payoff_p2_g2_flat = self.payoff_p2_game2.A1       #store the flattend payoff of p2 game 2\n",
    "        \n",
    "        payoff_p1_merged = np.concatenate((payoff_p1_g1_flat,payoff_p1_g2_flat))  #merge p1 payoffs\n",
    "        payoff_p2_merged = np.concatenate((payoff_p2_g1_flat,payoff_p2_g2_flat))  #merge p2 payoffs\n",
    "        \n",
    "        all_payoffs = np.array([payoff_p1_merged,payoff_p2_merged])  #create one array of payoffs\n",
    "        all_payoffs = np.transpose(all_payoffs)                      #and rotate this one\n",
    "        \n",
    "        rewards_convex_hull = ConvexHull(all_payoffs)                #retain the convex hull of the payoffs\n",
    "        plt.fill(all_payoffs[rewards_convex_hull.vertices,0], all_payoffs[rewards_convex_hull.vertices,1], color='k')\n",
    "        #here above we fill the convex hull in black\n",
    "       \n",
    "    def plot_all_rewards(self,FD_yn):\n",
    "        \"This plots all rewards and is based on the algorithm by Llea Samuel\"\n",
    "        \"IMPORTANT NOTE!: GAME IS HARDCODED WITHIN THIS FUNCTION, DUE TO TIME ISSUES, ONE SHOULD TAKE THIS INTO MIND\"\n",
    "        \n",
    "        np.seterr(all='warn',divide='warn') # this is to show some more information on possible errors, can be excluded\n",
    "\n",
    "        ## HERE BELOW IS HARDCODED GAME INFORMATION\n",
    "        A1 = np.array([16, 14, 28, 24, 4, 3.5, 7, 6])\n",
    "        B1 = np.array([16, 28, 14, 24, 4, 7, 3.5, 6])\n",
    "        p = np.array([0.8, 0.7, 0.7, 0.6, 0.5, 0.4, 0.4, 0.15])\n",
    "        ## ABOVE IS HARDCODED GAME INFORMATION\n",
    "        \n",
    "        # here below we initialize a lot of variables \n",
    "        x = np.zeros(8)\n",
    "        r = np.zeros(8)\n",
    "        y = np.zeros(8)\n",
    "        xstar = np.zeros(8)\n",
    "        x_a = np.zeros(8)\n",
    "        yp = np.zeros(4)\n",
    "        yp_not = np.zeros(4)\n",
    "        v_p1 = np.zeros(8)\n",
    "        v_p2 = np.zeros(8)\n",
    "        Q_vec = np.zeros(8)\n",
    "\n",
    "        T = 100000 # number of points to generate\n",
    "        payoff_p1 = np.zeros(T) # initialize payoffs for both players\n",
    "        payoff_p2 = np.zeros(T)\n",
    "        \n",
    "        # store the matrix A\n",
    "        matrixA = np.matrix('0.00 0.0 0.0 0.00 0.0 0.00 0.00 0.00; 0.35 0.3 0.3 0.25 0.2 0.15 0.15 0.05; 0.35 0.3 0.3 0.25 0.2 0.15 0.15 0.05; 0.7 0.6 0.6 0.5 0.4 0.3 0.3 0.1; 0 0 0 0 0 0 0 0; 0.35 0.3 0.3 0.25 0.2 0.15 0.15 0.05; 0.35 0.3 0.3 0.25 0.2 0.15 0.15 0.05; 0.7 0.6 0.6 0.5 0.4 0.3 0.3 0.1')\n",
    "        \n",
    "        # loop over the total number of points to generate\n",
    "        for t in range(0,T):\n",
    "\n",
    "            ### BEGIN Q-CHECKER #####\n",
    "\n",
    "            for i in range(0,3):\n",
    "                r[i] = np.random.beta(0.5,0.5) # generate frequencies\n",
    "\n",
    "            Norm_val = np.sum(r) # normalize them\n",
    "\n",
    "            for i in range(0,3):\n",
    "                x_a[i] = r[i]/Norm_val # normalize and store\n",
    "            \n",
    "            # do some more fancy X calculations\n",
    "            x_b1 = np.array([x_a[0], x_a[2], 0 , 0])\n",
    "            x_b2 = np.array([x_a[1], 0, 0, 0])\n",
    "            x_c1 = x_b1[np.random.permutation(x_b1.size)]\n",
    "            x_c2 = x_b2[np.random.permutation(x_b2.size)]\n",
    "            \n",
    "            for i in range(0,8):\n",
    "                if i < 4:\n",
    "                    x[i] = x_c1[i]\n",
    "                else:\n",
    "                    x[i] = x_c2[i-4]\n",
    "\n",
    "            # caclulate yi and yi_not\n",
    "            for i in range(0,4):\n",
    "                y[i] = x[i]/np.sum(x[0:4])\n",
    "\n",
    "            for i in range(4,8):\n",
    "                y[i] = x[i]/np.sum(x[4:8])\n",
    "\n",
    "            px = p - np.dot(x,matrixA) # calculate px\n",
    "            \n",
    "            # start with computing Q\n",
    "            for w in range(0,4):\n",
    "                for i in range(0,4):\n",
    "                    yp[i] = y[i+4]*px[0,i+4]\n",
    "                    yp_not[i] = y[i]+(1-px[0,i])\n",
    "\n",
    "                Q = np.sum(yp)/(np.sum(yp_not) + np.sum(yp)) # compute Q\n",
    "                Q_not = 1-Q\n",
    "                Q_vec[w] = Q\n",
    "                \n",
    "                # adjust frequency vector based on found Q\n",
    "                for i in range(0,4):\n",
    "                    xstar[i] = Q*y[i]\n",
    "\n",
    "                for i in range(4,8):\n",
    "                    xstar[i] = Q_not*y[i]\n",
    "\n",
    "                px = p - np.dot(xstar,matrixA) # adjust PX\n",
    "            \n",
    "            # apply Aitken's delta squared\n",
    "            Q_check_1 = Q_vec[0]-((Q_vec[1] - Q_vec[0])**2) / (Q_vec[2] - 2*Q_vec[1] + Q_vec[0])\n",
    "            Q_check_2 = Q_vec[1]-((Q_vec[2]- Q_vec[1])**2) / (Q_vec[3] - 2*Q_vec[2] + Q_vec[1])\n",
    "            diff = Q_check_1 - Q_check_2\n",
    "            \n",
    "            # if the distribution has not settled, then infinite calculate a new Q\n",
    "            while diff > abs(1e-8):\n",
    "\n",
    "                Q_check_1 = Q_check_2\n",
    "                \n",
    "                # new calculations of yi\n",
    "                for i in range(0,4):\n",
    "                    yp[i] = y[i+4]*px[0,i+4]\n",
    "                    yp_not[i] = y[i]*(1-px[0,i])\n",
    "\n",
    "                Q = (np.sum(yp)) / (np.sum(yp_not) + np.sum(yp)) # again calculate Q\n",
    "                Q_not = 1-Q\n",
    "                Q_vec[w+1] = Q\n",
    "                \n",
    "                # adjust frequency vector X\n",
    "                for i in range(0,4):\n",
    "                    xstar[i] = Q*y[i]\n",
    "\n",
    "                for i in range(4,8):\n",
    "                    xstar[i] = Q_not*y[i]\n",
    "\n",
    "                px = p - np.dot(xstar,matrixA) # px calculations\n",
    "                \n",
    "                # compute new Q based on aitken's\n",
    "                Q_check_2 = Q_vec[w-1] - ((Q_vec[w]-Q_vec[w-1])**2)/(Q_vec[w+1] - 2*Q_vec[w] + Q_vec[w-1])\n",
    "                diff = Q_check_1 - Q_check_2\n",
    "                w = w+1\n",
    "            \n",
    "            # apply FD function if activated\n",
    "            if FD_yn == True:\n",
    "                FD = 1-0.25*(xstar[1]+xstar[2])-(1/3)*xstar[3]-(1/2)*(xstar[5] + xstar[6]) - (2/3) * xstar[7]\n",
    "            else:\n",
    "                FD = 1 \n",
    "\n",
    "            # calculate payoffs\n",
    "            for i in range(0,8):\n",
    "                v_p1[i] = xstar[i]*A1[i]\n",
    "                v_p2[i] = xstar[i]*B1[i]\n",
    "            \n",
    "            # calculate definitive rewards based on FD function\n",
    "            payoff_p1[t] = FD*np.sum(v_p1)\n",
    "            payoff_p2[t] = FD*np.sum(v_p2)\n",
    "        \n",
    "        # store maximal payoffs\n",
    "        self.maximal_payoffs = np.zeros(2)\n",
    "        self.maximal_payoffs = [np.max(payoff_p1),np.max(payoff_p2)]\n",
    "    \n",
    "        all_payoffs = np.array([payoff_p1,payoff_p2])  #payoffs player 1 and and p2 merging\n",
    "        all_payoffs = np.transpose(all_payoffs)        #transpose for use in convex_hull\n",
    "        Convex_Hull_Payoffs = ConvexHull(all_payoffs)  #calculate convex_hull of the payoffs\n",
    "        \n",
    "        # plot a nice convex hull\n",
    "        plt.fill(all_payoffs[Convex_Hull_Payoffs.vertices,0], all_payoffs[Convex_Hull_Payoffs.vertices,1], color='y', zorder=5, label=\"Obtainable rewards\")\n",
    "\n",
    "    \n",
    "    def plot_threat_point(self):\n",
    "        \"This function plots the threat point if found\"\n",
    "        \n",
    "        plt.scatter(self.threat_point[0],self.threat_point[1], zorder=10, color = 'r', label='Threat point')\n",
    "        plt.legend()\n",
    "        \n",
    "    def plot_threat_point_lines(self):\n",
    "        \"This function plots lines around the threat point indicating the limits for the NE\"\n",
    "        \n",
    "        plt.plot([self.threat_point[0],self.threat_point[0]],[self.threat_point[1],self.maximal_payoffs[1]], color='k', zorder=15)\n",
    "        plt.plot([self.threat_point[0],self.maximal_payoffs[0]],[self.threat_point[1],self.threat_point[1]], color='k', zorder=15)\n",
    "        plt.axis('equal')\n",
    "        plt.savefig('TypeIII-FD.png', dpi=400)\n",
    "        \n",
    "        \n",
    "    def aitken_delta_squared(self,q1,q2,q3):\n",
    "        \"This is the Aitken's Delta Squared accelerator\"\n",
    "        \n",
    "        x3_x2 = np.subtract(q3,q2)\n",
    "        x2_x1 = np.subtract(q2,q1)\n",
    "        \n",
    "        x3_x2_squared = np.power(x3_x2,2)\n",
    "        denominator = np.subtract(x3_x2,x2_x1)\n",
    "        \n",
    "        fraction = np.divide(x3_x2_squared,denominator)\n",
    "        \n",
    "        return np.subtract(q3,fraction)\n",
    "    \n",
    "        \n",
    "    def optimized_maximin(self,points,show_strat_p1,show_strat_p2,FD_yn):\n",
    "        \"This is an optimized version for determining the maximin result\"\n",
    "        \n",
    "        print(\"Start of the maximin algorithm\")        \n",
    "         \n",
    "        def random_strategy_draw(points,number_of_actions):\n",
    "            \"This function draws random strategies from a beta distribution, based on the number of points and actions\"\n",
    "            \n",
    "            # draw some strategies and normalize them\n",
    "            strategies_drawn = np.random.beta(0.5,0.5,(points,number_of_actions))\n",
    "            strategies_drawn = strategies_drawn/np.sum(strategies_drawn, axis=1).reshape([points,1])\n",
    "            \n",
    "            return strategies_drawn\n",
    "         \n",
    "        def frequency_pairs_p1(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"Create strategies based on the best replies for player 1\"\n",
    "            \n",
    "            # store the size of the games\n",
    "            game_size_1 = self.payoff_p1_game1.size\n",
    "            game_size_2 = self.payoff_p1_game2.size\n",
    "            \n",
    "            # store the actions for game 1 and 2\n",
    "            p1_actions_game1 = self.payoff_p1_game1.shape[0]\n",
    "            p1_actions_game2 = self.payoff_p1_game2.shape[0]\n",
    "            \n",
    "            # calculate the combination of the actions and a range\n",
    "            p1_actions_combi = p1_actions_game1*p1_actions_game2\n",
    "            p1_action_range = np.arange(p1_actions_combi)\n",
    "            \n",
    "            # initialize a frequency pair\n",
    "            frequency_pairs = np.zeros((points*(p1_actions_game1*p1_actions_game2),game_size_1+game_size_2))\n",
    "            \n",
    "            # create actions ranges\n",
    "            p1_act_game1_range = np.arange(p1_actions_game1)\n",
    "            p1_act_game2_range = np.arange(p1_actions_game2)\n",
    "            \n",
    "            # loop over best responses for game 1\n",
    "            for i in np.nditer(p1_action_range):\n",
    "                for j in np.nditer(p1_act_game1_range):\n",
    "                    mod_remain = np.mod(i,p1_actions_game1)\n",
    "                    frequency_pairs[i*points:(i+1)*points,p1_actions_game1*mod_remain+j] = strategies_drawn[:,j]\n",
    "            \n",
    "            # loop over best responses for game 2\n",
    "            for i in np.nditer(p1_action_range):\n",
    "                for j in np.nditer(p1_act_game2_range):\n",
    "                    floor_div = np.floor_divide(i,p1_actions_game2)\n",
    "                    frequency_pairs[i*points:(i+1)*points,j+game_size_1+(p1_actions_game1*floor_div)] = strategies_drawn[:,p1_actions_game1+j]\n",
    "\n",
    "            return frequency_pairs\n",
    "        \n",
    "        def balance_equation(self,tot_act_ut,tot_act_thr,tot_payoffs_game1,tot_payoffs,frequency_pairs):\n",
    "            \"Calculates the result of the balance equations in order to adjust the frequency pairs\"\n",
    "            \n",
    "            # store the game sizes\n",
    "            game_size_1 = self.payoff_p1_game1.size\n",
    "            game_size_2 = self.payoff_p1_game2.size\n",
    "            \n",
    "            # initialize yi, Q and Q_new\n",
    "            yi = np.zeros((points*(tot_act_thr*tot_act_ut),game_size_1+game_size_2))\n",
    "            Q = np.zeros((1,points*(tot_act_thr*tot_act_ut)))\n",
    "            Q_new = np.zeros((1,points*(tot_act_thr*tot_act_ut))) \n",
    "            \n",
    "            # compute yi\n",
    "            yi[:,0:tot_payoffs_game1] = frequency_pairs[:,0:tot_payoffs_game1]/np.sum(frequency_pairs[:,0:tot_payoffs_game1], axis=1).reshape([points*tot_payoffs_game1,1])\n",
    "            yi[:,tot_payoffs_game1:tot_payoffs] = frequency_pairs[:,tot_payoffs_game1:tot_payoffs]/np.sum(frequency_pairs[:,tot_payoffs_game1:tot_payoffs], axis=1).reshape([points*(tot_payoffs-tot_payoffs_game1),1])\n",
    "            \n",
    "            index_values = np.arange(points*(tot_act_thr*tot_act_ut)) # set a range\n",
    "            \n",
    "            # set px range\n",
    "            p1_px_between = np.asarray(px)\n",
    "            p1_px = p1_px_between[0]\n",
    "            \n",
    "            # loop for 35 iterations\n",
    "            for i in np.arange(35):\n",
    "                \n",
    "                # in the first iteration we calculate the first Q and adjust X\n",
    "                if i == 0:\n",
    "                    new_x = p1_px - np.dot(frequency_pairs,self.etp_matrix)\n",
    "\n",
    "                    upper_part_Q = np.sum(np.multiply(yi[:,tot_payoffs_game1:tot_payoffs],new_x[:,tot_payoffs_game1:tot_payoffs]),axis=1)\n",
    "                    leftdown_part_Q = np.sum(np.multiply(yi[:,0:tot_payoffs_game1],(1-new_x[:,0:tot_payoffs_game1])),axis=1)\n",
    "                    rightdown_part_Q = np.sum(np.multiply(yi[:,tot_payoffs_game1:tot_payoffs],new_x[:,tot_payoffs_game1:tot_payoffs]),axis=1)\n",
    "\n",
    "                    Q_between = upper_part_Q/(leftdown_part_Q+rightdown_part_Q)\n",
    "                    Q = Q_between\n",
    "                    \n",
    "                    frequency_pairs[:,0:tot_payoffs_game1] = (np.multiply(Q,yi[:,0:tot_payoffs_game1]))\n",
    "                    frequency_pairs[:,tot_payoffs_game1:tot_payoffs] = np.multiply((1-Q),yi[:,tot_payoffs_game1:tot_payoffs])\n",
    "                \n",
    "                # here we just calculate Q in order to guarantee stability\n",
    "                if i > 0 and i < 10:\n",
    "                    new_x = p1_px - np.dot(frequency_pairs,self.etp_matrix)\n",
    "\n",
    "                    upper_part_Q = np.sum(np.multiply(yi[:,tot_payoffs_game1:tot_payoffs],new_x[:,tot_payoffs_game1:tot_payoffs]),axis=1)\n",
    "                    leftdown_part_Q = np.sum(np.multiply(yi[:,0:tot_payoffs_game1],(1-new_x[:,0:tot_payoffs_game1])),axis=1)\n",
    "                    rightdown_part_Q = np.sum(np.multiply(yi[:,tot_payoffs_game1:tot_payoffs],new_x[:,tot_payoffs_game1:tot_payoffs]),axis=1)\n",
    "\n",
    "                    Q_between = upper_part_Q/(leftdown_part_Q+rightdown_part_Q)\n",
    "                    Q = np.hstack((Q,Q_between))\n",
    "                    \n",
    "                    frequency_pairs[:,0:tot_payoffs_game1] = (np.multiply(Q[:,i],yi[:,0:tot_payoffs_game1]))\n",
    "                    frequency_pairs[:,tot_payoffs_game1:tot_payoffs] = np.multiply((1-Q[:,i]),yi[:,tot_payoffs_game1:tot_payoffs])\n",
    "                \n",
    "                # here we calculate Q based on aitken's delta squared\n",
    "                if i == 10:\n",
    "                    \n",
    "                    Q_new = self.aitken_delta_squared(Q[:,i-3],Q[:,i-2],Q[:,i-1])\n",
    "                    nan_org = np.where(np.isnan(Q_new)) # check where there are NaN's\n",
    "                    nan_indic = nan_org[0]\n",
    "                    Q_new[nan_indic,:] = Q_between[nan_indic,:] # remove NaN's with last known values\n",
    "                    \n",
    "                    Q_old = np.copy(Q_new)\n",
    "                    \n",
    "                    Q = np.hstack((Q,Q_new))\n",
    "\n",
    "                # all remaining iterations are with Aitkens  \n",
    "                if i > 10:\n",
    "\n",
    "                    Q_new[index_values,:] = self.aitken_delta_squared(Q[index_values,i-3],Q[index_values,i-2],Q[index_values,i-1])\n",
    "                    Q_old2 = np.copy(Q_old)\n",
    "                    nan_res = np.where(np.isnan(Q_new,Q_old)) # check for NaN's\n",
    "                    nan_indices = nan_res[0] # look where NaN's are\n",
    "                    \n",
    "                    # delete values which are NaN for future computations\n",
    "                    nan_between = np.where(np.in1d(index_values,nan_indices))\n",
    "                    nan_cands = nan_between[0]\n",
    "                    index_values = np.delete(index_values,nan_cands)\n",
    "                    \n",
    "                    Q_new[nan_indices,:] = Q_old2[nan_indices,:] \n",
    "                    \n",
    "                    Q = np.hstack((Q,Q_new))\n",
    "                    \n",
    "                    Q_old = np.copy(Q_new)\n",
    "                    results = np.where(Q[index_values,i-1] == Q[index_values,i]) # check where convergence has occured\n",
    "                    remove_indices = results[0]\n",
    "                    \n",
    "                    # remove indices which have converged\n",
    "                    removal_between = np.where(np.in1d(index_values,remove_indices))\n",
    "                    removal_cands = removal_between[0]\n",
    "                    index_values = np.delete(index_values,removal_cands)\n",
    "            \n",
    "            # compute the definitive frequency pair x\n",
    "            frequency_pairs[:,0:tot_payoffs_game1] = (np.multiply(Q[:,34],yi[:,0:tot_payoffs_game1]))\n",
    "            frequency_pairs[:,tot_payoffs_game1:tot_payoffs] = np.multiply((1-Q[:,34]),yi[:,tot_payoffs_game1:tot_payoffs])\n",
    "            \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def frequency_pairs_p2(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"Best responses for P2 based on threaten strategies drawn\"\n",
    "            \n",
    "            # store the game sizes\n",
    "            game_size_1 = self.payoff_p2_game1.size\n",
    "            game_size_2 = self.payoff_p2_game2.size\n",
    "            \n",
    "            # store the actions for p1 and p2 and create ranges\n",
    "            p1_actions_range = np.arange(p1_actions)\n",
    "            p2_actions_range = np.arange(p2_actions)\n",
    "            \n",
    "            p2_actions_game1 = self.payoff_p2_game1.shape[1]\n",
    "            p2_actions_game2 = self.payoff_p2_game2.shape[1]\n",
    "            \n",
    "            p2_actions_combo = p2_actions_game1*p2_actions_game2\n",
    "            p2_action_range = np.arange(p2_actions_combo)\n",
    "            \n",
    "            # initialize the frequency pair\n",
    "            frequency_pairs = np.zeros((points*(p2_actions_game1*p2_actions_game2),game_size_1+game_size_2))\n",
    "            \n",
    "            # generate best responses for game 1\n",
    "            for i in np.nditer(np.arange(p2_actions_game1)):\n",
    "                for j in np.nditer(p2_action_range):\n",
    "                    modul = np.mod(j,p2_actions_game1)\n",
    "                    frequency_pairs[j*points:(j+1)*points,p2_actions_game1*i+modul] = strategies_drawn[:,i]\n",
    "            \n",
    "            # generate best respones for game 2\n",
    "            for i in np.nditer(np.arange(p2_actions_game2)):\n",
    "                for j in np.nditer(p2_action_range):\n",
    "                    divide = np.floor_divide(j,p2_actions_game2)\n",
    "                    frequency_pairs[j*points:(j+1)*points,p2_actions_combo+divide+(i*p2_actions_game2)] = strategies_drawn[:,i+p2_actions_game1]                 \n",
    "                    \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def payoffs_sorted(points,payoffs,actions):\n",
    "            \"Sort the payoffs for determination of maximin\"\n",
    "            \n",
    "            # store the range of points and actions\n",
    "            points_range = np.arange(points)\n",
    "            actions_range = np.arange(actions)\n",
    "        \n",
    "            payoffs_sort = np.zeros((points,actions)) # initialize the payoffs sort\n",
    "            \n",
    "            # sort the payoffs!\n",
    "            for x in np.nditer(points_range):\n",
    "                for i in np.nditer(actions_range):\n",
    "                    payoffs_sort[x,i] = payoffs[points*i+x]\n",
    "            \n",
    "            return payoffs_sort\n",
    "        \n",
    "        ## Start of p1 maximin ##\n",
    "        \n",
    "        start_time = time.time() # START TIME\n",
    "        \n",
    "        # flatten the transition matrices\n",
    "        flatten1_1 = self.transition_matrix_game1_to1.flatten()\n",
    "        flatten2_1 = self.transition_matrix_game2_to1.flatten()\n",
    "        \n",
    "        # store and compute some action stuff\n",
    "        actions_p2_game1 = self.payoff_p1_game1.shape[1]\n",
    "        actions_p2_game2 = self.payoff_p1_game2.shape[1]\n",
    "        total_actions_p2 = actions_p2_game1 + actions_p2_game2\n",
    "        \n",
    "        actions_p1_game1 = self.payoff_p1_game1.shape[0]\n",
    "        actions_p1_game2 = self.payoff_p1_game2.shape[0]\n",
    "        total_actions_p1 = actions_p1_game1 + actions_p1_game2\n",
    "        \n",
    "        # flatten the payoffs\n",
    "        payoff_p1_game_1flatten = self.payoff_p1_game1.flatten()\n",
    "        payoff_p1_game_2flatten = self.payoff_p1_game2.flatten()\n",
    "        \n",
    "        # compute and store some payoffs stuff\n",
    "        total_payoffs_p1_game1 = payoff_p1_game_1flatten.size\n",
    "        total_payoffs_p1_game2 = payoff_p1_game_2flatten.size\n",
    "        total_payoffs_p1 = total_payoffs_p1_game1 + total_payoffs_p1_game2\n",
    "        \n",
    "        payoff_p2_game_1flatten = self.payoff_p2_game1.flatten()\n",
    "        payoff_p2_game_2flatten = self.payoff_p2_game2.flatten()\n",
    "\n",
    "        total_payoffs_p2_game1 = payoff_p2_game_1flatten.size\n",
    "        total_payoffs_p2_game2 = payoff_p2_game_2flatten.size\n",
    "        total_payoffs_p2 = total_payoffs_p2_game1 + total_payoffs_p2_game2\n",
    "        \n",
    "        total_payoffs_p2_game1 = payoff_p2_game_1flatten.size\n",
    "        total_payoffs_p2_game2 = payoff_p2_game_2flatten.size\n",
    "        total_payoffs_p2 = total_payoffs_p2_game1 + total_payoffs_p2_game2\n",
    "        \n",
    "        # initialize the payoff stuff for p1\n",
    "        payoff_p1 = np.zeros(total_payoffs_p1)\n",
    "        payoff_p1[0:total_payoffs_p1_game1] = payoff_p1_game_1flatten\n",
    "        payoff_p1[total_payoffs_p1_game1:total_payoffs_p1] = payoff_p1_game_2flatten\n",
    "\n",
    "        px = np.concatenate([flatten1_1,flatten2_1],axis=1) # px for the first time\n",
    "        \n",
    "        y_punisher = random_strategy_draw(points,total_actions_p1) # draw some strategies\n",
    "\n",
    "        frequency_pairs = frequency_pairs_p2(points,total_actions_p1,total_actions_p2,y_punisher) # sort them based on best replies\n",
    "        \n",
    "        # do the balance equations with Aitken's\n",
    "        frequency_pairs = balance_equation(self,actions_p2_game1,actions_p2_game2,total_payoffs_p2_game1,total_payoffs_p2,frequency_pairs)\n",
    "        \n",
    "        # activate FD function if necessary\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+frequency_pairs[:,2])-(1/3)*frequency_pairs[:,3]-(1/2)*(frequency_pairs[:,5] + frequency_pairs[:,6]) - (2/3) * frequency_pairs[:,7]\n",
    "        else:\n",
    "            FD = 1     \n",
    "        \n",
    "        # calculate the payoffs\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p1),axis=1)\n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "        \n",
    "        max_payoffs = payoffs_sorted(points,payoffs,(actions_p2_game1*actions_p2_game2)) # sort the payoffs\n",
    "\n",
    "        nan_delete = np.where(np.isnan(max_payoffs)) # delete results which are NaN (see thesis why)\n",
    "\n",
    "        max_payoffs = np.delete(max_payoffs,nan_delete[0],0) # actually delete these payoffs\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        minimax_found = np.nanmax(np.nanmin(max_payoffs,axis=1)) # look for maximin value\n",
    "        print(\"Maximin value for P1 is\",minimax_found)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "        if show_strat_p1 == True:\n",
    "            minimax_indices_p2 = np.where(max_payoffs == minimax_found)\n",
    "            found_strategy_p2 = y_punisher[minimax_indices_p2[0]]\n",
    "            fnd_strategy_p2 = found_strategy_p2.flatten()\n",
    "            fnd_strategy_p2[0:2] = fnd_strategy_p2[0:2]/np.sum(fnd_strategy_p2[0:2])\n",
    "            fnd_strategy_p2[2:4] = fnd_strategy_p2[2:4]/np.sum(fnd_strategy_p2[2:4])\n",
    "            print(\"Player 1 plays stationary strategy:\", fnd_strategy_p2)\n",
    "            print(\"While player 2 replies with a best pure reply of:\", self.best_pure_strategies[minimax_indices_p2[1]])\n",
    "            \n",
    "        end_time = time.time()\n",
    "        print(\"Seconds done to generate\", points, \"points\", end_time-start_time)\n",
    "        \n",
    "        ## End of P1 maximin algorithm ##\n",
    "        \n",
    "        start_time_p2 = time.time() # start the time \n",
    "        \n",
    "        # flatten the payoffs\n",
    "        payoff_p2_game_1flatten = self.payoff_p2_game1.flatten()\n",
    "        payoff_p2_game_2flatten = self.payoff_p2_game2.flatten()\n",
    "        \n",
    "        # compute and store the payoffs\n",
    "        total_payoffs_p2_game1 = payoff_p2_game_1flatten.size\n",
    "        total_payoffs_p2_game2 = payoff_p2_game_2flatten.size\n",
    "        total_payoffs_p2 = total_payoffs_p2_game1 + total_payoffs_p2_game2\n",
    "        \n",
    "        # initialize the payoffs and store them\n",
    "        payoff_p2 = np.zeros(total_payoffs_p2)\n",
    "        payoff_p2[0:total_payoffs_p2_game1] = payoff_p2_game_1flatten\n",
    "        payoff_p2[total_payoffs_p2_game1:total_payoffs_p2] = payoff_p2_game_2flatten\n",
    "\n",
    "        px = np.concatenate([flatten1_1,flatten2_1],axis=1) # px store\n",
    "\n",
    "        x_punisher = random_strategy_draw(points,total_actions_p2) # generate new random strategies for punsher\n",
    "         \n",
    "        frequency_pairs = frequency_pairs_p1(points,total_actions_p1,total_actions_p2,x_punisher) # best reponses p1\n",
    "        \n",
    "        # balance equations with Delta Squared\n",
    "        frequency_pairs = balance_equation(self,actions_p1_game1,actions_p1_game2,total_payoffs_p1_game1,total_payoffs_p1,frequency_pairs)\n",
    "        \n",
    "        # activate FD function if necessary\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+frequency_pairs[:,2])-(1/3)*frequency_pairs[:,3]-(1/2)*(frequency_pairs[:,5] + frequency_pairs[:,6]) - (2/3) * frequency_pairs[:,7]\n",
    "        else:\n",
    "            FD = 1     \n",
    "        \n",
    "        # compute the payoffs with payoffs and FD function\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p2),axis=1)        \n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "        \n",
    "        max_payoffs = payoffs_sorted(points,payoffs,(actions_p1_game1*actions_p1_game2)) # sort the payoffs\n",
    "        \n",
    "        nan_delete = np.where(np.isnan(max_payoffs)) # check where there are nan's\n",
    "\n",
    "        max_payoffs = np.delete(max_payoffs,nan_delete[0],0) # delete these nan's\n",
    "        \n",
    "        minimax_found_p2 = np.nanmax(np.nanmin(max_payoffs,axis=1)) # find the maximin value for p2\n",
    "        print(\"Maximin value for P2 is\",minimax_found_p2)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "\n",
    "        if show_strat_p2 == True:\n",
    "            maximin_indices_p2 = np.where(max_payoffs == minimax_found_p2)\n",
    "            found_strategy = x_punisher[maximin_indices_p2[0]]\n",
    "            fnd_strategy = found_strategy.flatten()\n",
    "            fnd_strategy[0:2] = fnd_strategy[0:2]/np.sum(fnd_strategy[0:2])\n",
    "            fnd_strategy[2:4] = fnd_strategy[2:4]/np.sum(fnd_strategy[2:4])\n",
    "            print(\"Player 2 plays stationairy strategy:\", fnd_strategy)\n",
    "            print(\"While player 2 replies with a best pure reply of:\", self.best_pure_strategies[maximin_indices_p2[1]])\n",
    "        \n",
    "        end_time_p2 = time.time() # end the timer\n",
    "        print(\"Seconds done to generate\", points, \"points\", end_time_p2-start_time_p2)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "       \n",
    "    def threat_point_optimized(self,points,show_strat_p1,show_strat_p2,print_text,FD_yn):\n",
    "        \"OPtimized threat point algorithm for ETP games\"\n",
    "        \n",
    "        def random_strategy_draw(points,number_of_actions):\n",
    "            \"This function draws random strategies from a beta distribution, based on the number of points and actions\"\n",
    "            \n",
    "            # draw some strategies and normalize them\n",
    "            strategies_drawn = np.random.beta(0.5,0.5,(points,number_of_actions))\n",
    "            strategies_drawn = strategies_drawn/np.sum(strategies_drawn, axis=1).reshape([points,1])\n",
    "            \n",
    "            return strategies_drawn\n",
    "        \n",
    "        def frequency_pairs_p1(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"This function sorts the strategies based on the responses\"\n",
    "            \n",
    "            # store the game size\n",
    "            game_size_1 = self.payoff_p1_game1.size\n",
    "            game_size_2 = self.payoff_p1_game2.size\n",
    "            \n",
    "            # store the actions of p1 in both game\n",
    "            p1_actions_game1 = self.payoff_p1_game1.shape[0]\n",
    "            p1_actions_game2 = self.payoff_p1_game2.shape[0]\n",
    "            \n",
    "            p1_actions_combi = p1_actions_game1*p1_actions_game2\n",
    "            p1_action_range = np.arange(p1_actions_combi)\n",
    "            \n",
    "            # initialize frequency pairs\n",
    "            frequency_pairs = np.zeros((points*(p1_actions_game1*p1_actions_game2),game_size_1+game_size_2))\n",
    "            \n",
    "            # set the range for both games\n",
    "            p1_act_game1_range = np.arange(p1_actions_game1)\n",
    "            p1_act_game2_range = np.arange(p1_actions_game2)\n",
    "            \n",
    "            # create best response for game 1\n",
    "            for i in np.nditer(p1_action_range):\n",
    "                for j in np.nditer(p1_act_game1_range):\n",
    "                    mod_remain = np.mod(i,p1_actions_game1)\n",
    "                    frequency_pairs[i*points:(i+1)*points,p1_actions_game1*mod_remain+j] = strategies_drawn[:,j]\n",
    "            \n",
    "            # loop for best responses for game 2\n",
    "            for i in np.nditer(p1_action_range):\n",
    "                for j in np.nditer(p1_act_game2_range):\n",
    "                    floor_div = np.floor_divide(i,p1_actions_game2)\n",
    "                    frequency_pairs[i*points:(i+1)*points,j+game_size_1+(p1_actions_game1*floor_div)] = strategies_drawn[:,p1_actions_game1+j]\n",
    "            return frequency_pairs\n",
    "         \n",
    "        def balance_equation(self,tot_act_ut,tot_act_thr,tot_payoffs_game1,tot_payoffs,frequency_pairs):\n",
    "            \"Calculates the result of the balance equations in order to adjust the frequency pairs\"\n",
    "            \n",
    "            # store the game sizes\n",
    "            game_size_1 = self.payoff_p1_game1.size\n",
    "            game_size_2 = self.payoff_p1_game2.size\n",
    "            \n",
    "            # initialize yi, Q and Q_new\n",
    "            yi = np.zeros((points*(tot_act_thr*tot_act_ut),game_size_1+game_size_2))\n",
    "            Q = np.zeros((1,points*(tot_act_thr*tot_act_ut)))\n",
    "            Q_new = np.zeros((1,points*(tot_act_thr*tot_act_ut))) \n",
    "            \n",
    "            # compute Yi\n",
    "            yi[:,0:tot_payoffs_game1] = frequency_pairs[:,0:tot_payoffs_game1]/np.sum(frequency_pairs[:,0:tot_payoffs_game1], axis=1).reshape([points*tot_payoffs_game1,1])\n",
    "            yi[:,tot_payoffs_game1:tot_payoffs] = frequency_pairs[:,tot_payoffs_game1:tot_payoffs]/np.sum(frequency_pairs[:,tot_payoffs_game1:tot_payoffs], axis=1).reshape([points*(tot_payoffs-tot_payoffs_game1),1])\n",
    "            \n",
    "            index_values = np.arange(points*(tot_act_thr*tot_act_ut)) # create a range of index values\n",
    "            \n",
    "            p1_px_between = np.asarray(px) # set px\n",
    "            p1_px = p1_px_between[0]\n",
    "            \n",
    "            # iterate for 35 iterations\n",
    "            for i in np.arange(35):\n",
    "                \n",
    "                # first iteration, just calculate Q\n",
    "                if i == 0:\n",
    "                    new_x = p1_px - np.dot(frequency_pairs,self.etp_matrix)\n",
    "\n",
    "                    upper_part_Q = np.sum(np.multiply(yi[:,tot_payoffs_game1:tot_payoffs],new_x[:,tot_payoffs_game1:tot_payoffs]),axis=1)\n",
    "                    leftdown_part_Q = np.sum(np.multiply(yi[:,0:tot_payoffs_game1],(1-new_x[:,0:tot_payoffs_game1])),axis=1)\n",
    "                    rightdown_part_Q = np.sum(np.multiply(yi[:,tot_payoffs_game1:tot_payoffs],new_x[:,tot_payoffs_game1:tot_payoffs]),axis=1)\n",
    "\n",
    "                    Q_between = upper_part_Q/(leftdown_part_Q+rightdown_part_Q)\n",
    "                    Q = Q_between\n",
    "                    \n",
    "                    frequency_pairs[:,0:tot_payoffs_game1] = (np.multiply(Q,yi[:,0:tot_payoffs_game1]))\n",
    "                    frequency_pairs[:,tot_payoffs_game1:tot_payoffs] = np.multiply((1-Q),yi[:,tot_payoffs_game1:tot_payoffs])\n",
    "                \n",
    "                # for stability, calculate until iteration 9 normal Q\n",
    "                if i > 0 and i < 10:\n",
    "                    new_x = p1_px - np.dot(frequency_pairs,self.etp_matrix)\n",
    "\n",
    "                    upper_part_Q = np.sum(np.multiply(yi[:,tot_payoffs_game1:tot_payoffs],new_x[:,tot_payoffs_game1:tot_payoffs]),axis=1)\n",
    "                    leftdown_part_Q = np.sum(np.multiply(yi[:,0:tot_payoffs_game1],(1-new_x[:,0:tot_payoffs_game1])),axis=1)\n",
    "                    rightdown_part_Q = np.sum(np.multiply(yi[:,tot_payoffs_game1:tot_payoffs],new_x[:,tot_payoffs_game1:tot_payoffs]),axis=1)\n",
    "\n",
    "                    Q_between = upper_part_Q/(leftdown_part_Q+rightdown_part_Q)\n",
    "                    Q = np.hstack((Q,Q_between))\n",
    "                    \n",
    "                    frequency_pairs[:,0:tot_payoffs_game1] = (np.multiply(Q[:,i],yi[:,0:tot_payoffs_game1]))\n",
    "                    frequency_pairs[:,tot_payoffs_game1:tot_payoffs] = np.multiply((1-Q[:,i]),yi[:,tot_payoffs_game1:tot_payoffs])\n",
    "                \n",
    "                # apply Aitken's\n",
    "                if i == 10:\n",
    "                    \n",
    "                    Q_new = self.aitken_delta_squared(Q[:,i-3],Q[:,i-2],Q[:,i-1])\n",
    "                    nan_org = np.where(np.isnan(Q_new)) # check whether Nan's occur\n",
    "                    nan_indic = nan_org[0]\n",
    "                    \n",
    "                    Q_new[nan_indic,:] = Q_between[nan_indic,:] # replace NaN with last known value\n",
    "                    \n",
    "                    Q_old = np.copy(Q_new)\n",
    "                    \n",
    "                    Q = np.hstack((Q,Q_new))\n",
    "\n",
    "                # and only Aitken's\n",
    "                if i > 10:\n",
    "\n",
    "                    Q_new[index_values,:] = self.aitken_delta_squared(Q[index_values,i-3],Q[index_values,i-2],Q[index_values,i-1])\n",
    "                    Q_old2 = np.copy(Q_old)\n",
    "                    nan_res = np.where(np.isnan(Q_new,Q_old)) # check for NaN's\n",
    "                    nan_indices = nan_res[0]\n",
    "                    \n",
    "                    nan_between = np.where(np.in1d(index_values,nan_indices))\n",
    "                    nan_cands = nan_between[0]\n",
    "                    \n",
    "                    index_values = np.delete(index_values,nan_cands) # delete NaN's after being returned in last known\n",
    "                    \n",
    "                    Q_new[nan_indices,:] = Q_old2[nan_indices,:]\n",
    "                    \n",
    "                    Q = np.hstack((Q,Q_new))\n",
    "                    \n",
    "                    Q_old = np.copy(Q_new)\n",
    "                    results = np.where(Q[index_values,i-1] == Q[index_values,i]) # check whether Q converged\n",
    "                    remove_indices = results[0]\n",
    "\n",
    "                    removal_between = np.where(np.in1d(index_values,remove_indices))\n",
    "                    removal_cands = removal_between[0]\n",
    "                    index_values = np.delete(index_values,removal_cands)\n",
    "            \n",
    "            # compute definitive x\n",
    "            frequency_pairs[:,0:tot_payoffs_game1] = (np.multiply(Q[:,34],yi[:,0:tot_payoffs_game1]))\n",
    "            frequency_pairs[:,tot_payoffs_game1:tot_payoffs] = np.multiply((1-Q[:,34]),yi[:,tot_payoffs_game1:tot_payoffs])\n",
    "        \n",
    "            return frequency_pairs\n",
    "       \n",
    "        def frequency_pairs_p2(points,p2_actions,p1_actions,strategies_drawn):\n",
    "            \"Create frequency pairs for P2 based on best responses\"\n",
    "            \n",
    "            # store the size of the games\n",
    "            game_size_1 = self.payoff_p2_game1.size\n",
    "            game_size_2 = self.payoff_p2_game2.size\n",
    "            \n",
    "            # store the ranges of the actions of both players\n",
    "            p1_actions_range = np.arange(p1_actions)\n",
    "            p2_actions_range = np.arange(p2_actions)\n",
    "            \n",
    "            p2_actions_game1 = self.payoff_p2_game1.shape[1]\n",
    "            p2_actions_game2 = self.payoff_p2_game2.shape[1]\n",
    "            \n",
    "            p2_actions_combo = p2_actions_game1*p2_actions_game2\n",
    "            p2_action_range = np.arange(p2_actions_combo)\n",
    "            \n",
    "            # initialize frequency pairs\n",
    "            frequency_pairs = np.zeros((points*(p2_actions_game1*p2_actions_game2),game_size_1+game_size_2))\n",
    "            \n",
    "            # loop over the first game\n",
    "            for i in np.nditer(np.arange(p2_actions_game1)):\n",
    "                for j in np.nditer(p2_action_range):\n",
    "                    modul = np.mod(j,p2_actions_game1)\n",
    "                    frequency_pairs[j*points:(j+1)*points,p2_actions_game1*i+modul] = strategies_drawn[:,i]\n",
    "            # loop over the second game\n",
    "            for i in np.nditer(np.arange(p2_actions_game2)):\n",
    "                for j in np.nditer(p2_action_range):\n",
    "                    divide = np.floor_divide(j,p2_actions_game2)\n",
    "                    frequency_pairs[j*points:(j+1)*points,p2_actions_combo+divide+(i*p2_actions_game2)] = strategies_drawn[:,i+p2_actions_game1]\n",
    "                    \n",
    "            return frequency_pairs\n",
    "        \n",
    "        def payoffs_sorted(points,payoffs,actions):\n",
    "            \"This function sorts the payoffs in order to prepare the threat point\"\n",
    "            \n",
    "            # create ranges for points and actions\n",
    "            points_range = np.arange(points)\n",
    "            actions_range = np.arange(actions)\n",
    "        \n",
    "            payoffs_sort = np.zeros((points,actions)) # nitialize the payoffs sort\n",
    "            \n",
    "            # sort the payoffs!\n",
    "            for x in np.nditer(points_range):\n",
    "                for i in np.nditer(actions_range):\n",
    "                    payoffs_sort[x,i] = payoffs[points*i+x]\n",
    "            \n",
    "            return payoffs_sort\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"The start of the algorithm for finding the threat point\")\n",
    "            print(\"First let's find the threat point for Player 1\")\n",
    "\n",
    "        # flatten the transition matrices\n",
    "        flatten1_1 = self.transition_matrix_game1_to1.flatten()\n",
    "        flatten2_1 = self.transition_matrix_game2_to1.flatten()\n",
    "        \n",
    "        #  store the actions for both players\n",
    "        actions_p2_game1 = self.payoff_p1_game1.shape[1]\n",
    "        actions_p2_game2 = self.payoff_p1_game2.shape[1]\n",
    "        total_actions_p2 = actions_p2_game1 + actions_p2_game2\n",
    "        \n",
    "        actions_p1_game1 = self.payoff_p1_game1.shape[0]\n",
    "        actions_p1_game2 = self.payoff_p1_game2.shape[0]\n",
    "        total_actions_p1 = actions_p1_game1 + actions_p1_game2\n",
    "        \n",
    "        # Start of algorithm for player 1\n",
    "        \n",
    "        start_time = time.time() # timer start\n",
    "        \n",
    "        # flatten payoffs game 1 and 2        \n",
    "        payoff_p1_game_1flatten = self.payoff_p1_game1.flatten()\n",
    "        payoff_p1_game_2flatten = self.payoff_p1_game2.flatten()\n",
    "        \n",
    "        # store size of the payoffs\n",
    "        total_payoffs_p1_game1 = payoff_p1_game_1flatten.size\n",
    "        total_payoffs_p1_game2 = payoff_p1_game_2flatten.size\n",
    "        total_payoffs_p1 = total_payoffs_p1_game1 + total_payoffs_p1_game2\n",
    "        \n",
    "        # initialize and assign payoffs\n",
    "        payoff_p1 = np.zeros(total_payoffs_p1)\n",
    "        payoff_p1[0:total_payoffs_p1_game1] = payoff_p1_game_1flatten\n",
    "        payoff_p1[total_payoffs_p1_game1:total_payoffs_p1] = payoff_p1_game_2flatten\n",
    "        \n",
    "        px = np.concatenate([flatten1_1,flatten2_1],axis=1) # store px\n",
    "        \n",
    "        y_punisher = random_strategy_draw(points,total_actions_p2) # draw strategies for the punisher\n",
    "              \n",
    "        frequency_pairs = frequency_pairs_p1(points,total_actions_p2,total_actions_p1,y_punisher) # sort based on best reply\n",
    "        \n",
    "        # do the balance equations calculations\n",
    "        frequency_pairs = balance_equation(self,actions_p1_game1,actions_p1_game2,total_payoffs_p1_game1,total_payoffs_p1,frequency_pairs)\n",
    "        \n",
    "        # activate the FD function\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+frequency_pairs[:,2])-(1/3)*frequency_pairs[:,3]-(1/2)*(frequency_pairs[:,5] + frequency_pairs[:,6]) - (2/3) * frequency_pairs[:,7]\n",
    "        else:\n",
    "            FD = 1     \n",
    "        \n",
    "        # calculate the payoffs with the frequency pairs and FD function\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p1),axis=1)\n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "        \n",
    "        max_payoffs = payoffs_sorted(points,payoffs,(actions_p1_game1*actions_p1_game2)) # sort the payoffs\n",
    "        nan_delete = np.where(np.isnan(max_payoffs)) # delete payoffs which are a NaN\n",
    "\n",
    "        max_payoffs_p1 = np.delete(max_payoffs,nan_delete[0],0) # actually delete them\n",
    "        threat_point_p1 = np.nanmin(np.nanmax(max_payoffs_p1,axis=1)) # determine the threat point\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"Threat point value is\",threat_point_p1)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "        \n",
    "        if show_strat_p1 == True:\n",
    "            threat_point_indices_p1 = np.where(max_payoffs_p1 == threat_point_p1)\n",
    "            found_strategy_p1 = y_punisher[threat_point_indices_p1[0]]\n",
    "            fnd_strategy_p1 = found_strategy_p1.flatten()\n",
    "            fnd_strategy_p1[0:2] = fnd_strategy_p1[0:2]/np.sum(fnd_strategy_p1[0:2])\n",
    "            fnd_strategy_p1[2:4] = fnd_strategy_p1[2:4]/np.sum(fnd_strategy_p1[2:4])\n",
    "            print(\"Player 2 plays stationary strategy:\", fnd_strategy_p1)\n",
    "            print(\"While player 1 replies with a best pure reply of:\", self.best_pure_strategies[threat_point_indices_p1[1]])\n",
    "            \n",
    "        end_time = time.time() # stop the time!\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"Seconds done to generate\", points, \"points\", end_time-start_time)\n",
    "            print(\"\")\n",
    "        \n",
    "        # End of algorithm player 1\n",
    "        \n",
    "        # Start of algorithm player 2\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"First start the threat point for player 2\")\n",
    "        start_time_p2 = time.time() # start the time (for p2)\n",
    "        \n",
    "        # flatten the payoffs of the gamew\n",
    "        payoff_p2_game_1flatten = self.payoff_p2_game1.flatten()\n",
    "        payoff_p2_game_2flatten = self.payoff_p2_game2.flatten()\n",
    "        \n",
    "        # check the sizes of the total payoffs\n",
    "        total_payoffs_p2_game1 = payoff_p2_game_1flatten.size\n",
    "        total_payoffs_p2_game2 = payoff_p2_game_2flatten.size\n",
    "        total_payoffs_p2 = total_payoffs_p2_game1 + total_payoffs_p2_game2\n",
    "        \n",
    "        # initialize the payoffs for p2 and assign them\n",
    "        payoff_p2 = np.zeros(total_payoffs_p2)\n",
    "        payoff_p2[0:total_payoffs_p2_game1] = payoff_p2_game_1flatten\n",
    "        payoff_p2[total_payoffs_p2_game1:total_payoffs_p2] = payoff_p2_game_2flatten\n",
    "\n",
    "        px = np.concatenate([flatten1_1,flatten2_1],axis=1) # trix with px\n",
    "\n",
    "        x_punisher = random_strategy_draw(points,total_actions_p1) # draw some awesome strategies\n",
    "\n",
    "        frequency_pairs = frequency_pairs_p2(points,total_actions_p2,total_actions_p1,x_punisher) # sort them based on best replies\n",
    "        \n",
    "        # do some balance equation accelerator magic\n",
    "        frequency_pairs = balance_equation(self,actions_p2_game1,actions_p2_game2,total_payoffs_p2_game1,total_payoffs_p2,frequency_pairs)\n",
    "    \n",
    "        # if the FD function must be activated, activate it\n",
    "        if FD_yn == True:\n",
    "            FD = 1-0.25*(frequency_pairs[:,1]+frequency_pairs[:,2])-(1/3)*frequency_pairs[:,3]-(1/2)*(frequency_pairs[:,5] + frequency_pairs[:,6]) - (2/3) * frequency_pairs[:,7]\n",
    "        else:\n",
    "            FD = 1     \n",
    "        \n",
    "        # payoffs are calculated\n",
    "        payoffs = np.sum(np.multiply(frequency_pairs,payoff_p2),axis=1)\n",
    "        payoffs = np.multiply(FD,payoffs)\n",
    "        payoffs = payoffs.reshape((payoffs.size,1))\n",
    "    \n",
    "        max_payoffs = payoffs_sorted(points,payoffs,(actions_p2_game1*actions_p2_game2)) # awesome sorting process\n",
    "        nan_delete = np.where(np.isnan(max_payoffs)) # look for NaN's\n",
    "\n",
    "        max_payoffs_p2 = np.delete(max_payoffs,nan_delete[0],0) # delete them where necessary    \n",
    "        threat_point_p2 = np.nanmin(np.nanmax(max_payoffs_p2,axis=1)) # determine the threat point\n",
    "        \n",
    "        print(payoffs)\n",
    "        print(np.nanmax(payoffs))\n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"Threat point value is\",threat_point_p2)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "        \n",
    "        if show_strat_p2 == True:\n",
    "            threat_point_indices_p2 = np.where(max_payoffs_p2 == threat_point_p2)\n",
    "            found_strategy = x_punisher[threat_point_indices_p2[0]]\n",
    "            fnd_strategy = found_strategy.flatten()\n",
    "            fnd_strategy[0:2] = fnd_strategy[0:2]/np.sum(fnd_strategy[0:2])\n",
    "            fnd_strategy[2:4] = fnd_strategy[2:4]/np.sum(fnd_strategy[2:4])\n",
    "            print(\"Player 1 plays stationairy strategy:\", fnd_strategy)\n",
    "            print(\"While player 2 replies with a best pure reply of:\", self.best_pure_strategies[threat_point_indices_p2[1]])\n",
    "            \n",
    "        end_time_p2 = time.time() # stop the time\n",
    "        \n",
    "        if print_text == True:\n",
    "            print(\"\")\n",
    "            print(\"Seconds done to generate\", points, \"points\", end_time_p2-start_time_p2)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "        \n",
    "        self.threat_point = np.zeros(2)\n",
    "        self.threat_point = [threat_point_p1,threat_point_p2] # store the threat point!\n",
    "        \n",
    "        return [threat_point_p1,threat_point_p2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of the maximin algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HarmelinkRLA\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:229: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\HarmelinkRLA\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:229: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\HarmelinkRLA\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:376: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\HarmelinkRLA\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:377: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Maximin value for P1 is 4.45863901112896\n",
      "\n",
      "\n",
      "Seconds done to generate 100000 points 5.769620895385742\n",
      "Maximin value for P2 is 4.458709573181736\n",
      "\n",
      "\n",
      "Seconds done to generate 100000 points 6.140970706939697\n",
      "\n",
      "\n",
      "The start of the algorithm for finding the threat point\n",
      "First let's find the threat point for Player 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HarmelinkRLA\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:719: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\HarmelinkRLA\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:720: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Threat point value is 4.460099200359857\n",
      "\n",
      "\n",
      "Player 2 plays stationary strategy: [0.99710076 0.00289924 0.498112   0.501888  ]\n",
      "While player 1 replies with a best pure reply of: [[0 1 1 0]]\n",
      "Seconds done to generate 10000 points 0.5405879020690918\n",
      "\n",
      "\n",
      "\n",
      "First start the threat point for player 2\n",
      "[[5.77156449]\n",
      " [4.71351353]\n",
      " [6.51145038]\n",
      " ...\n",
      " [5.96112274]\n",
      " [2.63238176]\n",
      " [2.49998949]]\n",
      "13.608378417700441\n",
      "\n",
      "\n",
      "Threat point value is 4.45952530703882\n",
      "\n",
      "\n",
      "Player 1 plays stationairy strategy: [0.91422247 0.08577753 0.80559649 0.19440351]\n",
      "While player 2 replies with a best pure reply of: [[0 1 1 0]]\n",
      "\n",
      "Seconds done to generate 10000 points 0.5494990348815918\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HarmelinkRLA\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:146: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\HarmelinkRLA\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:147: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VOX1+PHPmSSSsEYQEFkEVNYQAgZkF0oBdxG1raUqKloUN75KxeXn1lKrUrcurlWsUisu0Na6g4IoFoJEdghLhACyGtYQksz5/XFv4iRkmYSZuZnMeb9e88rMXU9uJnPmPs+95xFVxRhjTOzyeR2AMcYYb1kiMMaYGGeJwBhjYpwlAmOMiXGWCIwxJsZZIjDGmBhnicCElIhMF5HfhWG794jIS6Hebg3iuFFEdojIQRFp5nU8xoSCJYIoISLZIpLnfgB9737gNvQ6rkhR1d+r6vhglhWRB0Xk9VDHICIJwBPASFVtqKp7ysxvLyLq/o2KH9+688aJSFHA9E0i8oqIdKpgX2MDls0TEX/gdkP9u1VGRHJEZKj7fLyIfF7ePBO9LBFElwtVtSGQBvQC7vYqEBGJ92rfHmoJJAIrq1gu2U0UDVW1Z8D0he7frwnwUyAPWCIiKWU3oKozircBnAtsC9hmzHwBMJFhiSAKqer3wEc4CQEAEaknItNEZLPbdPGciCS58+aJyKXu80Hut9bz3Nc/FZFM9/lpIjJXRPaIyG4RmSEiyQH7yBaRu0RkGXBIROJFpJeIfCMiB0TkTZwPynK534q/FJE/icg+EVkjIsMD5p8iIv8Wkb0isl5Erg+YV/ItP+Cb99Xu77tbRO51550D3AP8vJxv5BvdODeJyNgKYqwnIk+JyDb38ZQ7rROw1l0sV0TmBvv3KktVi1R1g6reBMwDHqzuNkTkbvd4B057VkSmuc8XiMhUEclwj/UsETkxYNmBIvK1iOSKSKaIDKnp71NJjPVF5HV3H6tEZIqIZAfMvy/gb7JSRC4KmDfefd8+466/XkTOEpHrRGSL+x7/VcDyiSLyRMC8v4pIhe9FU5olgigkIm1wviWuD5j8KNAJJzmcDrQG7nfnzQOGus+HABuBswNezyveNPAIcArQFWjLsR9SVwDnA8k475/ZwGtAU+At4NIqwj/L3f9JwAPAuyLS1J33BpDj7v8y4PeBiaIcg4DOwHDgfhHpqqofAr8H3iz+Ri4iDYBngHNVtREwAMisYJv3Av1wjmNPoC9wn6quA7q7yySr6k+q+D2D9S4wuAbrvQacLyKNAUTkBOByd3qxq9zHKTh/2yfdZdsC/8Y5/k2BKTh/h1D3eTzs7rs9MAr4VZn564CBOGdIU4F/iEjLgPkDgcVAM+BtYCbO3+R04BrgLyJS3112GtABSAXOcPd5b4h/n7pLVe0RBQ8gGzgIHAAUmIPzgQTOP/kh4LSA5fsDm9znw4Fl7vMPgfHA1+7recCYCvY5GlhaJoZrA14PAbYBEjDtK+B3FWxvXDnLLwKuxEk6RUCjgHmPANPd5w8Cr7vP27vHoE2Z7fyi7LLu6wZALk6SSqriOG8Azgt4PQrILrPf+ArWLZ6fG/C4M+B3X1DOOucABVXENBTIKWf6J8A1AX+rZQHzFgT+HXA+II+475V7gVfKbGsOMLaC/ecAQ93n44HPy5tXznqbgeEBrycUH8sKll8BnB+wn9UB83q5x7ZZwLR9QArOF5IjwKkB8wYDWZH436wLDzsjiC6j1flGOxTogvOtGqA5UB+nvTlXRHJxPvCbu/MXAp3cb1tpwN+BtiJyEs433vkAItJCRP4pIltFZD/wesA+im0JeH4KsFXd/zzXd1X8DuUtf4r72KuqB8rMa13Jtr4PeH4YKLftXFUPAT/H+SDaLiL/FZEuFWzzFEr/DsXxVcdJqprsPqZVsWxrYG81t1/sVX78lv0rSp8NQOm/1XdAPZwzgFOBK4rfK+77pR/V/z2r0qpMDIHPi5vrvg2IIfA9DbAj4HkeUKSlO+jzcP7mJ+P8boHbeg9oEbpfpW6zRBCFVHUeMB3ndBhgN84/RfeAD6Am6nYqquphYAlwG7BCVY/ifHP/P2CDqu52t/MIzreuVFVtjPPhImV3H/B8O9BaRAKXaVdF+OUtv819NBWRRmXmba1ie+U5pqSuqn6kqiNwPpzWAC9WsO42nA/KsvGFyyXAFzVc913gTBHpjtNU+I8y89sGPG8H5OMknS04ZwTJAY8Gqvp4DeOoyPdAm/LiEZGOwLPAjTjf8pNx/i5l32/B2AEcBTqXef83qXnoscUSQfR6ChghImmq6sf5YHtSRFoAiEhrERkVsPw84GZ+7A/4vMxrgEY4zU+5ItIamFxFDAuBQuBWt+N4DM4ZRmVauMsniMjlOH0R76vqFpzk9Ijb8ZcKXAfMqGJ75dkBtBcRH4CItBSRi9y+gnz3dyyqYN03gPtEpLl7xnQ/zplRyIhInIh0EJE/4ZzdPVST7bgJfhZOzF+qatmkeZWIdHF/74eAme7Z2GvAJSIywo0lUUSGiUiozwhmAveISLLbrzUxYF5DnIS9CxARGY9zRlBtqloEvAQ85f7dRETaiMjI44w/ZlgiiFKqugunief/uZPuwuk8/tpt1vkUpyO12DycD/r5FbwG58OiN07b639xvnFWFsNRYAxO+/cPOM0vla4D/A+nM283TgfhZQGn+1fgtLNvw/mAe0BVP6lie+V5y/25R0S+wXmf3+Fudy9OR/lNFaz7OyADWAYsB75xp4VCf3HuAdiPk4gbA31UdflxbPNVoAfHNgvhTnsd58wtDrgdQFWzcc5E/h/OB/FmnOMT6s+DB3CScjbwMU5iyHdjWIbTgb/Ija8Lznujpu7Aaf5ahPP+/RjnfWaCIKWba40JHxEZB4xX1UFex1JXuE0sy4CTVfVgwPQFwEuqOt2r2MoSkVtw+rkquxLMeMDOCIyJUm7T1/8B/whMArWF2zw5QER8ItIVmIRzpmdqmVi8O9SYqCciTXA60rNxLnGtjerh9F21x2k6fAN43suATPmsacgYY2KcNQ0ZY0yMi4qmoZNOOknbt2/vdRjGGBNVlixZsltVm1e1XFQkgvbt25ORkeF1GMYYE1VEpKo7/QFrGjLGmJhnicAYY2KcJQJjjIlxUdFHUJ6CggJycnI4cuSI16EYU6XExETatGlDQkKC16EYc4yoTQQ5OTk0atSI9u3bU7qYpTG1i6qyZ88ecnJy6NChg9fhGHOMqE0ER44csSRgwmb20q08/tFatuXmcUpyEpNHdWZ0r8qGRqiYiNCsWTN27doV4iiNCY2oTQSAJQETFrOXbuXud5eTV+BUqt6am8fd7zoFQo8nGRhTW1lnsTFlPP7R2pIkUCyvoIjHP1pbwRrGRLeoPiMI9OWXJ1NQsKPqBYOUkNCSgQO/r3SZnJwcJk6cyKpVq/D7/VxwwQU8/vjjnHDCCUyfPp2MjAz+/Oc/l1onNzeXf/zjH9x0U0Xl8H80YMAAvvrqqxrFP3ToUKZNm0Z6enqp6RXFVVtlZ2dzwQUXsGLFiojtc1tuXrWmGxPt6swZQSiTQDDbU1XGjBnD6NGjycrKYt26dRw8eJB777230vVyc3P561//GlQMNU0C4VJYWBj2fRQVVTRwWOSckpxUrenGRLs6kwgibe7cuSQmJnLNNdcAEBcXx5NPPsnLL7/M4cOHAdiyZQvnnHMOnTt35qGHnNEIp0yZwoYNG0hLS2Py5MkcPHiQ4cOH07t3b3r06MG//vWvkn00bOiMxf75558zdOhQLrvsMrp06cLYsWMprhr78MMP06dPH1JSUrjhhhsIrCb7+uuvM2DAAFJSUli0aNExv8OuXbu49NJL6dOnD3369OHLL788Zpnp06dz+eWXc+GFFzJypDPy3+OPP06fPn1ITU3lgQceAOCxxx7jmWeeAWDSpEn85Cc/AWDOnDn86lfO+Oo33ngj6enpdO/evWQ9cEqIPPzwwwwaNIi33nqLJUuW0LNnT/r3789f/vKXkuVWrlxJ3759SUtLIzU1laysrOD/YNUweVRnkhLiSk1LSohj8qjOFaxhTHSrM01DkbZy5UrOPPPMUtMaN25Mu3btWL9+PQCLFi1ixYoV1K9fnz59+nD++efzhz/8gRUrVpCZmQk437JnzZpF48aN2b17N/369eOiiy46pnNx6dKlrFy5klNOOYWBAwfy5ZdfMmjQIG6++Wbuv/9+AK688kree+89LrzwQgAOHTrEV199xfz587n22muPaV657bbbmDRpEoMGDWLz5s2MGjWK1atXH/O7Lly4kGXLltG0aVM+/vhjsrKyWLRoEarKRRddxPz58xkyZAh//OMfufXWW8nIyCA/P5+CggIWLFjA4MGDAZg6dSpNmzalqKiI4cOHs2zZMlJTUwHnOvsFCxYAkJqayp/+9CfOPvtsJk/+cdjk5557jttuu42xY8dy9OjRsJ09FHcIh+qqIWNqO0sENaSq5V4JEjh9xIgRNGvWDIAxY8awYMECRo8efczy99xzD/Pnz8fn87F161Z27NjBySefXGq5vn370qZNGwDS0tLIzs5m0KBBfPbZZzz22GMcPnyYvXv30r1795JEcMUVVwAwZMgQ9u/fT25ubqltfvrpp6xatark9f79+zlw4ACNGjUqtdyIESNo2rQpAB9//DEff/wxvXr1AuDgwYNkZWVx1VVXsWTJEg4cOEC9evXo3bs3GRkZfPHFFyVnCjNnzuSFF16gsLCQ7du3s2rVqpJE8POf/xyAffv2kZuby9lnnw04ye2DDz4AoH///kydOpWcnBzGjBnDGWeEb0ja0b1a2we/iRmWCGqoe/fuvPPOO6Wm7d+/ny1btnDaaaexZMmSYxJFeYljxowZ7Nq1iyVLlpCQkED79u3LvVu6Xr16Jc/j4uIoLCzkyJEj3HTTTWRkZNC2bVsefPDBUutWtX+/38/ChQtJSqq87btBgwYlz1WVu+++m1//+tfHLNe+fXteeeUVBgwYQGpqKp999hkbNmyga9eubNq0iWnTprF48WJOPPFExo0bVyrW4n1UlGABfvnLX3LWWWfx3//+l1GjRvHSSy+VNEEZY2rO+ghqaPjw4Rw+fJi///3vgNPJeccddzBu3Djq168PwCeffMLevXvJy8tj9uzZDBw4kEaNGnHgwIGS7ezbt48WLVqQkJDAZ599xnffBVU1FqDkg/Skk07i4MGDvP3226Xmv/nmmwAsWLCAJk2a0KRJk1LzR44cWerqoeLmqsqMGjWKl19+mYMHnSFyt27dys6dOwHnzGPatGkMGTKEwYMH89xzz5GWloaIsH//fho0aECTJk3YsWNHybf8spKTk2nSpElJM9GMGTNK5m3cuJGOHTty6623ctFFF7Fs2bIq4zXGVK3OJIKEhJYR3Z6IMGvWLN566y3OOOMMOnXqRGJiIr///e9Llhk0aBBXXnklaWlpXHrppaSnp9OsWTMGDhxISkoKkydPZuzYsWRkZJCens6MGTPo0qVL0DEmJydz/fXX06NHD0aPHk2fPn1KzT/xxBMZMGAAEyZM4G9/+9sx6z/zzDNkZGSQmppKt27deO6556rc58iRI/nlL39J//796dGjB5dddllJYhs8eDDbt2+nf//+tGzZksTExJL+gZ49e9KrVy+6d+/Otddey8CBAyvcxyuvvMLEiRPp379/qbOVN998k5SUFNLS0lizZg1XXXVVUMfJGFO5qBizOD09XcsOTLN69Wq6du3qUUTGVJ+9Z02kicgSVU2vark6c0ZgjDGmZiwRGGNMjLNEYIwxMc4SgTHGxDhLBMYYE+MsERhjTIyzRHAc4uLiSEtLIyUlhcsvv7yk2Fxdkp2dTUpKSki2dd555x1T5qKs6dOns23btmpt98MPP6Rv37506dKFtLQ0fv7zn7N58+bjCbVa3nrrLbp3747P56PsZc7GRANLBMchKSmJzMxMVqxYwQknnBDUDVnFalLSOdrLQL///vskJydXukx1E8GKFSu45ZZbePXVV1mzZg2ZmZmMHTuW7Ozs44w2eCkpKbz77rsMGTIkYvs0JpRiJhHMXrqVgX+YS4cp/2XgH+Yye+nWkG5/8ODBrF+//phv0NOmTePBBx8EnMFi7rnnHs4++2yefvrpqCsDHejzzz9nyJAhXHLJJXTr1o0JEybg9/sBeOONN+jRowcpKSncddddpfaze/dusrOz6dq1K9dffz3du3dn5MiR5OXl8fbbb5ORkcHYsWNJS0sjLy+PKVOm0K1bN1JTU7nzzjuPiePRRx/lnnvuKXWj1kUXXVTyofziiy/Sp08fevbsyaWXXlpy1jZu3DhuvPFGhg0bRseOHZk3bx7XXnstXbt2Zdy4cSXb+vjjj+nfvz+9e/fm8ssvLymtEahr16507mwlqk30iolEUDwG7dbcPJQfx6ANVTIoLCzkgw8+oEePHlUum5uby7x587jjjjtKykAvXryYd955h/Hjx5e7zsKFC3n11VeZO3duqTLQmZmZLFmypKQM9BdffAFARkYGBw8eLLcMdEZGBsuWLWPevHmlavUUl4H+xS9+wTXXXMMzzzzDwoULK/1dFi1axB//+EeWL1/Ohg0bePfdd9m2bRt33XUXc+fOJTMzk8WLFzN79uxj1s3KymLixImsXLmS5ORk3nnnHS677LKSUhuZmZnk5eUxa9YsVq5cybJly7jvvvuO2c7KlSvp3bt3hTGOGTOGxYsX8+2339K1a9dSpTZ++OEH5s6dy5NPPsmFF17IpEmTWLlyJcuXLyczM5Pdu3fzu9/9jk8//ZRvvvmG9PR0nnjiiUqPiTHRKCYSQbjGoM3LyyMtLY309HTatWvHddddV+U6xeWWwSkDffPNN5OWlsZFF11UUga6rIrKQPfu3Zs1a9aQlZXFmWeeWaoMdP/+/UvKQBcngpkzZ9K7d2969erFypUrS5WgrqwMdEX69u1Lx44diYuL44orrmDBggUsXryYoUOH0rx5c+Lj4xk7dizz588/Zt0OHTqQlpYGwJlnnlluU07jxo1JTExk/PjxvPvuuyXF/CqyZ88e0tLS6NSpE9OmTQOcpqPBgwfTo0cPZsyYwcqVK0uWv/DCCxERevToQcuWLenRowc+n4/u3buTnZ3N119/zapVqxg4cCBpaWm8+uqr1SoKaEy0CFsZahFpC/wdOBnwAy+o6tMi8iBwPbDLXfQeVX0/XHFA+MagLe4jCBQfH1/SRAIcU1I6sKRztJWBLqu8MtfB1q4qW1Y7L+/Yv0V8fDyLFi1izpw5/POf/+TPf/4zc+fOLbVM9+7d+eabb+jZsyfNmjUjMzOTadOmlTThjBs3jtmzZ9OzZ0+mT5/O559/fkwMPp+vVDw+n4/CwkLi4uIYMWIEb7zxRlC/kzHRKpxnBIXAHaraFegHTBSRbu68J1U1zX2ENQlAZMegbdmyJTt37mTPnj3k5+fz3nvvVbhstJWBLmvRokVs2rQJv9/Pm2++yaBBgzjrrLOYN28eu3fvpqioiDfeeKPk7CIYgWW6Dx48yL59+zjvvPN46qmnyj0+v/nNb5g6dWqpkdUCr946cOAArVq1oqCgoNLfpTz9+vXjyy+/LBlx7vDhw6xbt65a2zAmGoQtEajqdlX9xn1+AFgNeDLkUyTHoE1ISOD+++/nrLPO4oILLqi0rHS0lYEuq3///kyZMoWUlBQ6dOjAJZdcQqtWrXjkkUcYNmwYPXv2pHfv3lx88cVV/l7Fxo0bx4QJE0hLS+PAgQNccMEFpKamcvbZZ/Pkk08es3yPHj14+umnueqqq+jSpQsDBw5k9erV/PKXvwTgt7/9LWeddRYjRoyoVolvgObNmzN9+nSuuOIKUlNT6devH2vWrDlmuVmzZtGmTRsWLlzI+eefz6hRo6q1H2O8FpEy1CLSHpgPpAD/B4wD9gMZOGcNP5Szzg3ADQDt2rU7s2zbbHVL+s5eutXGoA2hzz//nGnTplV6xmNKszLUJtKCLUMd9qEqRaQh8A5wu6ruF5Fngd8C6v78I3Bt2fVU9QXgBXDGIzjeOGwMWmOMKV9YE4GIJOAkgRmq+i6Aqu4ImP8iYF8po9DQoUMZOnSo12EYY0IgbH0E4lxS8jdgtao+ETC9VcBilwArwhWDMcaYqoXzjGAgcCWwXESKL/e4B7hCRNJwmoaygWOvgzTGGBMxYUsEqroAKO+C9LBfLmqMiTy7ICN6hb2z2BhT9xWXcSm+g7+4jAtgySAKxESJiXCxMtTVU1fLUE+ePJkuXbqQmprKJZdcUuXvWBeFq4yLiQxLBMfBylBXT10tQz1ixAhWrFjBsmXL6NSpE4888kjE9l1bhKuMi4mM2EkEy2bCkynwYLLzc9nMkG7eylDHbhnqkSNHEh/vtLL269ePnJycco9ZXRbJMi4m9GIjESybCf+5FfZtAdT5+Z9bQ5YMrAy1laEu9vLLL3PuuedWukxdFMkyLib0YqOzeM7DUFDmFLUgz5me+rMab7a4DDU4ZwTXXXddlc0aZctQB5aCLi5D3ahRo1LrVFSGGpzCbFlZWVx11VWlylD37t27pAx18ZnCzJkzeeGFFygsLGT79u2sWrWK1NTUUnGVV4a6ogJ1xWWogZIy1AkJCSVlqIGSMtSjR48utW51y1Cff/75XHDBBZUe2z179jB8+HAOHz7MDTfcwJ133smKFSu47777yM3N5eDBg6XqAJVXhhooKUOdk5NTUoYa4OjRo/Tv37/C/U+dOrWk9HasKe4QtquGolNsJIJ9FZyqVzQ9SFaG2spQF3v11Vd57733mDNnTtDHr66xMi7RKzaahpq0qd7042BlqGOvDPWHH37Io48+yr///e8qB88xpjaKjTOC4fc7fQKBzUMJSc70EAssQ92hQ4cqy1BPnDiR1NRUCgsLGTJkSJVXHo0cOZLVq1eXNFE0bNiQ119/nRYtWjB48GCmTp1K//79adCgQYVlqDt27FhlGeprr72W+vXrV1pSubgM9fLly0s6jn0+X0kZalXlvPPOq1EZ6qSkJD744AMuvvhijhw5gqpWWYb6wIEDNGvWjHbt2vHQQw8BP5ahPvXUU+nRo0e5I8BVJLAMdX5+PgC/+93v6NSpU6nlbr75ZvLz8xkxYgTgJJDqXEFmjNciUob6eKWnp2tGRkapadUu6btsptMnsC/HORMYfv9x9Q/EOitDXX1WhtpEWq0pQ11rpP7MPviNMaYcsZMITEhZGWpj6o7Y6Cw2xhhTIUsExhgT4ywRGGNMjLNEYIwxMc4SQQ3t2bOHtLQ00tLSOPnkk2ndujVpaWkkJyfTrVu3sOwzMzOT998P3bg+AwYMqHKZp556qk6W1zbG/MgSQQ0VlzPIzMxkwoQJTJo0qeS1z1f1Ya1JSelQJ4KvvvqqymUsERhT98VOIpgxA9q3B5/P+VnNcgPVUVRUdEyJZQi+DPWiRYsYMGAAvXr1YsCAAaxdu5ajR49y//338+abb5KWlsabb75Zap/Tp0/n4osv5pxzzqFz584ld9YCPPHEE6SkpJCSksJTTz1VMr1hw4aAc3PY0KFDueyyy+jSpQtjx45FVXnmmWfYtm0bw4YNY9iwYWE7XsYYj6lqrX+ceeaZWtaqVauOmVah119XrV9fFX581K/vTA+BBx54QB9//HFVVd20aZPGxcXp0qVLVVX18ssv19dee01VVc8++2y98cYbS9a74oor9IsvvlBV1e+++067dOmiqqr79u3TgoICVVX95JNPdMyYMaqq+sorr+jEiRPLjeGVV17Rk08+WXfv3q2HDx/W7t276+LFizUjI0NTUlL04MGDeuDAAe3WrZt+8803qqraoEEDVVX97LPPtHHjxrplyxYtKirSfv36lcR16qmn6q5du0JynGJdtd6zxoQAkKFBfMbGxg1l994LZZs3Dh92poehZHBlJZaDKUO9b98+rr76arKyshARCgoKgtrviBEjaNasGeDU4V+wYAEiwiWXXFJSXXTMmDF88cUXJWWsi/Xt25c2bZwifGlpaWRnZzNo0KDq//LGmKgTG4mgovFrwzSubWUlloMpQ33LLbcwbNgwZs2aRXZ2dtB38IayLHQkhsU0xtQOsdFH0K5d9aZHSEVlqPft20fr1k5d9+nTp5fMDyzRXJ5PPvmEvXv3kpeXx+zZsxk4cCBDhgxh9uzZHD58mEOHDjFr1qySiqTBqGqfxpjoFxuJYOpUKFsnvn59Z7qHnnnmGTIyMkhNTaVbt24lpYt/85vfcPfddzNw4MBSg8kPGzaMVatWldtZDDBo0CCuvPJK0tLSuPTSS0lPT6d3796MGzeOvn37ctZZZzF+/PhjmoUqc8MNN3DuuedaZ7ExdVjslKGeMcPpE9i82TkTmDo1LP0DXpk+fToZGRmlzjBM7WJlqE2kWRnqssaOrVMf/MYYEyqxkwjquHHjxjFu3DivwzDGRKGo7iOIhmYtY8Deq6Z2i9pEkJiYyJ49e+wfzNR6qsqePXtITEz0OhRjyhW2piERaQv8HTgZ8AMvqOrTItIUeBNoD2QDP1PVH6q7/TZt2pCTk8OuXbtCF7QxYZKYmFhyw54xtU04+wgKgTtU9RsRaQQsEZFPgHHAHFX9g4hMAaYAd1V34wkJCXTo0CGkARtjTCwKW9OQqm5X1W/c5weA1UBr4GLgVXexV4HR4YrBGGNM1SpNBCIySkSuE5H2ZaZfW52duOv3Av4HtFTV7eAkC6BFBevcICIZIpJhzT/GGBM+FSYCEfk9cC/QA5gjIrcEzL452B2ISEPgHeB2Vd0f7Hqq+oKqpqtqevPmzYNdzRhjTDVVdkZwIfATVb0dOBM4V0SedOdJxav9SEQScJLADFV91528Q0RaufNbATtrFLkxxpiQqCwRxKtqIYCq5uIkhsYi8hZwQlUbFqcU5t+A1ar6RMCsfwNXu8+vBv5Vk8CNMcaERmWJYIOInF38QlWLVPU6YC0QTMGUgcCVwE9EJNN9nAf8ARghIlnACPe1McYYj1R2+ejl5U1U1ftE5NmqNqyqC6i4CWl4ELEZY4yJgAoTgarmVTJva3jCMcYYE2lRW2LCGGNMaFgiMMaYGFfVDWU+EVnmmYnWAAAb80lEQVQRqWCMMcZEXqWJQFX9wLci4u3gvsYYY8ImmKJzrYCVIrIIOFQ8UVUvCltUxhhjIiaYRPBQ2KMwxhjjmSoTgarOE5FTgTNU9VMRqQ/EhT80Y4wxkVDlVUMicj3wNvC8O6k1MDucQRljjImcYC4fnYhTLmI/gKpmUUHpaGOMMdEnmESQr6pHi1+ISDxgAwUbY0wdEUwimCci9wBJIjICeAv4T3jDMsYYEynBJIIpwC5gOfBr4H3gvnAGZYwxJnKCuXz0POBvqvpiuIMxxhgTecGcEfwCyBKRx0QkmHEIjDHGRJEqE4Gq/gpn4PkNwCsistAdWL5R2KMzxhgTdkFVH3UHnX8H+CdOyYlLgG/KDGhvjDEmCgVzQ9mFIjILmAskAH1V9VygJ3BnmOMzxhgTZsF0Fl8OPKmq8wMnquphEbk2PGEZY4yJlGBqDV1Vybw5oQ3HGGNMpAXTNNRPRBaLyEEROSoiRSKyPxLBGWOMCb9gOov/DFwBZAFJwHjgT+EMyhhjTOQE00eAqq4XkThVLcK5hPSrMMdlTFSbvXQrj3+0lm25eZySnMTkUZ0Z3au112EZU65gEsFhETkByBSRx4DtQIPwhmVM9Jq9dCt3v7ucvIIiALbm5nH3u8sBLBmYWimYpqErcQaiuRlnqMq2wKXhDMqYaPb4R2tLkkCxvIIiHv9orUcRGVO5YK4a+s59mocNW2lMlbbl5lVrujFeqzARiMhyKhl3QFVTwxKRMVHulOQktpbzoX9KcpIH0RhTtcrOCC6IWBTG1CGTR3Uu1UcAkJQQx+RRnT2MypiKVZgIVPU7ERkNnA4sV9WPIheWMdGruEPYrhoy0aKypqG/At2Br4DfikhfVf1tsBsWkZdxzip2qmqKO+1B4HqcgW4A7lHV92sYuzG11uhere2D30SNyq4aGgL8RFXvBoYCo6u57enAOeVMf1JV09yHJQEP3X777dx+++0R3Wd+/jbWr5/EoUNrIrpfY0zFKusjOOreQFZcYE6qs2FVnS8i7Y8jNhNmmZmZEd9nVtYt7N49m23bnuekk0bTseOjJCa2jXgcxpgfVXZG0EVElrmP5QGvl4vIsuPY583udl4WkRMrWsgd/CZDRDJ27dpV0WImiuzfv4i9ez8A/Pj9eezc+TaLFnUmK2sSBQW5XodnTMyqLBF0BS50HxcEvL7A/VkTzwKnAWk4dyj/saIFVfUFVU1X1fTmzZvXcHemtlBV1q69Hr8/8LLKAvz+PLZte46vv25HTs7T+P0FnsVoTKyq9KqhUO9MVXcUPxeRF4H3Qr0PUzvt2jWTvLwN5c5TPUJR0RE2bryXLVum0anT8zRrdl6EIzQmdgU1VGWoiEirgJeXACsiuX/jjaKiI2Rl3YLff6jS5fz+Q+Tn57By5c9YunQohw+vi1CExsS2sCUCEXkDWAh0FpEcEbkOeCygj2EYMClc+ze1x5Ytj1NUVHkSCOT3H2Lfvi/IyEgjK2sShYUHwxidMSaoMtQ1oapXlDP5b+Han6md8vO/Z/PmP+D3H67mmk6H8vbtz7Njx9/p1Ok5mje/jGpevGZigJX8Pn5Wa8iE1YYNd6Ba8w5gvz8Pvz+PNWuuISfnabp0mU79+qeHMEITzazkd2hU1jRUfHXQh+5jrPt4H3g7/KGZaHfgQCa7d886rkRQzO8/xP79X5ORkcqmTQ/h9x8NQYQm2lnJ79CoMBGo6nfulUMDVfU3qrrcfUwBRkUuRBONVJV1667H7z8Swq0W4ffnsWXLYyxa1Jl9+2ygvFhnJb9DI5jO4gYiMqj4hYgMwEYoM1XYvftfHDq0mkpaF2vM7z/MkSPZfPvtT9m8eVrIt2+iR0Wlva3kd/UEkwiuBf4iItkisgn4qzvNmHL5/flkZd1U5eWix7+fPLKzH7BkEMMmj+pMUkJcqWlW8rv6Kr1qSER8wOmq2lNEGgOiqvsiE5qJVjk5T1NYGJm3id9/mOzsBxAR2ra9IyL7NLWHlfwOjUoTgar6ReRmYKaq7o9QTCaKHT26i+zsh2twuWjN+f2H2bTpfkBo2/b/IrZfUztYye/jF0zT0CcicqeItBWRpsWPsEdmotLGjVNQLYz4fp1k8P/YsuWpiO/bmGgXzA1lxf0BEwOmKdAx9OGYaHbo0Ep27nwD1XxP9u8kg3txzgxu8yQGY6JRlYlAVTtEIhAT/daunYDf700SKOYkg3sQEdq0udXTWIyJFkGVmBCRFKAbkFg8TVX/Hq6gTPTZs+cDDh5cCvi9DgW//zAbN94NCG3a3OJ1OMbUelUmAhF5AGeoym44dxWfCywALBEYAPz+AtatmxD2y0Wrw0kGU3CSwc1eh2NMrRZMZ/FlwHDge1W9BugJ1AtrVCaqbNv2LAUFe7wO4xhOMvgNOTl/8ToUY2q1YBJBnqr6gUL3XoKdWEexcRUU/MCmTffVqrOBQH5/npsM/up1KMbUWsH0EWSISDLwIrAEOAgsCmtUJmps2nRfSIrKhZNzZjAZEaF16xu9DseYWieYq4Zucp8+JyIfAo1V9XgGrzd1xOHD6/j++5dDXFguPPz+w2zY4Nx5bMnAmNKC6Sz+O/AF8IWqrgl/SCZarFt3U1QNNu/357Fhw52A0Lr1BK/DMabWCKaPYDrQCviTiGwQkXdExO7WiXE//DCX/fsXAkVVLlubFJ8ZbNv2vNehGFNrBNM0NFdE5gF9cMYZngB0B54Oc2ymllItYu3a6yNaTyiU/P7DrF8/CRBOOeUGr8MxxnPBNA3NwRl/YCFOE1EfVd0Z7sBM7bV9+0scPbrD6zCOi9+fx/r1t+Mkg+u9DscYTwXTNLQMOAqkAKlAiojYqA8xqrBwPxs23FVrLxetDicZ3Ma2bS95HYoxngqmaWgSgIg0BK4BXgFOxm4qi0nZ2Q95VlQuHJxkcCsiQqtW13kdjjGeCKZp6GZgMHAm8B3wMk4TkYkxeXmb2Lbt2ai4XLQ6/P48srJuAYRWrWzwPRN7grmhLAl4AliiXhSaN7VGVtYttf7msZpyksHNOMngGq/DMSaiquwjUNXHgQTgSgARaS4iVpo6xuTmLiA39zNPBp2JFCcZTGT79le8DsWYiKoyEbjVR+8C7nYnJQCvhzMoU7uo+lm7dnzUXi5aHT8mg+leh2JMxARz1dAlwEXAIQBV3QY0CmdQpnb5/vvXyM/P8TqMiHGSwU3s3Pmm16EYExHBJIKjqqo4w1MiIg3CG5KpTYqKDrFhw6Q6cblodfj9eaxdOz7q75cwJhjBJIKZIvI8kCwi1wOfAnbhdYz47rupde4qoWD5/UdZu/bXXodhTNgF01k8DXgbeAfoDNyvqs9UtZ6IvCwiO0VkRcC0piLyiYhkuT9PPJ7gTXgdOZJDTs5T+P15XofiCdWj/PDDJ+zd+7HXoRgTVsGcEaCqn6jqZFW9E5grImODWG06cE6ZaVOAOap6BjDHfW1qqfXrb8Pvr7tXCQXD7z/MmjVXU1QUm8nQxIYKE4GINBaRu0XkzyIyUhw3AxuBn1W1YVWdD+wtM/li4FX3+avA6BrGbcJs//5F7N37AVA37xuojsLC/WRnP+x1GMaETWVnBK/hNAUtB8YDHwOXAxer6sU13F9LVd0O4P5sUdGCInKDiGSISMauXbtquDtTE6rqVhe1b8HgnBVs3fo0hw9neR2KMWFR2Z3FHVW1B4CIvATsBtqp6oFIBKaqLwAvAKSnp2sk9mkcu3bNJC9vg9dh1CpOx/F40tI+R0S8DseYkKrsjKCkTUBVi4BNIUgCO0SkFYD708pZ1zJFRU7dnVi7XLRqRRw4sIQ9e/7jdSDGhFxliaCniOx3HweA1OLnIrK/hvv7N3C1+/xq4F813I4Jky1bHqeoyJJAefz+Q6xdewNFRbF5Oa2puypMBKoap6qN3UcjVY0PeN64qg2LyBs4g9l0FpEcEbkO+AMwQkSygBHua1NL5Od/z+bNj8ZEKYmaKio6wJYtj3odhjEhFUz10RpR1SsqmDU8XPs0x2fDhjvqbHXRUPH7D7N586O0anU99eqd4nU4xoREUPcRmLrvwIGl7N49yxJBEFQLWb/+/7wOw5iQsURgUFXWrbshZktJVJdqAXv2/JsDB5Z6HYoxIWGJwLB79784dGg1bl1BEwS//wjr1k3AqcdoTHSzRBDz/GRl3WSXi1abcujQSvbu/dDrQIw5bpYIYlx+/lYKC/d5HUZU8vsPkZV1M6p+r0Mx5rhYIohhqgUcOZJtl4seh6NHd7Bz5xteh2HMcbFEEMPy8jZaG/dx8vsPsW7djTFfpdVEN0sEMerQoZUUFOzEOoiPj8/XgHr12iJi/0omeoXthjJTu61dO8Hato+Dz1efuLiGnHHGn2je/HIrRGeimiWCGLRnzwccPGjXwNeEyAmIxNOu3RTatp1MXFyi1yEZc9wsEcQYv7+Adet+bZeLVpvg8yVy0kljOO20adSrd7LXARkTMpYIYsy2bc9SUFB24DhTGZ+vAQ0adKdTp+dp1CjN63CMCTlLBDGkoOAHNm26z84GguTzNSA+vjGdOj1Ls2YXWT+AqbMsEcSQTZvus6JyQRCph0g87ds/SJs2t+LzneB1SMaElSWCumzZTJjzMOzLwd+4JUVtcvC3sMJyFfPh89WjRYtf0rHjHzjhhJO8DsiYiLBEUFctmwn/uRUKnAHoffu/p9OaOFTj2dnSbn4qy+drQKNGvenU6TkaNOjmdTjGRJQlgrpqzsMlSaBYnF/ouDGRnS0PehRU7ePzNSAhoRmdOj1Ps2bneB2OMZ6wRFBX7cspd3K9fOvwBBBJxOdLoEOHqZxyyo34fPavYGKXvfvrqiZtYN+WYybn14v1khJx+HwncPLJ19Khw29JSDjR64CM8ZwVSKmrht+PJiSVmlTkUzZ2jN3OYp+vPieeOIz09G/p1OnPlgSMcdkZQV2V+jN27Hid5IzPqZfvnAls7HgkJjuKfb6GnHBCSzp3foETT/yJ1+EYU+tYIqij8vI2sa7ep/j751W9cB3l8yUhcgKnnfYYrVpdh0ic1yEZUytZIqijsrJuCenNYy12xNNxYyL18qXWn12IxCOSwCmn3ET79g8QH9/I65CMqdUsEdRBublfkJv7Gaqh+aBusSOezmuTiPM7Vxwl5gud1yYBebUuGfh89UlO/glnnPEnkpLaex2OMVHBEkEdo+pn7drrQzr8ZMeNiSVJoFhtuyfB52tAYmI7OnV6geTkQV6HY0xUsURQx3z//Wvk55d/D0FNVXTvQW24J8Hnq4/Pl8jppz9Jy5a/spHCjKkBSwR1SFHRITZsmBTy6qL59ZTEcj70vb0nIQGfL542bW7n1FPvIS6ugYexGBPdLBHUId99NxW/P/T3CWzseKRUHwF4e0+Cz1efpk3P5fTTnyIxsY0nMRhTl1giqCOOHMkhJ+cp/P7QXy7qdAjneX7VkM/XgKSk0+jc+UUaN+4b0X0bU5d5kghEJBs4ABQBhaqa7kUcdcn69bfh94dvrIGdLQs96xi2geKNCS8vzwiGqepuD/dfZ+zfv4i9ez8AatelnMfLBoo3JjKsaSjKqap7uWhduoPYBoo3JpK8SgQKfCwiCjyvqi94FEfU27VrJnl5G7wOI2ScgeK70anTCzZQvDER4lUiGKiq20SkBfCJiKxR1fmBC4jIDcANAO3atfMixlqvqCiPrKxb6sRg9DZQvDHe8eTuG1Xd5v7cCcwCjrkERFVfUNV0VU1v3rx5pEOMClu2PE5RUc2SQItPofEqSP4W+v3Cee0FkXr4fA1o3/4B+vXL5qSTLrYkYEyERfyMQEQaAD5VPeA+Hwk8HOk4ol1+/vds3vxojUpJtPgUOk8Dn3uRUeIO5zXAzp+GMMhKFQ8UfwUdOz5qA8Ub4yEvmoZaArPcb33xwD9U9UMP4ohqGzbcUePqoh1fgrh8WA8cBIYC5IP/Mdj/XuhirIiIj7i4RiQldSIubgNwWfh3GmZpaWk89dRTXodhTI1EPBGo6kagZ6T3W5f4/fns3DmT6lwuGlhGml8ozDlC8xWl1/eF7zYEwEkAIgkkJXUiIaFpeHdmjAmaXT4ahQoL9+PzxeP3B5cIypaRJlngwiSWkgcByeBIS/g6DF9qfb5ERIoHip+Az5cQ+p0YY2rMEkEUKizch0jwf7ryykhzgsDwRFjh3C1cVA82jg9llPDjQPHX0KHD72yMYGNqKUsEUaioaD/VueCronLR2kRAIL+FkwRC2VHs89WnSZMBnHHGX6lf/4zQbdgYE3KWCKJQYeF+IPhLLCssI52ofD33+OMpPYwlbOnciJOGvWYDxRsTJWwUjyjknBEEPxbAxo5HKPKVXj5UZaSL+x8S830IQmK+cPrqIk7cYmWkjIkWlgiiUGHhflT9QS+/s2UhazvncaSeH0U5Us/P2s6hGW+4vP4HKciDOXZriDHRwpqGolBR0T6qW2k0HGWkfb761Muv4LvEvtAOl2mMCR87I4hChYX7wzr2QFV8vgbUr9+F1NSPkCZty1+oiY0cZky0sEQQhQoLf8AZ0yeyfL4k4uOb0qnTX+nTZyXJyYNg+P2QkFR6wYQkZ7oxJipY01AUKiiIdEdsAj5fQvkDxaf+zPk552GnOahJGycJFE83xtR6lgiiUEHB3ojty+dLomnT8yofKD71Z/bBb0wUs0QQhZymofCygeKNiR2WCKJQYeG+sG3bGSi+AWec8WcbKN6YGGGJIAoVFR0I+TZFEhBJsIHijYlBlgiiUFFRKO8HsIHijYl1lgiiUE1GJSuPDRRvjAFLBFHL50tCJIEfi8/5US1CtQDVQny+eiXt/XFxDYmLa0J8fDIJCScSH9+MhIRmNG7ch6ZNz7N+AGNinCWCKJSWNp+DB5cSF9eY+PgmxMc3dp87P+PiGiBi9woaY4JjiSAKNWrUi0aNenkdhjGmjrCvjcYYE+MsERhjTIyzRGCMMTHOEoExxsQ4SwTGGBPjLBEYY0yMs0RgjDExzhKBMcbEOFFVr2OokojsAr6r4eonAZEe0isYFlf1WFzVY3FVT12N61RVbV7VQlGRCI6HiGSoarrXcZRlcVWPxVU9Flf1xHpc1jRkjDExzhKBMcbEuFhIBC94HUAFLK7qsbiqx+KqnpiOq873ERhjjKlcLJwRGGOMqYQlAmOMiXExkQhE5EER2Soime7jPA9jOUdE1orIehGZ4lUc5RGRbBFZ7h6jDA/jeFlEdorIioBpTUXkExHJcn+eWEvi8vy9JSJtReQzEVktIitF5DZ3uqfHrJK4PD1mIpIoIotE5Fs3rofc6R1E5H/u8XpTRE6oJXFNF5FNAccr9AOMq2qdfwAPAnfWgjjigA1AR+AE4Fugm9dxBcSXDZxUC+IYAvQGVgRMewyY4j6fAjxaS+Ly/L0FtAJ6u88bAeuAbl4fs0ri8vSY4Qz03dB9ngD8D+gHzAR+4U5/DrixlsQ1HbgsnPuOiTOCWqQvsF5VN6rqUeCfwMUex1TrqOp8YG+ZyRcDr7rPXwVGRzQoKozLc6q6XVW/cZ8fAFYDrfH4mFUSl6fUcdB9meA+FPgJ8LY73YvjVVFcYRdLieBmEVnmnt5HvFnB1RrYEvA6h1rwjxFAgY9FZImI3OB1MGW0VNXt4HzAAC08jidQbXhvASAi7YFeON8ma80xKxMXeHzMRCRORDKBncAnOGfquapa6C7iyf9m2bhUtfh4TXWP15MiUi/U+60ziUBEPhWRFeU8LgaeBU4D0oDtwB+9CrOcabXp+t2BqtobOBeYKCJDvA4oCtSW9xYi0hB4B7hdVfd7FUdZ5cTl+TFT1SJVTQPa4Jypdy1vschGdWxcIpIC3A10AfoATYG7Qr3f+FBv0Cuq+tNglhORF4H3whxORXKAtgGv2wDbPIrlGKq6zf25U0Rm4fyDzPc2qhI7RKSVqm4XkVY435g8p6o7ip97+d4SkQScD9sZqvquO9nzY1ZeXLXlmLmx5IrI5zht8ckiEu+eFXj6vxkQ1zmqOs2dnC8irwB3hnp/deaMoDLuP0GxS4AVFS0bZouBM9yrE04AfgH826NYShGRBiLSqPg5MBLvjlN5/g1c7T6/GviXh7GUqA3vLRER4G/AalV9ImCWp8esori8PmYi0lxEkt3nScBPcfovPgMucxfz4niVF9ea4uPlHs/RhOF4xcSdxSLyGs5pqOJcGfPr4rZTD2I5D3gK5wqil1V1qhdxlCUiHYFZ7st44B9exSYibwBDcUrw7gAeAGbjXNXRDtgMXK6qEe24rSCuoXj83hKRQcAXwHLA706+B6c93rNjVklcV+DhMRORVJzO4DicL8MzVfVh93/gnzjNL0uBX6lqfi2Iay7QHKdpOROYENCpHJp9x0IiMMYYU7GYaBoyxhhTMUsExhgT4ywRGGNMjLNEYIwxMc4SgTHGxDhLBKbWE5Eit+riChH5T/G11h7E0V4CKo+WmZ7nxrhKRJ4TEV9Fy4c5xqkiskVEQnp5oanbLBGYaJCnqmmqmoJT9G1iJHYqInHVWHyDWxogFafCZtgLllUQ339w7gg3JmiWCEy0WUhAMTARmSwii92CXMX1238jIre6z590b8hBRIaLyOvu82dFJCOw7rs7PVtE7heRBcDlInKmWx9+IUEkILc8wVfA6YHT3bODL0TkG/cxwJ3+mlsPq3i5GSJykVt87PGA3+3X7vyh4tT4/wfOjVpl9/+1VzdLmuhlicBEDfcb8HDcshwiMhI4A+cbcBpwplsobz4w2F0tHWjo1rwpvtMV4F5VTcf5Bn+2e1dnsSOqOkhV/wm8Atyqqv2DjLG+G2PZD+mdwAi3qN/PgWfc6S8B17jrNgEGAO8D1wH7VLUPTrGx60Wkg7tOXzf+bsHEZExVLBGYaJDklubdg3P7/yfu9JHuYynwDU6FxjOAJThJoRGQj3MWkY6THIoTwc9E5Bt33e44zTnF3oSSD+ZkVZ3nTn+tkhhPc2P8Evivqn5QZn4C8KKILAfeKt6fu+3TRaQFTumFd9yzipHAVe42/wc0c383gEWquqmSWIypljpTfdTUaXmqmuZ+ML+H00TzDE7tlUdU9fmyK4hINs437a+AZcAwnNLHq91v1ncCfVT1BxGZDiQGrH6oeDMEX4q4uI+gIpNw6hP1xPkCdiRg3mvAWJwihNcG7PsWVf2ozO81NCA+Y0LCzghM1FDVfcCtwJ1uU89HwLVuvXtEpLX7zRqc5qE73Z9fABOATHWKazXG+TDdJyItccZfKG9/ue4yg9xJY48j/CbAdlX1A1fiFBYrNh243d3nSnfaR8CN7u+JiHRyq8IaE3KWCExUUdWlOGM9/0JVPwb+ASx0m1zexhkbF5wP/1bAQrf+/RF3Gqr6LU6T0ErgZZzmnIpcA/zF7SzOO47Q/wpcLSJfA50I+Fbvxrcapz+i2EvAKuAb9xLU5wniDF5EHhORHKC+iOSIyIPHEbOJEVZ91BiPuR3My3EGet/ndTwm9tgZgTEeEpGfAmuAP1kSMF6xMwJjjIlxdkZgjDExzhKBMcbEOEsExhgT4ywRGGNMjLNEYIwxMe7/A4L6JXHxw0hyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"ETP Example game as described in the thesis, based on a game developed by Llea Samuel\"\n",
    "p1_1 = np.matrix('16 14; 28 24')\n",
    "p2_1 = np.matrix('16 28; 14 24')\n",
    "\n",
    "p1_2 = np.matrix('4 3.5; 7 6')\n",
    "p2_2 = np.matrix('4 7; 3.5 6')\n",
    "\n",
    "trans1_1 = np.matrix('0.8 0.7; 0.7 0.6')\n",
    "trans2_1 = np.matrix('0.5 0.4; 0.4 0.15')\n",
    "\n",
    "trans1_2 = np.matrix('0.2 0.3; 0.3 0.4')\n",
    "trans2_2 = np.matrix('0.5 0.6; 0.6 0.85')   \n",
    "\n",
    "matrixA = np.matrix('0.00 0.0 0.0 0.00 0.0 0.00 0.00 0.00; 0.35 0.3 0.3 0.25 0.2 0.15 0.15 0.05; 0.35 0.3 0.3 0.25 0.2 0.15 0.15 0.05; 0.7 0.6 0.6 0.5 0.4 0.3 0.3 0.1; 0 0 0 0 0 0 0 0; 0.35 0.3 0.3 0.25 0.2 0.15 0.15 0.05; 0.35 0.3 0.3 0.25 0.2 0.15 0.15 0.05; 0.7 0.6 0.6 0.5 0.4 0.3 0.3 0.1')\n",
    "matrixB = np.matrix('0.00 0.0 0.0 0.00 0.0 0.00 0.00 0.00; 0.0 0.0 0.0 0.00 0.0 0.0 0.0 0.00; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0 0 0 0 0 0 0 0; 0.0 0.0 0.0 0.00 0.0 0.00 0.00 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0')\n",
    "\n",
    "FirstTryETP = ETPGame(p1_1,p2_1,p1_2,p2_2,trans1_1,trans2_1,trans1_2,trans2_2,matrixA)\n",
    "FirstTryETP.optimized_maximin(100000,False,False,True)\n",
    "FirstTryETP.threat_point_optimized(10000,True,True,True,True)\n",
    "FirstTryETP.plot_single_period_pure_rewards()\n",
    "FirstTryETP.plot_all_rewards(True)\n",
    "FirstTryETP.plot_threat_point()\n",
    "FirstTryETP.plot_threat_point_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
